\documentclass{report}

\input{../../LatexTemp/preamble}
\input{../../LatexTemp/macros}
\input{../../LatexTemp/letterfonts}

 \title{\Huge{Analisi (M2)}\\Appunti}
\author{\huge{Alex Bastianini}}
\date{}

\setlength{\parindent}{0pt}
\usepackage{graphicx}
\usepackage{pgfplots}
\pgfplotsset{
    integral segments/.code={\pgfmathsetmacro\integralsegments{#1}},
    integral segments=3,
    integral/.style args={#1:#2}{
        ybar interval,
        domain=#1+((#2-#1)/\integralsegments)/2:#2+((#2-#1)/\integralsegments)/2,
        samples=\integralsegments+1,
        x filter/.code=\pgfmathparse{\pgfmathresult-((#2-#1)/\integralsegments)/2}
    }
}

\begin{document}
\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Introduzione agli appunti}
\section{Le varie parti degli appunti}
Diversi box colorati per indicare diverse parti degli appunti:
\dfn{Definizione}{}
\thm{Teorema}{}
\pf{Dimostrazione}{}
\cor{Corollario}{}
\nt{Una roba importante}

\chapter{Integrali}
\section{Motivazioni}
Motivazioni:
\begin{itemize}
\item Calcolo di aree di figure curvilinee
\item Lunghezze di curve (non lo faremo)
\end{itemize}
Le nostre figure curvilinee sono sottografici di funzioni.
\section{Area sottesa a una curva}
\dfn{Area sottesa}{
  Data $f:[a,b] \to \mathbb{R}$
  \[
    A = \{(x,y) \in \mathbb{R}^2 | x \in [a,b]. 0 \leq y \leq f(x)\}
  \]
  
}
\subsection{Costruzione integrale di Riemann}
Speziamo un intervallo [a,b] in $n\in \mathbb{N}$ sottointervalli uguali. L'ampiezza di ciuascun intervallo e' di $\frac{b-a}{n}$. 
\begin{itemize}
\item $x_0 = a$
\item $x_1 = a + \frac{b-a}{n}$
\item $x_n = b$
\end{itemize}
In ogni intervallo fisso un punto arbirario $\epsilon_n$ 
\dfn{Somma di Riemann associata a una scomposizione}{
  Data una funzione $f:[a,b]\to\mathbb{R}$, fatta la costruzione precedente (spezzettamento), $\forall n \in \mathbb{N}$ la somma di Riemann n-esima di f e' il numero seguente:
  \[
  S_n = \sum_{k=1}^{n} f(\epsilon_k)(x_k - x_{k-1})
  \]
  
}
\begin{align}
  \begin{tikzpicture}[/pgf/declare function={f=-15*(x-5)+(x-5)^3+50;}]
    \begin{axis}[
        domain=0:10,
        samples=100,
        axis lines=middle
    ]
    \addplot [
        red,
        fill=red!50,
        integral=2:8
    ] {f};
    \addplot [thick] {f};
    \end{axis}
  \end{tikzpicture}
\end{align}
\begin{align*}
  \text{Esempio di somma di Riemann di una funzione $f:[2,8]\to\mathbb{R}$ con n=3:}
\end{align*}
\thm{Integrabilita' delle funzioni continue}{
  Sia $f:[a,b]\to\mathbb{R}$ continua. Sia $(S_n)$ la successione delle somme di Riemann, allora:
  \[
    \lim_{n\to +\infty} S_n = l \in \mathbb{R}
  \]
  E non dipende da quale $\epsilon_n$ scegliamo per ogni segmento.
}
\dfn{Integrale}{
  \[
    \int_{a}^{b} f(x)dx = \lim_{n\to+\infty} S_n
  \]
  La x e' una variabile \textbf{muta}. 
}
\nt{
\begin{itemize}
  \item Se $\forall x \in [a,b]. f(x) \geq 0$, allora $\int_a^b f$ = Area del sottografico.
  \item $\int_a^a f = 0$ (poiche $\forall n \in \mathbb{N}. S_n = 0$)
  \item $ f(x) = k (\text{funzione costante}) \implies \int_{a}^{b}f = (b-a)k (\text{Area del rettangolo}) $
\end{itemize}
}
\subsection{Proprieta' dell'integrale}
\subsubsection{Linearita'}
Se abbiamo due funzioni f, g continue su [a,b], $A, \mu \in \mathbb{R}$
\[
  \int^b_a (Af(x)+\mu g(x))dx = A\int^b_a f(x)dx + \mu\int^b_a g(x)dx
\]
\subsubsection{Additivita}
Data $f:[a,b]\to\mathbb{R}$ continua, dato $c \in [a,b]$, vale: \label{HA}
\[
\int^b_a f = \int^c_a f + \int^b_c f
\]
\nt{
Convenzione su integrali con estremi "rovesciati":\\
Dato $f:[a,b]\to\mathbb{R}$
\[
\int^b_a f = -\int^a_b f
\]
}
In questo modo possiamo generalizzare la proprieta' addittiva togliendo dall' ipotesi la restrizione sul valore di c.
\subsubsection{Monotonia}
Se $f:[a,b]\to\mathbb{R}$ e $\forall x \in [a,b]. f(x) \geq 0$, allora:
\[
\int^b_a f \geq 0 
\]
\subsection{Media Integrale}

\textbf{Premessa 1}\\
\thm{Valori intermedi}{
  Sia $f:[a,b]\to\mathbb{R}$ continua, $x_1, x_2, \in [a,b]. f(x_1) \leq f(x_2)$, allora:
  \[
    \forall y \in [f(x_1), f(x_2)].\exists c \in [x1, x2].f(c)=y
  \]
  
}
\textbf{Premessa 2}\\
\thm{Weierstrass}{
  Sia $f:[a,b]\to\mathbb{R}$ continua:
  \[
    \exists x_1, x_2 \in [a,b].\forall x \in [a,b]. f(x_1) \leq f(x) \leq f(x_2)
  \]
}
\begin{figure}[h!]
    \centering
    \includegraphics[width=50mm]{img/1200px-Extreme_Value_Theorem.svg.png}
    \caption{Esempio di Weierstrass}
  \end{figure}

\thm{Media integrale}{
  Sia $f:[a,b]\to\mathbb{R}$ continua, allora:
  \[
    \exists c \in [a,b]. \frac{1}{b-a}\int^b_a f(x)dx = f(c)
  \]      
}

\begin{figure}[h!]
  \centering
  \includegraphics[width=100mm]{img/2024-02-23-16-46-38.png}
\end{figure}

Quindi esiste un punto c in [a,b] t.c. il rettangolo che ha come base b-a e come altezza f(c) ha la stessa area dell'integrale di f da a a b.\\

\pf{Dimostrazione della media integrale}{
  Sia f continua su [a,b]. Per Weierstrass abbiamo che $\exists x_1, x_2 \in [a,b].\forall x \in [a,b].f(x_1) \leq f(x) \leq f(x_2)$. Per la proprieta' di monotonia risulta $\int^b_a f(x_1)dx \leq \int^b_a f(x)dx \leq \int^b_a f(x_2)dx$, ovvero $f(x_1) \leq \frac{1}{b-a}\int^b_a f(x)dx \leq f(x_2)$. Quindi per il teorema dei valori intermedi $\exists c \in [a,b].f(c)=\frac{1}{b-a}\int^b_a f(x)dx$.
}

\nt{
  La continuita' di f e' \textbf{necessaria}. Ex:
  \[
    f:[-1,1] \to \mathbb{R}, f(x) = x \implies \frac{1}{2} \int_{-1}^{1}xdx = 0 = f(0) (c=0)
  \]
 \begin{align*} 
    \begin{tikzpicture}
      \draw[->] (-1.5,0)--(1.5,0) node[below] {$ x $};
      \draw[->] (0,-1.5)--(0,1.5) node[left] {$ y $};
      \draw[domain=-1:1] plot (\x, \x);
      \draw[dashed] (1,0)--(1,1) (0.7, 0.3) node{$ A_1 $};
      \draw[dashed] (-1,0)--(-1,-1) (-0.7, -0.3) node{$ A_2 $};
      \draw (1.5, 1.5) node{$ A_1 = -A_2 $};
    \end{tikzpicture}
 \end{align*}
  Se considerassi $ g:[-1,1] \to \mathbb{R}$
  \[
    g(x) = \begin{cases}
    x & x \neq 0\\
    2 & x = 0
    \end{cases}
  \]
  Si dimostra che g e' intagarbile, e che vale
  \[
    \int_{-1}^{1}g(x)dx = 0
  \]
  Pero' non esiste $ c \in [-1,1] $ tale che g(c) = 0, quindi non soddisfa la media integrale.
}

\section{Primitive di una funzione}
\dfn{Primitiva di f}{
  Sia $ f: A \to \mathbb{R}, A \subseteq \mathbb{R} $\\
  Una funcione $ F: A \to \mathbb{R} $ si dice primitiva di f su A se vale
  \[
    \forall x \in A. F'(x) = f(x)
  \]
}
\ex{Primitiva}{
  $ f:\mathbb{R}\to \mathbb{R} $, $f(x) = \cos x \implies F(x) = sin x$ e' una primitiva di f su $ \mathbb{R} $. Infatti $\forall x. \frac{d}{dx}sinx = cosx $
}
\nt{
  $ \forall k \in \mathbb{R}\text{, la funzione }G(x) = sin(x)+k $ e' anchessa primitiva di $ f $. Quindi se $ F $ e' primitiva di $ f $ su A allora ci sono infinite primitive di $ f $ su A (una per ogni $ k \in \mathbb{R} $).\\
}
Domanda: sono tutte le possibili primitive?
\mprop{Caratterizzazione delle primitive di una funzione su un intervallo}{
  Sia $ f:]a,b[\to\mathbb{R}$. Siano $F:]a,b[\to\mathbb{R}\text{ e }G:]a,b[\to\mathbb{R}$ due primitive di $ f $ su $ ]a,b[ $.\\
  Allora $ \exists k \in \mathbb{R} $ tale che:
  \[
    \forall x \in ]a,b[. G(x)=F(x)+k
  \]
  Ovvero $ F $ e $ G $ "differiscono per una costante".
}
\pf{Dimostrazione}{
  Considero $ H:]a,b[\to\mathbb{R}, H(x)=G(x)-F(x) $. Sappiamo che F'(x)=f(x) e G'(x)=f(x) (def. primitiva).
  $ \frac{d}{dx}H(x) = \frac{d}{dx}G(x)-\frac{d}{dx}F(x) = f(x)-f(x) = 0 $. Dunque $ H $ ha derivata nulla su $ ]a,b[ $, quindi (per coroll. Lagrange) $ H $ e' costante.
}

\section{Funzioni integrali}
D'ora in poi $ A = ]a,b[ $.
\dfn{Funzione Integrale}{
  Sia $ f:A\to\mathbb{R} $ continua.\\
  Sia $ c\in A $. Introduco $ I_c:A\to\mathbb{R} $:
  \[
    I_c(x) = \int_{c}^{x}f(t)dt
  \]
  Nota: $ I_c $ e' ben definita essendo f continua.
}
\nt{
  \begin{enumerate}
    \item $ f:A\to\mathbb{R}, c\in A, I_c(x) = \int_{c}^{x}f \implies I_c(c)=\int_{c}^{c}f(t)dt = 0 $.
    \item Dati $ c,c' \in A, f:A\to\mathbb{R} \implies I_c(x)-I_{c'}(x) $ = costante. Infatti:
      \[
        I_c(x) - I_{c'}(x) = \int_{c}^{x}f - \int_{c'}^{x}f = \int_{c}^{x}f + \int_{x}^{c'}f = \int_{c}^{c'}f(t)dt = k
      \]
  \end{enumerate}
}
\subsection{Teoremi fondamentali del calcolo integrale}
\thm{Fondamentale del calcolo sulla derivata della funzione integrale}{
  Sia $ f:A\to\mathbb{R} $ continua, $ c \in A $. Sia $ I_c $ la funzione integrale, allora:
  \[
    I_c \text{ e' derivabile in ogni punto } x \in A \text{ e } I_c'(x) = f(x)
  \]
  Cioe' $ \frac{d}{dx}\int_{c}^{x}f(t)dt = f(x), \forall x \in A $, quindi $ I_c $ e' \textbf{primitiva} di $ f(x) $.
}
Una possibile interpretazione di questo teorema e' quello della derivata dell'area sottesa che e' uguale alla funzione stessa, ovvero:
\[
  \forall x. f(x) \geq 0 \implies \frac{d}{dx} \text{Area} = f(x)
\]
$ A = [-2,2], f(x)=\frac{x^2}{5} + 1 $:\\
\begin{tikzpicture}
  \fill[gray!40, domain=-2:2, variable=\x]
   (-2,0)
   -- plot (\x, \x*\x/5 + 1)    
   -- (2,0)
   -- cycle;
   \draw[domain=-2:2] plot (\x, {(\x*\x/5 + 1)}) node[right]{$ f(x) = \frac{d}{dx}Area(x) $};

   \draw[->] (-3,0)--(3,0) node[below]{$ x $};
   \draw[->] (0,0)--(0,3) node[left]{$ y $};
\end{tikzpicture}
\begin{tikzpicture}

  \draw[domain=-2:2] plot (\x, {\x/2*(1+(\x*\x/15)) + 19/15}) node[right]{$ Area(x) = \int_{-2}^{x}f(x)dx $};
   \draw[->] (-3,0)--(3,0) node[below]{$ x $};
   \draw[->] (0,0)--(0,3) node[left]{$ y $};
\end{tikzpicture}
\nt{
  Il teorema assicura che ogni funzione $ f:A\to\mathbb{R} $ continua ammette primitive.
}
\pf{Dimostrazione}{
  $ f:A\to\mathbb{R}, c\in A, I_c:A\to\mathbb{R} $. Devo calcolare la derivata di $ I_c $. Calcolo $ \lim_{h\to 0^+}\frac{I_c(x+h)-I_c(x)}{h} $, che equivale a $ \lim_{h\to 0^+}\frac{1}{h} \int_{x}^{x+h}f(t)dt $. Per teo. media integrale sappiamo che $ \exists c \in [x,x+h]. f(c) = \frac{1}{h}\int_{x}^{x+h}f(t)dt $, quindi possiamo riscrivere la formula come $ \lim_{h\to 0^+}f(c_h) $. Dato che $ h \to 0^+ $ e $ x \leq c \leq x+h $, per il teo. dei carabinieri $ c=x $, quindi $ \frac{d}{dx}I_c(x) = f(x) $.
}

\thm{Fondamentale del calcolo per integrali definiti}{
  Sia $ f:A\to\mathbb{R} $ continua su A. Sia $ F:A\to\mathbb{R} $ primitiva di f su A. Dati $ a,b\in A $, vale:
  \[
    \int_{a}^{b}f(x)dx = [F(b) - F(a)] = [F(x)]_a^b
  \]
}
\pf{Dimostrazione}{
  Sia $ c\in A $, $ I_c:A\to \mathbb{R} $ la funzione integrale ($ I_c(x) = \int_{c}^{x}f $). Per il teo. fond. del calc. sulla derivata di $ I_c $, $ I_c $ e' una primitiva di f su A. Per le proprieta' delle primitive, $ \exists k \in \mathbb{R}.\forall x \in A.F(x)=I_c(x)+k $. 
  \[
    F(b)-F(a)=(I_c(b)+k)-(I_c(a)+k)=I_c(b)-I_c(a)=\int_{c}^{b}f-\int_{c}^{a}f=\int_{a}^{c}f+\int_{c}^{b}f=\int_{a}^{b}f
  \]
}

\subsection{Integrazione per parti}
Si parte dalla regola del prodotto delle derivate ($ \frac{d}{dx}f(x)\cdot g(x) = f'(x)g(x)+f(x)g'(x) $) per trovare una regola di integrazione.
\thm{Integrazione per parti}{
  Dati $ f,g:A\to\mathbb{R} $, A intervallo aperto e sia $ F $ primitiva di $ f $ su $ A $ con F,f,g continue, g derivabile e g' continua:
  \[
    \int_{a}^{b}\frac{d}{dx}(F(x)g(x))dx = \int_{a}^{b}f(x)g(x)dx + \int_{a}^{b}F(x)g'(x)dx
  \]
  Quindi usando il teorema fondamentale:
  \[
    [F(x)g(x)]_a^b = \int_{a}^{b}f(x)g(x)dx + \int_{a}^{b}F(x)g'(x)dx
  \]
  \[
    \int_{a}^{b}f(x)g(x)dx = [F(x)g(x)]_a^b - \int_{a}^{b}F(x)g'(x)dx
  \]
}

\subsection{Cambio di variabile}
\thm{Formula del cambio di variabile}{ \label{cambVar}
  Date $ h:I\to A $, $ f:A\to\mathbb{R} $, $ I,A \subseteq \mathbb{R} $ e $ \exists(f\circ h): I\to\mathbb{R}.(f\circ h)(t)=f(h(t)) $. $ f $ continua, $ h $ derivabile e $ h' $ continua. Presi $ \alpha,\beta\in I $, vale:
  \[
    \int_{h(\alpha)}^{h(\beta)} f(x)dx = \int_{\alpha}^{\beta}f(h(t))h'(t)dt
  \]
  Questa e' la versione generalizzata del teo. fond. del calcolo
}
\pf{Dimostrazione}{
  Date due funzioni $G,H: I\to\mathbb{R}. G(z) = \int_{\alpha}^{z}f(h(t))h'(t)dt, H(z) = \int_{h(\alpha)}^{h(z)}f(x)dx $, dobbiamo dimostrare che $ G(z) = H(z) $. Ci riduciamo a dimostrare che:
  \begin{enumerate}
    \item $ G(\alpha) = H(\alpha) $: ovvio perche' integrali su intervallo degenere ($ G(\alpha)=H(\alpha)=0 $)
    \item $ \forall z \in I.G'(z)=H'(z) $:
      \begin{itemize}
        \item $ G'(z) = \frac{d}{dz}\int_{\alpha}^{z}f(h(t))h'(t)dt = f(h(z))h'(z) $
        \item $ H'(z) = \frac{d}{dz}\int_{h(\alpha)}^{h(z)}f(x)dx = f(h(z))h'(z) $ (generalizzazione del teorema fondamentale del calcolo)
      \end{itemize}
  \end{enumerate}
}
\nt{
  \begin{enumerate}
    \item t integrata in $ \alpha,\beta $, allora x sara' integrata in $ h(\alpha),h(\beta) $.
    \item $ dx $ si e' trasformato in $ h'(t)dt $ ($ \frac{d}{dt}h(t) = h'(t) \implies dh(t) = h'(t)dt $)
  \end{enumerate}
}

\section{Integrali generalizzati (impropri)}
\dfn{Integrali generalizzati su intervalli illimitati}{
  Sia $ f: [a, +\infty[\to\mathbb{R} $. Si dice che l'integrale generalizzato $ \int_{a}^{+\infty} f(x)dx $ e' \textbf{convergente} se e' finito il limite $ \lim_{r\to+\infty}\int_{a}^{r}f(x)dx \coloneq \int_{a}^{+\infty}f(x)dx $. Altrimenti se il limite diverge o oscilla e' detto \textbf{divergente} (o oscillante).\\
  La definizione e' analoga per $ \int_{-\infty}^{a}f(x)dx $.
}

\dfn{Integrali generalizzati su intervalli limitati}{
  Sia $ f: ]a, b]\to\mathbb{R} $. Si dice che l'integrale $ \int_{a}^{b}f(x)dx $ e' \textbf{convergente} se il limite $ \lim_{r\to a^+}\int_{r}^{b}f(x)dx $ e' finito. Altrimenti se il limite diverge o oscilla e' detto \textbf{divergente} (o oscillante).\\
  La definizione e' analoga per $ f: [a, b[\to\mathbb{R} $.
}

\chapter{Spazio euclideo $\mathbb{R}^n$}
(Spazio \textbf{euclideo} = c'e' il prodotto scalare)\\
$ \mathbb{R}^n = \{x=(x_1,x_2,...,x_n)|\forall j\in \{1,2,...,n\}.x_j \in \mathbb{R} \} = \mathbb{R} \times \mathbb{R} \times ... \mathbb{R} $ (n volte).
\begin{itemize}
\item $ n=1 $ retta reale
\item $ n=2 $ piano cartesiano
\item $ n=3 $ spazio ordinario
\end{itemize}
\section{Operazioni su $ \mathbb{R}^n $}
\begin{itemize}
  \item Somma di vettori: $ x=(x_1,...,x_n), y=(y_1,...,y_n) $. Definiamo $ x+y=(x_1+y_1,...,x_n+y_n) \in \mathbb{R}^n $
  \item Prodotto di $ x\in\mathbb{R}^n $ per uno scalare $ \lambda \in \mathbb{R} $. Dato $ x=(x_1,...,x_n), \lambda \in \mathbb{R} $, definiamo $ \lambda x= (\lambda x_1, ..., \lambda x_n) $.
\end{itemize}

\dfn{Prodotto scalare}{
  Dati $ x,y \in \mathbb{R}_n $, definiamo il prodotto scalare $ <x,y> = \sum_{k=1}^{n}x_ky_k=x_1y_1+...+x_ny_n \in \mathbb{R}^n  $.\\
  Notazione alternativa: $ <x,y> = x \cdot y $.
}
\nt{
  Il prodotto scalare non da' un nuovo vettore, ma solo un valore scalare!
}
\subsection{Proprieta' del prodotto scalare (euclideo)}
\begin{itemize}
  \item $ \forall x,y \in \mathbb{R}^n. <x,y> = <y,x> $ (simmetria)
  \item $ \forall x,y,z \in \mathbb{R}^n, \lambda, \mu \in \mathbb{R}. <\lambda x+\mu y, z> = \lambda<x,z> + \mu<y,z> $ (linearita' nel primo argomento)\\
    Per simmetria vale $ <z, \lambda x + \mu y> = \lambda<z,x> + \mu<z,y> $ (linearita' nel secondo argomento)
  \item $ \forall x \in \mathbb{R}^n. <x,x> \geq 0 $, inoltre $ <x,x> = 0 \iff x = (0,0,...,0) = \underline{0} $ (vettore nullo). 
\end{itemize}

\section{Ortogonalita'}
\dfn{Vettori ortogonali}{
  Due vettori $ x,y \in \mathbb{R}^n $ si dicono \textbf{ortogonali} se vale:
  \[
  <x,y> = 0
  \]
}

\section{Norma euclidea}
Sinonimi: modulo, lunghezza
\dfn{}{
  Dato $ x \in \mathbb{R}^n $, allora 
  \[
  \lVert x \rVert = \sqrt{<x,x>} \geq 0
  \]
  Rappresenta la "lunghezza" del vettore usando il teorema di Pittagora. \\
  Notazione alternativa: $ |x| $
}
\subsection{Proprieta' della norma}
\begin{itemize}
\item $ \forall x \in \mathbb{R}^n. \lVert \lambda x \rVert = |\lambda| \cdot \lVert x \rVert $
\item $ \forall x \in \mathbb{R}^n. \lVert x \rVert = 0 \iff x = 0 $
\item $ \forall x,y \in \mathbb{R}^n. |x+y| \leq |x|+|y| $ (Disuguaglianza triangolare)
\end{itemize}
\section{Vettore normalizzato}
\dfn{Normalizzato}{
  Dato $ x \neq 0 \in \mathbb{R}^n $, cerco $ r > 0 $ t.c.
  \[
  |rx|=1
  \]
  Visto che $ r>0 $, $ r|x|=1 $ quindi $ r = \frac{1}{|x|} $.\\
  Il vettore $ \frac{x}{|x|} $ ha norma 1 e si dice \textbf{normalizzato} di $ x \in \mathbb{R}^n\setminus \{\underline{0}\} $.\\
  $ \frac{x}{|x|} $ si dice \textbf{vettore unitario}.
}
\nt{
  Possiamo scrivere $ x = |x|\cdot \frac{x}{|x|} $ se $ x \neq 0 $.
}
\section{Coordinate polari}
In $ \mathbb{R}^2 $, ogni $ (x,y)\neq(0,0) $ si scrive nella forma $ |(x,y)|\cdot (\frac{x}{|(x,y)|}, \frac{y}{|(x,y)|}) $. L'insieme di coordinate $ \{(\frac{x}{|(x,y)|}, \frac{y}{|(x,y)|})|x,y\in\mathbb{R}\} $ forma una \textbf{circonferenza unitaria} (dato che il loro modulo e' sempre 1), quindi $ \exists \theta \in [0,2\pi[ $ tale che $ (cos\theta, sin\theta) = (\frac{x}{|(x,y)|}, \frac{y}{|(x,y)|}) $ ($ \theta $ si chiama \textbf{"argomento"} di $ (x,y) $). Ponendo $ r = |(x,y)| = \text{modulo} $, scriviamo:
\[ (x,y) = r(cos\theta, sin\theta) \]
dove $ r > 0 $ e $ \theta \in [0,2\pi[ $ si chiamano \textbf{coordinate polari} di (x,y).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.3\textwidth]{img/2024-03-15-10-51-45.png}
    \begin{itemize}
    \item $ r =  $ vettore unitario
    \end{itemize}
\end{figure}


\subsection{Prodotto scalare in coordinate polari}
Considero due vettori $ (x,y) = (r cos\theta, r sin \theta) $ e $ (n,o) = (\rho cos \gamma, \rho sin\gamma) $. Il loro prodotto scalare $ <(x,y), (n,o)> $ diventa $ r\rho cos\theta cos\gamma + r\rho sin\theta sin\gamma $, che puo' essere riscritto come $ r\rho cos(\gamma - \theta) $, ovvero:
\[
  |(x,y)|\cdot|(n,o)|\cdot cos(\gamma - \theta)
\]
\mprop{Disuguaglianza di Cauchy-Schwarz}{ \label{cauchySchwarz}
  Dati $ x,y \in \mathbb{R}^n $, vale:
  \[
  |<x,y>| \leq \lVert x \rVert \cdot \lVert y \rVert
  \]
  L'uguaglianza vale solo se $ x $ e $ y $ sono linearmente dipendenti.
}
\nt{
  Vale in ogni $ \mathbb{R}^n \forall n \in \mathbb{N}$
}
\mprop{Quadrato di binomio}{
  \[
  |x+y|^2 = |x|^2 + |y|^2 + 2<x,y>
  \]
  Generalizzazione di Pitagora (In due dimensioni diventa il teorema di Carneau).
}
\mprop{Disuguaglianza Triangolare}{
  $ \forall x,y \in \mathbb{R}^n $ si ha che:
  \[
  |x+y| \leq |x| + |y|
  \]
}
\pf{}{
  Dimostriamo il quadrato della disuguaglianza per poter usare il quadrato di binomio:
  \[
    |x+y|^2 = |x|^2 + |y|^2 + 2<x,y> \leq |x|^2+|y|^2+2|<x,y>| \leq |x|^2+|y|^2+2|x||y| = (|x|+|y|)^2
  \]
  \[
    \implies |x+y|^2 \leq (|x|+|y|)^2
  \]
}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{img/2024-03-15-11-28-54.png}
\end{figure}


\section{Distanza tra punti}
\dfn{Distanza fra due punti}{
  Dati $ x,y \in \mathbb{R}^n $, la distanza fra $ x $ e $ y $ e' $ |x-y| $
}

\subsection{Punto di minima distanza da una retta}
Problema $ \mathbb{R}^n, v\neq 0, x \in \mathbb{R}^n $. Considero le linee $ l_v = \{tv|t\in\mathbb{R}\} $, cerco fra tutti i punti di $ l_v $ quello che ha minima distanza da $ x $. Devo minimizzare la funzione $ h:\mathbb{R}\to\mathbb{R}, h(t)=|x-tv| = \text{distanza fra x e tv} $
\mprop{}{
  Dati $ v\neq 0 $ e $ x \in \mathbb{R}^n $, il punto di minima distanza $ \frac{<x,v>}{|v|^2}v $ soddisfa:
  \[
  <x-\frac{<x,v>}{|v|^2}v, v> = 0
  \]
  Quindi il vettore che parte dal punto di minima distanza e arriva al punto x e' perpendicolare alla retta $ l_v $.
}

\section{Intorni}
\dfn{Intorno sferico di un punto}{
  Dato $ x \in \mathbb{R}^n $, $ r > 0 $, poniamo
  \[
    D(x,r) = \{y\in \mathbb{R}^n | |x-y| < r\}
  \]
  $ D(x,r) $ si dice disco di centro $ x $ e raggio $ r $.
}
\dfn{Insiemi aperti}{
  $ A \subseteq \mathbb{R}^n $ si dice aperto se:
  \[
    \forall x \in A. \exists \epsilon > 0.D(x,\epsilon) \subseteq A
  \]
}
\section{Successioni in $ \mathbb{R}^n $}
Una successione $ (x_k)_{k\in \mathbb{N}} $ e' un vettore di $ k $ successioni: $ x_k = (x_k^1, x_k^2, ..., x_k^n) $ con $ k \in \mathbb{N} $
\chapter{Funzioni a piu' variabili}
Dati $ A \subseteq \mathbb{R}^n, B \subseteq \mathbb{R}^q $, consideriamo funzioni $ f:A\to B $ (A = dominio, B = codominio).

Casi modello:
\begin{itemize}
  \item $ f:\mathbb{R}^n\to\mathbb{R} $ (funzioni scalari)
  \item $ f:\mathbb{R}\to\mathbb{R}^q $ (cammini in $ \mathbb{R}^q $)
\end{itemize}

\section{Insiemi di livello}
\dfn{}{
  $ A \subseteq \mathbb{R}^n, f:A\to\mathbb{R}, b \in \mathbb{R} $. L'insieme di livello $ b $ di $ f $ e':
  \[
    L_b = \{x \in A | f(x) = b \} = f^{-1}(b)
  \]
}
Se cammino lungo l'insieme di livello, la funzione corrispondente non cambia
\section{Continuita'}
\dfn{Funzioni continue}{
  $ A \subseteq \mathbb{R}^n, f:A\to\mathbb{R}, k \in A $. Si dice che $ f $ e' continua in $ k \in A $ se $ \forall (x_n)_{n\in\mathbb{N}} \in \mathbb{R}^n $ vale:
  \[
  \begin{cases}
  x_n \in A & \forall n \in \mathbb{N}\\
  x_n \longrightarrow k & k \longrightarrow +\infty
  \end{cases} \implies f(x_n)\longrightarrow f(k)
  \]
}
\mprop{}{
  Sia $ A \subseteq \mathbb{R}^n, f:A\to\mathbb{R}, k\in A $. Allora $ f $ e' continua in $ k $ se:
  \[
  \forall \epsilon > 0\exists\delta>0. \begin{cases}
  x \in A & \\
    |x-k|<\delta & 
  \end{cases} \implies |f(x)-f(k)|<\epsilon
  \]
}

\section{Derivata parziale}
\dfn{Derivata parziale}{
  Data $ f:A\to \mathbb{R}, A \subseteq \mathbb{R}^2 $. Dati $ (\overline{x},\overline{y}) \in A $ $ f $ si dice derivabile parzialmente rispetto a $ \overline{x} $ se:
  \[
    \exists \lim_{h\to 0}\frac{f(\overline{x}+h,\overline{y})-f(\overline{x},\overline{y})}{h} = \frac{\partial f}{\partial x}(\overline{x}, \overline{y})
  \]
  In modo analogo per $ \frac{\partial f}{\partial y}(\overline{x}, \overline{y}) $.
}
\dfn{Gradiente}{
  Se $ f:\mathbb{R}^2\to\mathbb{R} $ ammette derivate parziali $ \forall(\overline{x}, \overline{y}) \in \mathbb{R}^2 $, definiamo il \textbf{gradiente di f} come:
  \[
    \nabla f(x,y) = (\frac{\partial f}{\partial x}(x,y), \frac{\partial f}{\partial y}(x,y))  
  \]
  $ \nabla f: \mathbb{R}^2\to\mathbb{R}^2 $ (funzione vettoriale)
}
\nt{
  $ f:\mathbb{R}^2\to\mathbb{R} $, $ (\overline{x}, \overline{y}) $ fissato:
  \[
    \partial_xf(\overline{x}, \overline{y}) = \lim_{h\to 0}\frac{f(\overline{x}+h,\overline{y})-f(\overline{x}, \overline{y})}{h} = \lim_{x\to \overline{x}}\frac{f(x, \overline{y})-f(\overline{x}, \overline{y})}{x-\overline{x}}
  \]
  Introduco una funzione $ g:\mathbb{R}\to\mathbb{R} $, $ g(x) = f(x,\overline{y}) $, in modo che, facendo la normale derivata di $ g $:
  \[
    g'(\overline{x}) = \lim_{x \to \overline{x}}\frac{g(x) - g(\overline{x})}{x-\overline{x}} = \frac{\partial f}{\partial x}f(\overline{x}, \overline{y})
  \]
  Abbiamo quindi trasformato una derivata parziale in una derivata "normale" fissando tutti i parametri tranne uno. 
}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{img/2024-03-22-10-57-10.png}
\end{figure}

\subsection{Derivate parziali in $ \mathbb{R}^n $}
In $ \mathbb{R}^n $, data $ f:\mathbb{R}^2\to\mathbb{R} $ possiamo riscrivere la derivata parziale cosi:
\[
  \frac{\partial f}{\partial x}(\overline{x}, \overline{y}) = \lim_{t\to 0}\frac{f((\overline{x}, \overline{y}) +t(1,0))- f(\overline{x}, \overline{y})}{t}
\]
Possiamo usare quindi le basi canoniche $ (e_1 = (1,...,0),...,e_n(0,...,1)) $ per indicare per quale dei valori del vettore passato come parametro vogliamo derivare.
\dfn{}{
  $ f:\mathbb{R}^n\to\mathbb{R} $, $ x=(x_1,...,x_n), \overline{x} \in \mathbb{R}^n $. Per $ k = \{1,...,n\} $:
  \[
    \frac{\partial f}{\partial x_k}(\overline{x}) = \lim_{t\to 0}\frac{f(\overline{x} + te_k)-f(\overline{x})}{t}
  \]
}

\section{Derivabilita' e continuita'}
In $ \mathbb{R} $, se una funzione era derivabile in un punto allora era anche continua, pero' se in piu' variabili non e' cosi:(guardare es slide)

\section{Differenziabilita'}
In una dimensione, possiamo dire che:
\[
  \exists f'(\overline{x}) \in \mathbb{R} \iff f(\overline{x}+h) = f(\overline{x}) + f'(\overline{x})h + o(h)   
\]
Quindi una funzione e' derivabile in un punto sse vale lo sviluppo di Taylor. Infatti, se sostituiamo $ x $ a $ \overline{x} + h $, dove $ x\to \overline{x} $ otteniamo il polinomio di Taylor di primo grado nel punto $ \overline{x} $: $ f(x) = f(\overline{x}) + f'(\overline{x})(x - \overline{x}) + o(x - \overline{x}) $.\\
Come vedremo, questa prposizione non vale quando aumentiamo le dimensioni. Infatti, solo in una dimensione differenziabilita' e derivabilita' coincidono.
\subsection{o piccolo in piu' variabili}
\dfn{o piccolo}{
  Dati $ A \subseteq \mathbb{R}^n, h \in A, g:A\to\mathbb{R} $, assumendo che $ 0 \in A $, si dice che $ g $ e' \textbf{o piccolo} di $ |h|^p $ (con $ p \geq 0 $) se:
  \begin{enumerate}
    \item $ g(0) = 0 $
    \item $ \forall \epsilon > 0. \exists \delta > 0.\forall h \neq 0. |h| < \delta. \frac{|g(h)|}{||h||^p} < \epsilon $
  \end{enumerate}
}
\ex{}{
  Verifica le seguenti uguaglianze:
  \begin{itemize}
    \item $ g(h,k) = h^2 + k^2 = o(|h+k|) $
      \[
      h^2 + k^2 = |(h,k)|^2
      \]
      Quindi:
      \[
      \frac{|(h,k)|^2}{|(h,k)|} = |(h,k)|
      \]
      Dobbiamo dimostrare che $ \forall \epsilon > 0.\exists \delta > 0. \forall 0 < |(h,k)| < \delta. |(h,k)| < \epsilon $, che possiamo fare mettendo $ \delta = \epsilon $ per ogni $ \epsilon > 0 $.
  \end{itemize}
}
Possiamo riscrivere questa definizione usando le successioni:
\[
  g(h) = o(|h|^p) \iff \forall (h_j)_{j\in\mathbb{N}}. \begin{cases}
  h_j \neq 0 & \forall j \in \mathbb{N}\\
    \lim_{j\to+\infty}(h_j) = 0 & 
  \end{cases} \implies \lim_{j\to +\infty}\frac{g(h_j)}{||h_j||^p} = 0
\]
\dfn{Differenziabilita' in due variabili}{
  $ (x,y),(h,x)\in\mathbb{R}^2 $. Sia $ f:\mathbb{R}^2\to\mathbb{R} $. Sia $ (\overline{x}, \overline{y})\in\mathbb{R}^n $. Si dice che $ f $ e' differenziabile in $ (\overline{x}, \overline{y}) $ se:
  \begin{enumerate}
    \item $ \exists \partial_xf(\overline{x}, \overline{y}), \partial_yf(\overline{x}, \overline{y}) $
    \item Vale lo sviluppo:
      \[
        f((\overline{x}, \overline{y})+(h,k)) = f(\overline{x}, \overline{y}) + <\nabla f(\overline{x}, \overline{y}), (h,k)> + o(|(h,k)|) = 
      \]
      \[
        f(\overline{x}, \overline{y}) + \partial_xf(\overline{x}, \overline{y})h + \partial_yf(\overline{x}, \overline{y})k + o(|(h,k)|)
      \]
      Per $ (h,k) \to (0,0) $
  \end{enumerate}
}

\section{Continuita' di una funzione differenziabile}
Sappiamo che l'esistenza delle derivate parziali non implica la continuita' della funzione. Mostreremo pero' che se sappiamo che una funzione e' differenziabile, allora sara' sicuramente continua.
\mprop{Differenziabilita' implica continuita'}{
  Data $ f:\mathbb{R}^2\to\mathbb{R} $, se $ f $ e' differenziabile in $ (\overline{x}, \overline{y}) \in \mathbb{R}^2 $, allora $ f $ e' continua in $ (\overline{x}, \overline{y}) $.
}
\pf{}{
  Verifichiamo che la proposizione valga. Per ipotesi, sappiamo che $ f $ e' differenziabile in $ (\overline{x}, \overline{y}) \in \mathbb{R}^2 $. Quindi $ f(\overline{x}+h, \overline{y}+k) = f(\overline{x}, \overline{y}) + \innerproduct{\nabla f(\overline{x}, \overline{y})}{(h,k)} + o(|h,k|) $ (TODO: capire perche' questa vale solo quando (h,k) tende a (0,0)). Dobbiamo dimostrare che $ f $ sia continua in $ (\overline{x}, \overline{y}) $, ovvero che (usando la definizione di continuita' per successioni): $ \forall (h_j, k_j): \mathbb{N} \to \mathbb{R}^2. (h_j, k_j) \xlongrightarrow{j\to +\infty}(0,0). \lim_{j\to +\infty} f(\overline{x}+h_j, \overline{y}+k_j) = f(\overline{x}, \overline{y}) $. Quindi usando la nostra ipotesi possiamo ridurci a dimostrare che $ \lim_{j \to +\infty} \innerproduct{\nabla f(\overline{x}, \overline{y})}{(h_j,k_j)} + o(|h_j,k_j|) = 0 $. Dato che $ (h_j, k_j) $ tende a $ (0,0) $ per $ j $ che tende a $ +\infty $, il prodotto scalare diventa nullo. Per quanto riguarda l'o-piccolo, possiamo usare la sua proprieta' per cui $ \lim_{j\to +\infty} \frac{o(|h_j, k_j|)}{|h_j, k_j|}  = 0 $ riscrivendolo come $ \frac{o(|h_j, k_j|)}{|h_j, k_j|}|h_j, k_j| $. Con $ j \to +\infty $ sia la frazione che il fattore moltiplicativo tendono a $ 0 $. 
}
\section{Condizioni sufficenti per la differenziabilita'}
\thm{}{
  Sia $ f:\mathbb{R}^2\to\mathbb{R} $. Assumo che esistano continue $ \partial_xf(\overline{x}, \overline{y}), \partial_yf(\overline{x}, \overline{y}) $ per ogni $ (\overline{x}, \overline{y}) \in \mathbb{R}^2 $, allora:
  \[
  f \text{ e' differenziabile in ogni punto di } \mathbb{R}^2
  \]
}
\nt{
  Questo teorema vale anche in $ \mathbb{R}^n $. Inoltre le funzioni elementari soddisfano sempre le ipotesi nel loro dominio, quindi sono differenziabili.
}
Per dimostrare questo teorema, ci serve prima una proposizione che equivale al teorema di Lagrange, pero' in piu' dimensioni. Usando le derivate parziali, ci riduciamo a due casi (uno dove ci si muove lungo $ x $, e uno lungo $ y $) dove ci riduciamo ad una funzione $ \mathbb{R}\to\mathbb{R} $ che sappiamo gia' dimostrare.
\mprop{Lagrange per derivate parziali}{
  Sia $ f:\mathbb{R}^2\to\mathbb{R} $ una funzione con derivate parziali $ \partial_xf, \partial_yf $ continue, $ \forall (\overline{x}, \overline{y}), (h,k) \in \mathbb{R}^2. \exists \delta,\overline{\delta} \in ]0,1[ $ tali che:
  \[
  \begin{cases}
    \frac{f(\overline{x}+h,\overline{y}) - f(\overline{x}, \overline{y})}{h} = \partial_xf(\overline{x}+\delta h, \overline{y}) & \\
    \frac{f(\overline{x}, \overline{y}+k) - f(\overline{x}, \overline{y})}{k} = \partial_yf(\overline{x}, \overline{y}+\overline{\delta}k) & 
  \end{cases}
  \]
}
Ora che abbiamo Lagrange per le derivate parziali, possiamo dimostrare il teorema:
\pf{Dimostrazione teo. differenziabilita'}{
  Sia $ (\overline{x}, \overline{y}) \in \mathbb{R}^2 $ e siano le derivate parziali di $ f $ continue, dobbiamo dimostrare che:
  \[
    f(\overline{x}+h, \overline{y}+k) - f(\overline{x}, \overline{y}) = <\nabla f(\overline{x}, \overline{y}), (h,k)> + o(|(h,k)|)  
  \]
  Riscriviamo la parte sinistra:
  \[
    f(\overline{x}+h, \overline{y}+k) - f(\overline{x}, \overline{y}) = [f(\overline{x}+h, \overline{y}+k)-f(\overline{x}+h, \overline{y})]_1 + [f(\overline{x}+h, \overline{y})-f(\overline{x}, \overline{y})]_2 
  \]
  Ci riduciamo a dimostrare che:
  \begin{align*}
    []_1 = \partial_y f(\overline{x}, \overline{y})k + o(|(h,k)|) \\
    []_2 = \partial_x f(\overline{x}, \overline{y})h + o(|(h,k)|)
  \end{align*}
  Analizziamo il caso $ []_2 $: usiamo Lagrange, che ci dice che $ \exists \theta \in ]1,0[.[]_2 = \partial_x f(\overline{x}+\theta h,\overline{y})h $, quindi dimostriamo che questo equivale a $ \partial f(\overline{x}, \overline{y})h + o(|(h,k)|) $:
  \begin{align*}
    [\partial f(\overline{x}+\theta h, \overline{y})-\partial f(\overline{x}, \overline{y})]h &= o(|(h,k)|) \\
      g(h,k) &= o(|(h,k)|)
  \end{align*}
  Sappiamo che $ g(0,0) = 0 $, quindi data una successione $ (h_n,k_n) \xlongrightarrow{n\longrightarrow\infty} (0,0) $ con $ (h_n,k_n) \neq \underline{0} \forall n \in \mathbb{N}$ dimostriamo che:
  \[
    \lim_{n\to +\infty} \left|\frac{[\partial f(\overline{x}+\theta h_n, \overline{y})-\partial f(\overline{x}, \overline{y})]h_n}{|(h_n,k_n)|}\right| = 0
  \]
  Grazie al valore assoluto sappiamo che la funzione sara' sempre $ \geq 0 $, e se la riscriviamo in questo modo:
  \[
    |[\partial f(\overline{x}+\theta h_n, \overline{y})-\partial f(\overline{x}, \overline{y})]|\frac{|h_n|}{|(h_n,k_n)|}
  \]
  possiamo dire che sara' sempre $ \leq |\partial f(\overline{x}+\theta h_n, \overline{y})-\partial f(\overline{x}, \overline{y})| $, dato che $ \frac{|h_n|}{|(h_n, k_n)|} \leq 1 $, $ \forall n \in \mathbb{N},\theta\in]0,1[ $. Usiamo quindi il teorema dei carabinieri e ci riduciamo a dimostrare che questo upper-bound tenda a $ 0 $ per $ n $ che tende a infinito. Grazie alla continuita' della derivata parziale, possiamo dire che:
  \[
    \lim_{n\to +\infty}\partial f(\overline{x}+\theta h_n, \overline{y}) = \partial f(\overline{x}, \overline{y})
  \]
  Quindi la loro differenza e' $ 0 $. Si possono fare passaggi simili per dimostrare $ []_1 $.
}

\section{Derivate direzionali}
Fino ad ora abbiamo visto il valore di crescita della funzione solo lungo le assi principali $ x,y $ (le derivate parziali), vediamo ora cosa succede se ci allontaniamo da un punto cambiando entrambe le sue coordinate:
\dfn{Derivata direzionale}{
  Sia $ f:\mathbb{R}^2\to\mathbb{R} $ e $ (\overline{x}, \overline{y}) \in \mathbb{R}^2 $. Dato un vettore unitario $ v=(v_1,v_2) $ $ (|v|=1) $, $ f $ si dice derivabile lungo la direzione $ v $ nel punto $ (\overline{x}, \overline{y}) $ se esiste finito:
  \[
    \frac{\partial f}{\partial v}(\overline{x}, \overline{y}) = \lim_{h\to 0}\frac{f((\overline{x}, \overline{y}) + hv)-f(\overline{x}, \overline{y})}{h}
  \]
}
\thm{Formula del gradiente}{
  Sia $ f:\mathbb{R}^n\to\mathbb{R} $ differenziabile in $ \overline{x} $, allora:
  \[
    \forall v = (v_1,...,v_n) \neq \underline{0}. |v| = 1:
  \]
  \[
    \frac{\partial f}{\partial v}(\overline{x}) = <\nabla f(\overline{x}), v>
  \]
}
\pf{}{
  Dobbiamo dimostrare che $ \lim_{t\to 0}\frac{f(\overline{x}+tv)}{t} = <\nabla f(\overline{x}), v> $. Essendo $ f $ differenziabile possiamo dire che $ f(\overline{x}+tv) = f(\overline{x}) + <\nabla f(\overline{x}), tv> + o(|vt|) $. Dato che $ |vt| = |v||t| = 1|t| = |t| $, possiamo riscrivere l'o-piccolo come $ o(|t|) $. Sostituendo, rimane $ \lim_{t\to 0}\frac{<\nabla f(\overline{x}), vt> + o(|t|)}{t} $. Possiamo dividere il limite, ottenendo $ <\nabla f(\overline{x}), v> $ (abbiamo diviso per $ t $) e $ \lim_{t\to 0}\frac{o(|t|)}{t} $ che per definizione e' $ 0 $. L'uguaglianza quindi vale.
}
\nt{
  Lineare in $ v_1,...,v_n $ con coefficenti le derivate parziali, quindi tutte le derivate direzionali $ \partial_vf $ si possono scrivere conoscendo solo le derivate parziali.
}
\subsection{Direzione di massima crescita}
\underline{Problema}: trovare la direzione di massima crescita di una funzione $ f $ in un assegnato punto.\\
Per rispondere a questa domanda, dobbiamo trovare il vettore unitario $ v $ la cui derivata direzionale e' massima, ovvero dobbiamo massimizzare la funzione $ <\nabla f(\overline{x}, \overline{y}), (v_1,v_2)> = \partial_x f(\overline{x}, \overline{y}) v_1 + \partial_yf(\overline{x}, \overline{y}) v_2 $. Riscrivendo il gradiente e $ v $ usando coordinate polari, otteniamo $ <r(cos\theta, sin\theta), (cos\gamma, sin\gamma)> = r(cos\theta cos\gamma+sin\theta sin\gamma) $, che per la formula del coseno di una differenza diventa $ rcos(\theta - \gamma) $, che raggiunge il suo valore massimo $ r $ quando $ \theta = \gamma $. Questo vuol dire che la direzione di massima crescita in un punto e' quella dove giace il vettore gradiente, e il valore di questa crescita e' dato dal modulo dello stesso vettore. Possiamo generalizzare questa scoperta in $ n $ dimensioni (usando la disuguaglianza di Cauchy-Schwarz \ref{cauchySchwarz}):
\mprop{}{
  Sia $ f:\mathbb{R}^n\to\mathbb{R} $ differenziabile in $ x \in \mathbb{R}^n $ e sia $ \nabla f(x) \neq \underline{0} $, allora:
  \begin{itemize}
    \item $ v_{\text{max}} = \frac{\nabla f(x)}{|\nabla f(x)|} $
    \item $ \frac{\partial f}{\partial v}(x) = |\nabla f(x)| $
  \end{itemize}
}

\section{Curve in $ \mathbb{R}^n $}
Una curva in $ \mathbb{R}^n $ e' una "funzione" (non sempre) $ r $ (oppure $ s, \gamma $):
\[
  r: (a,b)\to\mathbb{R}^n
\]
Che prende quindi un numero reale e ci da un punto in $ \mathbb{R}^n $, ed e' definita da un parametro $ t \in (a,b) $, quindi si puo' scrivere come $ r(t) = (r_1(t),...,r_n(t)) $. Esempio:
\[
  r:\begin{cases}
  x=t & \\
  y=2-t & \\
  z=2t & 
  \end{cases} \text{ equazione parametrica di retta in $ \mathbb{R}^3 $}
\]
Oppure come intersezione di due piani:
\[
r = \pi + \sigma \implies \begin{cases}
y=2-x & \\
z = 2x & 
\end{cases} \implies \begin{cases}
x+y-2=0 & \\
2x-z=0 & 
\end{cases}
\]
\subsection{Derivata di una curva}
Una curva e' una "funzione" di classe $ C^1((a,b)) $, quindi esistono tutte le derivate prime e sono continue (in $ (a,b) $). Il significato di derivata e' sempre lo stesso: e' il cambiamento del valore della funzione quando si incrementa l'input di un valore infinitesimo. Vediamo graficamente cosa succede (con $ r:\mathbb{R}\to\mathbb{R}^2 $ per semplicita'):
\begin{center}
    \includegraphics[width=0.5\textwidth]{ img/2024-05-05-15-17-31.png}
\end{center}
Vediamo che la differenza fra i valori di $ r $ e' rappresentata da un vettore $ r(t+h)-r(t) $, che puo' essere scomposto come la somma del cambiamento rispetto alle direzioni della base ortonormale di riferimento, in questo caso le direzioni delle assi:\\
\begin{center}
\includegraphics[width=0.25\textwidth]{img/2024-05-05-15-54-02.png}
\end{center}

Quindi possiamo applicare la definizione di limite e calcolare il rapporto incrementale in questo modo:
\[
  \lim_{h \to 0}\frac{r(t+h)-r(t)}{h} = \lim_{h\to 0}\frac{r_1(t+h)-r_1(t)}{h} + \lim_{h \to 0}\frac{r_2(t+h)-r_2(t)}{h}
\]
Ovvero, la derivata un punto $ t $ di una curva $ r:\mathbb{R}\to\mathbb{R}^n $ e' data dalla somma delle derivate delle $ n $ funzioni parametriche di $ r $. Quindi diamo la definizione:
\dfn{Velocita di una curva}{
  La velocita' (o derivata) di una curva e' il vettore tangente alla curva data:
  \[
    r'(t) = (r_1'(t),...,r_n'(t))
  \]
}
Quindi la direzione del vettore velocita' e' tangente alla curva nel punto dato, ma la sua lunghezza cosa indica?\\
Prendiamo due curve $ r(t) = (t, t^2),r' = (2t, 4t^2) $ e consideriamo il parametro $ t $ come il tempo trascorso. Notiamo che le due curve tracciate sono identiche, ma che una ci mette la meta' del tempo per percorrerla rispetto all'altra:
\begin{center}
  \includegraphics[width=0.2\textwidth]{img/2024-05-05-17-05-56.png}
  \includegraphics[width=0.2\textwidth]{img/2024-05-05-17-02-33.png}
\end{center}
Inoltre, la lunghezza del vettore velocita' nello stesso punto sulla curva e' doppia nella funzione piu' "veloce". Quindi possiamo intuire che la norma della derivata ci indica quanto "velocemente" ci stiamo muovendo nel punto dato della curva, chiamata anche \textbf{velocita' instantanea}:
\dfn{Velocita' scalare (instantanea)}{
  Non e' altro che la norma del vettore tangente, cioe':
  \[
    ||v(t)|| = ||r'(t)|| = \sqrt{(r_1'(t))^2+...+(r_n'(t))^2}
  \]
}
\subsection{Altre definizioni}
\dfn{Curva regolare}{
  Una curva e' detta \textbf{regolare} in $ t_0 \in (a,b) $ se:
  \[
    r'(t_0) \neq \underline{0}
  \]
}
\nt{
  Una curva regolare $ r:(a,b)\to\mathbb{R}^n $ e' anche detta essere una \textbf{immersione} di $ (a,b) $ in $ \mathbb{R}^n $.}
\dfn{Punto singolare}{
  Se $ t_0 \in (a,b) $ e' tale per cui
  \[
    r'(t_0) = \underline{0}
  \]
  Allora $ t_0 $ e' chiamato \textbf{punto singolare}
}
\dfn{Curve semplici}{
  Una curva e' detta \textbf{semplice} se e' iniettiva. (Cioe' la curva non si interseca)
}
\subsection{Curvatura}
Nelle curve almeno $ C^2 $, si definiscono:
\dfn{Curvatura}{
  Si definisce \textbf{curvatura} di $ r:(a,b)\to\mathbb{R}^n $ il modulo della accelerazione
  \[
    k = ||a(t)|| = ||v'(t)|| = |||r''(t)|
  \]
}
\dfn{Raggio di curvatura}{
  Si chiama \textbf{raggio di curvatura}
  \[
  R = \frac{1}{k}
  \]
  Bisogna considerarlo nella retta reale estesa $ \overline{\mathbb{R}} = [-\infty,+\infty] $, dato che le rette hanno il raggio di curvatura e ha senso che questo sia $ \infty $
}
\subsection{Derivata di una funzione scalare lungo una curva}
Prendiamo una funzione scalare $ f:\mathbb{R}^2\to\mathbb{R} $, definita come $ f(x,y) = x^2y $. Ora al posto delle due variabili di input $ x $ e $ y $ mettiamo due funzioni $ x(t)=cos(t) $ e $ y(t)=sin(t) $. Abbiamo ottenuto ora una funzione che dipende da un solo parametro $ (t) $, quindi e' possibile calcolare la sua derivata rispetto a questo parametro. Nel nostro esempio, possiamo espandere la nostra funzione in questo modo:
\[
  f(cos(t), sin(t)) = cos^2(t)sin(t)
\]
Quindi per calcolare la sua derivata basta applicare le classiche regole di derivazione:
\[
  \frac{d}{dt}f(cos(t), sin(t)) = -2cos(t)sin^2(t) + cos^3(t)
\]
Ma esiste un modo piu' generale per descrivere questo procedimento? Possiamo iniziare considerando una curva $ r:\mathbb{R}\to\mathbb{R}^2 $, che per seguire l'esempio sopra puo' essere definito come $ r(t) = (cos(t), sin(t)) $. Quindi ora possiamo usare il vettore dato dalla curva $ r $ come input alla nostra funzione scalare $ f $, creando quindi una composizione $ f \circ r: \mathbb{R}\to\mathbb{R} $ di cui vogliamo studiare la derivata. Analizziamo la situazione:
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-05-18-09-45.png}
\end{center}
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-05-18-18-30.png}
\end{center}
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-05-18-08-01.png}
\end{center}
Le tre figure ci mostrano, dall'alto verso il basso, come si propaga un aumento (infinitesimo) del parametro iniziale $ dt $, prima nella curva $ r $, poi dal valore spostato di $ r $ in $ f $. Alla fine otteniamo lo spostamento totale, chiamato $ df $ che ora vediamo come calcolare:
\begin{itemize}
  \item Prima di tutto, dobbiamo calcolare il valore del vettore $ r(t+h)-r(t) $, ovvero dobbiamo trovare lo "spostamento" causato da un aumento infinitesimo del parametro di input di $ r $. Se conosciamo la derivata della curva in quel punto, basta moltiplicarla per $ dt $ e otteniamo il vettore $ dr $:
    \[
      dr = r'(t)dt = (\frac{dx}{dt}dt, \frac{dy}{dt}dt)
    \]
  \item Ora che abbiamo il cambiamento in $ r $, dobbiamo vedere come calcolare il cambiamento che questo causa in $ f $. Puo' essere utile considerare le due componenti di $ dr $, in modo da avere il delta del primo e del secondo parametro di $ f $ separatamente. In modo analogo al primo passo moltiplichiamo le derivate parziali con gli spostamenti dei parametri corrispondenti, sommandoli per trovare $ df $:
    \[
    df = \frac{\partial f}{\partial x}\frac{dx}{dt}dt + \frac{\partial f}{\partial y}\frac{dy}{dt}dt
    \]
  \item Ci rimane soltanto dividere l'equazione per $ dt $ per trovare la derivata:
    \[
    \frac{df}{dt} = \frac{\partial f}{\partial x}\frac{dx}{dt}+\frac{\partial f}{\partial y}\frac{dy}{dt}
    \]
    Che puo' essere riscritta come un prodotto scalare:
    \[
      \frac{df}{dt} = \innerproduct{\nabla f(r(t))}{r'(t)}
    \]
\end{itemize}
\thm{}{
  Sia $ f:\mathbb{R}^n\to\mathbb{R} $ differenziabile, allora data una curva $ r:(a,b)\to\mathbb{R}^n $ (derivabile) si ha che:
  \[
    f \circ r:(a,b)\to\mathbb{R}
  \]
  E' derivabile, e la sua derivata e' la \textbf{derivata lungo la curva} $ r $, e si ha:
  \[
    (f \circ r)'(t)= <\nabla f(r(t)), r'(t)>
  \]
}
Prima di dare una dimostrazione formale di questo teorema, dobbiamo definire lo sviluppo di Taylor al primo ordine di una curva:
\dfn{}{
  Data $ r:]a,b[\to\mathbb{R}^n $ derivabile, sia $ k \in \{1,...,n\} $. Lo sviluppo di Taylor per $ r_k:]a,b[\to\mathbb{R} $ in $ t \in ]a,b[ $ e':
  \[
    r_k(t+s) = r_k(t) + r_k'(t)s + o_k(s)
  \]
  Quindi $ r(t+s) = r(t) + r'(t)s + o(s) $ (dove $ o(s) = (o_1(s),...,o_n(s)) $). 
}
\pf{Dimostrazione formale}{ 
  Usando la definizione di derivata, $ (f \circ r)'(t) = \lim_{s\to 0} \frac{f(r(t+s)) - f(r(t))}{s} $. Dato che $ f $ e' differenziabile, possiamo sostituire il numeratore con $ \innerproduct{f'(r(t))}{r(t+s)-r(t)} + o(|r(t+s)-r(t)|) $. Essendo derivabile, possiamo usare lo sviluppo di Taylor al primo ordine della curva $ r $ per sostituire $ r(t+s)-r(t) $ con $ r'(t)s + o(s) $. Usando la proprieta' distributiva e raccogliendo il fattore $ s $, il limite diventa $ \innerproduct{f(r(t))}{r'(t)} + \lim_{s\to 0}\frac{\innerproduct{f(r(t))}{o(s)}+o(|r'(t)s+o(s)|)}{s} $. Dividendo la frazione, nel primo addendo possiamo spostare il divisore all'interno del prodotto scalare che diventa $ \innerproduct{f(r(t))}{\frac{o(s)}{s}} $, che per definizione di o-piccolo diventa nullo. Per il secondo addendo, con $ s\to 0 $ abbiamo che $ o(|r'(t)s+o(s)|) = o(s) $, quindi sempre per definizione di o-piccolo anche il secondo addendo si annulla e il limite si azzera. 
}
\subsection{Ortogonalita' gradiente-insiemi di livello}
Possiamo utilizzare questo nuovo teorema per dimostrare la relazione di ortogonalita' fra il gradiente di una funzione calcolata in un punto e la curva di livello che passa per quel punto.\\
Come esempio, prendiamo una funzione $ f:\mathbb{R}^2\to\mathbb{R} $ e tracciamone delle curve di livello:
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-05-19-15-56.png}
\end{center}
Pensiamo ora di "zoomare" sul punto $ A $ e consideriamo la prossima curva di livello. Se ci avviciniamo abbastanza, possiamo considerare le due sezioni di curva come due linee parallele, quindi il segmento piu' corto che parte dal punto $ A $ e arriva alla prossima curva di livello e' quello perpendicolare alla curva di livello di $ A $:
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-05-19-33-43.png}
\end{center}
Dato che la direzione perpendicolare alla curva di livello in $ A $ coincide con la direzione che ci porta piu' velocemente alla prossima curva di livello (direzione di massima crescita), possiamo intuire che il vettore gradiente in $ A $ e' perpendicolare alla curva che passa per $ A $:
\mprop{}{
  Data una funzione $ f:\mathbb{R}^n\to\mathbb{R} $ differenziabile, si ha che:
  \[
    \forall \underline{x} \in \mathbb{R}^n. \nabla f(\underline{x}) \perp L_{f(\underline{x})}
  \]
  Dove $ L_{f(\underline{x})} $ e' la curva di livello $ \{x \in \mathbb{R}^n|f(x)=f(\underline{x})\} $
}
\pf{Dimostrazione formale}{
  Assumiamo di avere una funzione $ f:\mathbb{R}^n\to\mathbb{R} $ differenziabile e un punto $ \overline{x} \in \mathbb{R}^n $. Supponiamo di poter costruire una curva $ r: ]-1,1[\to\mathbb{R}^n $ tale che: $ \forall t \in ]-1,1[. r(t) \in L_b $, $ r(0) = \overline{x} $ e $ r'(0) \in \mathbb{R}^n \neq \underline{0} $ (direzione tangente a $ L_{f(\overline{x})} $ nel punto $ \overline{x} $). Dato che tutti i punti della curva $ r $ fanno parte dello stesso insieme di livello, $ \forall t \in ]-1,1[. (f\circ r)(t) = f(\overline{x}) $. Essendo una funzione costante, sul suo dominio la derivata e' nulla, ovvero $ \forall t \in ]-1,1[. (f\circ r)'(t) = 0 $, che puo' essere scritta come $ \innerproduct{\nabla f(r(t))}{r'(t)} = 0 $ (usando il teorema di derivata lungo una curva). In altre parole, sappiamo che il prodotto scalare fra il gradiente di $ f $ in un punto e la derivata della curva di livello nello stesso punto e' sempre nullo. Quindi il gradiente in $ \overline{x} $ e' ortogonale alla tangente della curva di livello in $ \overline{x} $.
}
\chapter{Punti di massimo e minimo in piu' dimensioni}
La definizione di punti di massimo/minimo in piu' dimensioni e' molto simile a quella in $ \mathbb{R} $:
\dfn{Maxima e minima}{
  Data $ f:A\to\mathbb{R}^n $, con $ A \subseteq \mathbb{R} $, $ \underline{x} \in A $ si dice punto di massimo/minimo se:
  \[
    \exists \delta > 0. \forall x \in D(\underline{x},\delta) \cup A. f(x) \geq (\leq) f(\underline{x})
  \]
}
Come facciamo a trovare questi punti? Guardiamo i metodi usati in una dimensione per vedere se sono generalizzabili:
\begin{itemize}
  \item Studio del segno della derivata prima: se $ f(\underline{x}) = 0 $ e in un intorno di $ \underline{x} $ si ha che $ f(x) > (<) 0 $, allora $ \underline{x} $ e' un punto di massimo (minimo)
  \item Utilizziamo il segno della derivata seconda per trovare la concavita' del grafico nei punti di stazionamento:
    \[
    \begin{cases}
      f'(\underline{x}) = 0& \\
      f''(\underline{x}) > (<) 0 & 
    \end{cases} \iff \underline{x} \text{ e' punto di massimo (minimo)} 
    \]
\end{itemize}
Quando $ n \geq 2 $, $ f'(x) $ viene sostituito dal gradiente $ \nabla f(x) $, che e' un vettore e non uno scalare. Dato che non ha senso chiedersi se un vettore e' $ \geq 0 $,  il concetto di "crescente" e "decrescente" non puo' essere applicato in piu' dimensioni (perche' puo' essere diversa in direzioni diverse). Quindi dobbiamo utilizzare il secondo metodo, che ci da' due condizioni da rispettare (una sulla derivata di primo grado, una sulla seconda). Vediamo come generalizzare queste condizioni in piu' dimensioni:
\section{Condizioni del primo ordine}
Guardiamo la prima condizione relativa alla derivata di primo grado. Questa ci dice che e' condizione necessaria (ma non sufficente) per un punto di massimo/minimo avere la derivata prima nulla, vediamo se cio' vale anche in piu' dimensioni:
\thm{Fermat}{
  Data $ f:A\to\mathbb{R} $, con $ A \subseteq \mathbb{R}^n $ aperto, si ha che se un punto $ \underline{x} \in A $ e' massimo o minimo, allora:
  \[ 
    \nabla f(\underline{x}) = 0
  \]
}
\pf{Dimostrazione}{
  Presa una funzione $ f:A\to\mathbb{R} $ ($ A \subseteq \mathbb{R}^n $ aperto) con punto minimo in $ \overline{x} $, devo dimostrare che le derivate parziali in quel punto siano nulle. Semplicemente basta guardare la funzione sui piani paralleli alle direzioni canoniche passanti per $ \overline{x} $ e usare Fermat per le funzioni semplici. Per fare cio' creiamo una funzione $ h_k:]-\delta, \delta[\to\mathbb{R} $, definita $ \forall k \in \{1,...,n\} $ come $ h_k(t) = f(\overline{x} + te_k) $. Il $ \delta $ scelto e' quello garantito dalla definizione di punto minimo, quindi si ha che $ \forall x  $ che dista meno di $ \delta $ da $ \overline{x} $ si ha che $ f(x) \leq (\geq) f(\overline{x}) $. Quindi $ h $ ha max/min con $ t=0 $, ed essendo una funzione semplice abbiamo che $ h_k'(0) = 0 $. Ora dimostriamo che cio' implica che tutte le derivate parziali sono nulle: $ h_k'(0)=\lim_{t\to 0}\frac{h_k(t)+h_k(0)}{t} = \lim_{t\to 0}\frac{f(\overline{x}+te_k)+f(\overline{x})}{t} = \frac{\partial f}{\partial x_k}(\overline{x}) = 0 $.
}
Pero' questa e' solo una condizione necessaria, non sufficente per essere un punto di massimo/minimo. Vedremo infatti che esistono dei punti "di sella" che sono stazionari ma che non sono ne massimi ne minimi (sono una generalizzazione dei punti di flesso a tg orizzontale). Per distinguere i tipi di punti stazionari, dobbiamo guardare la derivata seconda:
\section{Condizioni di secondo ordine}
\subsection{In una variabile}
Diamo un attimo una intuizione al perche' la condizione di secondo ordine funziona in una variabile. Per fare cio', prendiamo una funzione $ f $ e scriviamone il polinomio di Taylor nel punto stazionario $ x_0 $ fino al secondo grado (dato che stiamo facendo solo un ragionamento informale, l'errore possiamo anche tralasciarlo):
\[
   f(x) \approx f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 
\]
Dato che siamo interessati ai punti stazionari, $ f'(x_0) = 0 $, quindi possiamo semplificare l'approssimazione cosi':
\[
  f(x) \approx f(x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2
\]
Dato che questa e' un'approssimazione di un piccolo intorno centrato nel punto $ x_0 $, se vediamo che $ x_0 $ e' un max/min nell'approssimazione, allora significa che nella funzione effettiva $ f $ esiste un intervallo contenente $ x_0 $ dove questo e' maggiore/minore di tutti i punti dell'intervallo. Guardiamo i vari casi:
\begin{itemize}
  \item Se $ f''(x_0) > 0 $, allora $ x_0 $ e' un punto di minimo, dato che $ (x-x_0)^2 $ e' sempre positivo tranne quando $ x = x_0 $, che ci darebbe il valore del minimo $ f(x_0) \approx f(x_0) + 0 $.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{img/2024-05-10-12-37-28.png}
    \end{center}
  \item Se $ f''(x_0) < 0 $, allora $ x_0 $ e' un punto di massimo, dato che siamo nella situazione precedente ma $ (x-x_0)^2 $ e' moltiplicato per una costante negativa. Quindi il polinomio descrive una parabola "all'ingiu'", con vertice in $ x_0 $.
    \begin{center}
      \includegraphics[width=0.5\textwidth]{img/2024-05-10-12-38-04.png}
    \end{center}
  \item Se $ f''(x_0) = 0 $, allora siamo in un caso particolare dove la derivata seconda non riesce a darci informazioni aggiuntive rispetto alla natura del punto stazionario.
  \begin{center}
    \includegraphics[width=0.5\textwidth]{img/2024-05-10-12-38-41.png}
  \end{center}
\end{itemize}
Quindi, per riassumere: quando $ f'(x_0) = 0 $, sappiamo che se il termine quadratico dell'approssimazione di Taylor $ \frac{1}{2}f''(x_0)(x-x_0)^2 $ e' sempre positivo/negativo (includendo lo 0), allora $ x_0 $ e' un punto di massimo/minimo.

\section{Approssimazione quadratica in piu' dimensioni}
Proviamo a seguire lo stesso procedimento per funzioni in una sola variabile. La prima cosa da fare e' scrivere un'approssimazione quadratica (Taylor al secondo ordine) di una funzione in piu' dimensioni. Partiamo prima con la definizione delle derivate seconde in n dimensioni:
\subsection{Derivate seconde e hessiana}
Introduciamo le derivate seconde in n dimensioni, ovvero il calcolo due volte di fila delle derivate (parziali). La differenza questa volta e' che ci sono diverse derivate parziali, che quindi possiamo applicare in sequenza in $ n^2 $ modi diversi. Introduciamo la notazione:
\dfn{Derivate seconde in n dimensioni}{
  Sia $ f:\mathbb{R}^n\to\mathbb{R} $, assumiamo che $ \forall k \in \{1,...,n\}. \exists \frac{\partial f}{\partial x_k} $ in ogni punto di $ \mathbb{R}^n $. Consideriamo una nuova variabile $ x_j $, con $ j \in \{1,...,n\} $, si ha che:
  \begin{itemize}
    \item Se $ j \neq k $ (ovvero se stiamo applicando due derivate parziali diverse) scriviamo:
      \[
        \frac{\partial }{\partial x_j}(\frac{\partial f}{\partial x_k}) = \frac{\partial^2 f}{\partial x_j \partial x_k}
      \]
    \item Nel caso $ j = k $ (applichiamo due volte la stessa derivata parziale) scriviamo:
      \[
        \frac{\partial }{\partial x_j}(\frac{\partial f}{\partial x_k}) = \frac{\partial^2 f}{\partial x_k^2}
      \]
  \end{itemize} 
}
Seppure sembrano tante, in verita' si puo' dimostrare che l'ordine in cui applichiamo le derivate non cambia il valore finale:
\thm{Schwarz}{
  Sia $ f:\mathbb{R}^n\to\mathbb{R} $ tale che per qualche (almeno uno) intorno centrato in $ p \in \mathbb{R}^n $ la derivata seconda (non mista!) di $ f $ e' continua, allora:
  \[
   \forall k,j \in \{1,...,n\}. \frac{\partial^2 f}{\partial x_k \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_k}
  \]
}
Come abbiamo visto sopra, con $ n $ derivate parziali diverse esistono $ n^2 $ combinazioni di derivate seconde. Queste derivate seconde le possiamo mettere in una matrice (detta "hessiana") quadrata $ Hf(\overline{x}) \in M_n(\mathbb{R}) $ definita cosi':
\[
  Hf(\overline{x})_{jk} = \frac{\partial^2 f}{\partial x_j \partial x_k }
\]
\ex{}{
  Data una funzione in due dimensioni $ f:\mathbb{R}^2\to\mathbb{R} $, la sua matrice hessiana associata nel punto $ x_0 $ e':
  \[
    H_{f(x_0)} = \begin{pmatrix}
    \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
    \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}\\
    \end{pmatrix}
  \]
  Ho omesso l'esplicitazione del punto da derivare nella matrice perche' spesso si fa cosi'.
}
\subsection{Taylor secondo Lagrange}
Torniamo un attimo in una dimensione per dare una nuova formula di approssimazione, che come noteremo da' informazioni aggiuntive rispetto a Taylor con il resto di Peano:
\thm{Taylor secondo Lagrange}{
  (n=1) $ f:\mathbb{R}\to\mathbb{R} $ assumo $ f,f',f'' $ continue. Dati $ \overline{x} $ e $ \overline{x} +h \in \mathbb{R} $, esiste $ \theta \in (0,1) $.
  \[
    f(\overline{x}+h) = f(\overline{x}) + f'(\overline{x})h + \frac{1}{2}f''(\overline{x}+\theta h)h^2
  \]
}
\nt{
  $ \overline{x} + \theta h $ appartiene all'intervallo di estremi $ \overline{x} $ e $ \overline{x} + h $ (aperto)
}
\nt{
  Ricordiamo la formula nota: $ f(\overline{x} + h) = f(\overline{x}) = f'(\overline{x})h+\frac{1}{2}f''(\overline{x})h^2 + o(h^2) $ per $ h \to 0 $. Questa perde significato se $ h $ non tende a $ 0 $, mentre quella di Lagrange vale per punti qualsiasi.
}
\nt{
  Osserviamo che dalla formula di Lagrange possiamo ricondurci a quella sopra, quindi possiamo dire che contiene piu' "informazioni". Infatti sottraendo la seconda dalla prima rimane $ \frac{h^2}{2}(f''(\overline{x})-f''(\overline{x}+\theta h)) = o(h^2) $. Per verificarlo dividiamo la parte sinistra per $ h^2 $ e facciamo il limite per $ h \to 0 $, e si nota che effettivamente il limite e' $ 0 $ (usando la continuita' di $ f'' $). 
}
(decidere se ha senso fare prima la forma quadratica e poi scrivere taylor in n dimensioni o se ci possiamo arrivare prima alla formula senza abbreviarla e poi fare la forma quadratica per scriverla meglio)
\subsection{Forma quadratica}
Per classificare i punti stazionari dobbiamo analizzare la parte quadratica dello sviluppo di Taylor, che come poi dimostreremo ha la seguente forma:
\[
ax^2 + 2bxy + cy^2
\]
Un'equazione di questo tipo e' chiamata una \textbf{forma quadratica}, che puo' essere rappresentata anche tramite il prodotto scalare fra una matrice e due vettori, che e' utile in quanto una sola scrittura vale per qualsiasi numero di variabili. Vediamo che in questo modo l'equazione diventa:
\[
  \innerproduct{\begin{pmatrix}
  a & b\\
  b & c\\
  \end{pmatrix}\begin{pmatrix}
  x\\
  y\\
  \end{pmatrix}}{\begin{pmatrix}
  x\\
  y\\
  \end{pmatrix}}
\]
Notiamo che la matrice associata alle forme quadratiche e' sempre una matrice simmetrica. Vediamo ora la definizione di forma quadratica usando questa nuova scrittura:

\dfn{Forma quadratica}{
  Sia $ A \in M_n(\mathbb{R}) $ simmetrica ($ A = A^T $) e $ h \in \mathbb{R}^n $ (vettore colonna $ n\times 1 $), definiamo la funzione $ q_A: \mathbb{R}^n\to\mathbb{R} $:
  \[
    q_A(h) = \innerproduct{Ah}{h} \in \mathbb{R} 
  \] 
}
\nt{
  $ p(h) = 3h_1^2 + \frac{1}{3}h_1h_2 - 7h_2^2 $, quindi $ p = q_A $ con $ A = \begin{pmatrix}
  3 & 1/6\\
  1/6 & -7\\
  \end{pmatrix} $
}
\subsubsection{Segno}
Per generalizzare la condizione $ f''(\overline{x})> 0 $ in $ \mathbb{R} $ dobbiamo dare una nozione di segno di una forma quadratica:
\dfn{}{
  Sia $ A = A^T \in M_n(\mathbb{R}) $. Allora si dice che:
  \begin{itemize}
  \item $ A $ e' definita positiva se $ \innerproduct{Ah}{h} > 0 $ $ \forall h \neq 0 $
  \item $ A  $ e' definita negativa se $ \innerproduct{Ah}{h} < 0 $ $ \forall h \neq 0 $
  \item $ A $ e' indefinita se $ \exists h, t \in \mathbb{R}^n. \innerproduct{Ah}{h} < 0 < \innerproduct{At}{t} $
  \end{itemize}
}
\nt{
  Tutte le disuguaglianze sono strette
}
Per ora restringiamoci a solo due variabili. E' possibile sapere il segno di una forma quadratica conoscendo solo la sua matrice? Si, si puo' dimostrare trattando inizialmente $ y $ come una costante, chiamiamola $ y_0 $, e calcolando gli zeri:
\[
  y_0\left(\frac{-b \pm \sqrt{b^2-ac}}{a}\right)
\]
Vediamo subito che se $ y_0 = 0 $, allora abbiamo entrambe gli zeri nel punto $ (0,0) $. Notiamo inoltre che il segno della parabola dipende interamente dal segno del meno-delta (studiamo $ ac-b^2 $ per motivi che vedremo dopo) e dal segno di $ a $:
\begin{itemize}
\item se $ ac-b^2 < 0 $, allora esistono due zeri distinti $ \forall y_0 \neq 0 $ e la parabola e' sia negativa che positiva, quindi in questo caso la forma quadratica e' indefinita
  \item se $ ac-b^2 > 0 $, non esistono zeri reali e quindi la parabola e' tutta positiva se $ a>0 $ o tutta negativa se $ a < 0 $. Quindi il segno della forma quadratica dipende dal segno di $ a $.
    \item se $ ac-b^2 = 0 $ la parabola sara' sempre o positiva o negativa a seconda del segno di $ a $ in teoria ma in realta non e' definita? Why?

\end{itemize}
Il valore $ ac-b^2 $ in realta' corrisponde al determinante della matrice associata alla forma quadratica, quindi possiamo riscrivere i casi sopra in modo piu' breve:
\mprop{Regola dei segni delle forme quadratiche}{
  Sia $ A = \begin{pmatrix}
  a & b\\
  b & c\\
  \end{pmatrix} $. Allora:
  \begin{itemize}
  \item $ A > 0 \iff \begin{cases}
  a > 0 & \\
  detA > 0 & 
  \end{cases} $
    \item $ A < 0 \iff \begin{cases}
    a < 0 & \\
    detA > 0 & 
    \end{cases} $
    \item $ A $ e' indefinita $ \iff $ $ detA < 0 $ (punto di sella)
  \end{itemize}
}

\subsubsection{Forma quadratica Hessiana}
Essendo una matrice simmetrica, anche la matrice hessiana che abbiamo definito insieme alle derivate seconde in $ n $ dimensioni ha una forma quadratica associata:
\[
  q(h) = \innerproduct{H_{f(\overline{x})}h}{h}
\]
dove $ H_{f(\overline{x})} $ e la matrice hessiana di ordine $ n $ e $ h $ e' un qualunque vettore di $ \mathbb{R}^n $.

\subsection{Taylor di secondo ordine in n dimensioni}
\thm{Formula di Taylor con resto di Lagrange di secondo ordine in n dimensioni}{
  Data una funzione $ f:\mathbb{R}^n\to\mathbb{R} $ con tutte le derivate seconde continue, si ha che $ \forall x_0, h \in \mathbb{R}^n $, $ \exists \delta \in (0,1) $ tale che:
  \[
    f(x_0 + h) = f(x_0) + \innerproduct{\nabla f(x_0)}{h} + \frac{1}{2} \innerproduct{H_{f(x_0 + \delta h)}h}{h}
  \]  
}
\nt{
  Come avevamo visto in una dimensione, questa formula e' "globale", ovvero vale anche per $ h $ "lontani" da $ x_0 $.
}
\pf{}{
  Usiamo una funzione $ h: \mathbb{R}\to\mathbb{R} $ che ha derivata seconda continua (?). Per Taylor in una variabile sappiamo che: $ \forall t_0, t_0+t \in \mathbb{R}. \exists \theta \in ]0,1[: h(t_0+t) = h(t_0) + h'(t_0)t + \frac{1}{2}h''(t_0+\theta t)t^2 $. Prendendo come $ f $ la funzione del teorema e dati $ x_0, x_0+h \in \mathbb{R}^n $, definisco la funzione $ h(t) = f(x_0+th) $ (stare attenti a non confondere $ h $-funzione con $ h $-variabile). Scriviamo Taylor di $ h $ ponendo $ t_0 = 0 $ e $ t = 1 $: $ h(1) = h(0) + h'(0) + \frac{1}{2}h''(\theta) $. Usando la definizione di $ h(t) $, otteniamo $ h(1) = f(x_0 + h) $, $ h(0) = f(x_0) $, $ h'(0) = \innerproduct{\nabla f(x_0)}{h} $ (derivata di f lungo la curva $ v(t) = x_0+th $) e $ h''(\theta) = \innerproduct{H_{f(x_0+\theta h)h}}{h} $. Sostituendo otteniamo la tesi del teorema. 
}
\cor{Taylor di secondo ordine in n dimensioni con resto di Peano}{
  Data una funzione $ f:\mathbb{R}^n\to\mathbb{R} $ con tutte le derivate seconde continue, si ha che $ \forall x_0,h \in \mathbb{R}^n $ si ha che:
  \[
    f(x_0 + h) = f(x_0) + \innerproduct{\nabla f(x_0)}{h} + \frac{1}{2} \innerproduct{H_{f(x_0)}h}{h} + o(\norm{h}^2)
  \]
  Dove $ \forall \epsilon > 0. \exists \delta > 0. \forall 0 < \norm{h} < \delta \implies \left|\frac{o(\norm{h}^2)}{\norm{h}^2}\right| < \epsilon $
}
\pf{}{
  Scriviamo Taylor nelle due versioni:
  \[
    f(x_0+h) = f(x_0) + \innerproduct{\nabla f(x_0)}{h} + \frac{1}{2} \innerproduct{H_{f(x_0 + \theta h)}h}{h}
  \]
  \[
    f(x_0+h) = f(x_0) + \innerproduct{\nabla f(x_0)}{h} + \frac{1}{2} \innerproduct{H_{f(x_0)}h}{h} + o(|h|^2)
  \]
  Sostituiamo il membro a sinistra del secondo con il membro di destra del primo, e semplificando otteniamo:
  \[
    \frac{1}{2} \innerproduct{H_{f(x_0 + \theta h)}h}{h} - \frac{1}{2}\innerproduct{H_{f(x_0)}h}{h} = o(|h|^2)
  \]
  Moltiplichiamo per 2 e scriviamo le forme quadratiche per esteso raccogliendo i termini con le stesse derivate seconde. Otteniamo una somma di tre forme di questo tipo (dove $ j,k \in \{1,...,n\} $):
  \[
    h_jh_k(\partial_{jk}f(x_0+\theta h)-\partial_{jk}f(x_0))
  \]
  Dobbiamo mostrare che questa somma sia un o-piccolo di $ |h|^2 $, quindi basta controllare che $ \forall j,k \in \{,...,n\} $:
  \[
    \lim_{h\to(0,0)} \frac{h_jh_k}{|h|^2}(\partial_{jk}f(x_0+\theta h)-\partial f(x_0)) = 0
  \]
  Per analisi asintotica, $ \frac{h_jh_k}{|h|^2} $ tende a un numero reale, mentre il fattore a destra tende a 0.
}

\section{Classificazione di punti stazionari}
Diamo prima di tutto una definizione formale di punto di sella:
\dfn{Punto di sella}{
  Data una funzione $ f:\mathbb{R}^n\to\mathbb{R} $ con derivate seconde continue, il punto critico $ \overline{x} \in \mathbb{R}^n $ ($ \nabla f(\overline{x}) = \underline{0} $) si dice di \textbf{sella} se:
  \[
    \forall \delta > 0. \exists \overline{x}^-,\overline{x}^+.f(\overline{x}^-) < f(\overline{x}) < f(\overline{x}^+)
  \]
}
Ci serve anche una proprieta' delle forme quadratiche:
\mprop{}{ \label{valoreMinimo}
  Sia $ A = A^T $ definita positiva di ordine $ n $, allora $ \exists \lambda \in \mathbb{R} > 0 $ tale che $ \forall h \in \mathbb{R}^n $:
  \[
  \innerproduct{Ah}{h} \geq \lambda \norm{h}
  \]
  Possiamo anche scrivere $ \innerproduct{A\frac{h}{\norm{h}}}{\frac{h}{\norm{h}}} \geq \lambda $.
}
\pf{Dimostrazione in $ \mathbb{R}^2 $}{
  Prendiamo una matrice simmetrica di ordine 2 $ A $ definita positiva, con $ a,b,c \in \mathbb{R} $. Dobbiamo dimostrare che esiste un $ \lambda \in \mathbb{R} > 0 $ tale che per ogni vettore unitario $ v = (cos\theta, sin\theta) $ si ha che $ \innerproduct{Av}{v} \geq \lambda $. Chiamiamo $ g: [0,2\pi]\to\mathbb{R} $ la forma quadratica ottenuta. Essendo $ g $ continua su un intervallo chiuso possiamo applicare Weierstrass e dire che esiste un minimo assoluto, ovvero $ \exists \overline{\theta} \in [0,2\pi].\forall \theta \in [0,2\pi]: g(\theta) \geq g(\overline{\theta}) $. Inoltre, dato che $ g $ e' definita positiva, $ \forall \theta \in [0,2\pi]. g(\theta) > 0 $, quindi $ g(\overline{\theta}) $ e' strettamente positiva ed equivale al valore $ \lambda $ cercato.
}
\nt{
  $ \lambda $ puo' essere scelto come l'autovalore minore di $ A $.
}
Ora siamo in grado di dimostrare come il segno della matrice hessiana puo' classificare un punto critico:
\thm{Classificazione dei punti critici}{
  Data una funzione $ f:\mathbb{R}^n\to\mathbb{R} $ con derivate seconde continue, sia $ \overline{x} \in \mathbb{R}^n $ un punto critico, allora:
  \begin{itemize}
    \item Se $ H_{f(\overline{x})} > 0 $, allora $ \overline{x} $ e' un punto di minimo
    \item Se $ H_{f(\overline{x})} < 0 $, allora $ \overline{x} $ e' un punto di massimo
    \item Se $ H_{f(\overline{x})}  $ e' indefinita, allora $ \overline{x} $ e' un punto di sella  
  \end{itemize}
}
\pf{}{
  \begin{itemize}
    \item $ 1) $ Per ipotesi sappiamo che $ \nabla f(\overline{x}) = 0 $ e che $ H_{f(\overline{x})} > 0 $. Dobbiamo dimostrare che $ \exists \delta \in \mathbb{R} > 0. \forall h \in \mathbb{R}^n.\norm{h} < \delta: f(\overline{x}+h) - f(\overline{x}) \geq 0 $ (definizione di minimo). Dato che $ f $ ha derivate seconde continue per ipotesi, possiamo scrivere il suo sviluppo di Taylor di secondo ordine: $ f(\overline{x}+h)-f(\overline{x}) = \frac{1}{2}\innerproduct{H_{f(\overline{x})}h}{h} + o(\norm{h}^2) $ (dato che il gradiente e' nullo). Per \ref{valoreMinimo} sappiamo che $ \innerproduct{H_{f(\overline{x})}h}{h} \geq \lambda \norm{h} $, quindi possiamo continuare l'equazione sopra aggiungendo $ \geq \frac{1}{2}\lambda \norm{h}^2 + o(\norm{h}^2) = \norm{h}^2 \left(\lambda+\frac{o(\norm{h}^2)}{\norm{h}^2}\right) $. Usiamo la definizione di o-piccolo, scegliendo come $ \epsilon $ il valore $ \frac{\lambda}{2} $ e attribuiamo al $ \delta $ che stiamo cercando il valore positivo per cui $ \forall h \in \mathbb{R}^n.\norm{h} < \delta: \left|\frac{o(\norm{h^2})}{\norm{h}^2}\right| < \frac{\lambda}{2} $. Controlliamo che $ \forall h . \norm{h} < \delta $ valga la proprieta' di minimo: $ \geq \frac{\norm{h}^2}{2}\left(\lambda - \frac{\lambda}{2}\right) \geq 0 $.
    \item $ 2) $ (Uguale a quello sopra)
    \item $ 3) $ 
  \end{itemize}
}


\chapter{Integrali su piu' variabili}
Vediamo ora come generalizzare il concetto di integrale definito in due dimensioni. Considero $ f: A\to\mathbb{R} $, $ A \subseteq \mathbb{R}^2 $, $ \forall (x,y) \in A. f(x,y) \geq 0 $, il nostro scopo e' quello di trovare il volume del sottografico rispetto all'area $ A $, ovvero dell'insieme di punti $ \{(x,y,z) \in \mathbb{R}^3 | (x,y) \in A, 0 \leq z \leq f(x,y)\} $.
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-13-14-46-04.png}
\end{center}
Si dimostra che, se $ A $ e' \textbf{opportuno} ed $ f $ e' continua, il colume del sottografico e' dato da:
\[
  \int_{A}^{} f(x,y)dxdy = \text{ integrale doppio}
\]
Se mettiamo $ f = 1 $ costante, si trova $ \int_{A}^{} 1dxdy $ che ha lo stesso valore dell' area di A (come $ \int_{a}^{b}1dx = b-a $ = lunghezza di $ [a,b] $).
\section{Insiemi semplici}
\dfn{Insieme y-semplice nel piano $ xy $}{
  Sia $ [a,b] \subseteq \mathbb{R} $, siano $ g_1,g_2:[a,b]\to\mathbb{R} $, tali che $ g_1(x) \leq g_2(x) \forall x \in [a,b] $ (con $ g_1,g_2 $ continue). 
  \[
    A = \{(x,y) \in \mathbb{R}^2 | x \in [a,b], g_1(x) \leq y \leq g_2(x)\}
  \]
}
Si chiama y-semplice perche' intersecandolo con una retta verticale otteniamo sempre un segmento continuo:
\begin{center}
  \includegraphics[width=0.35\textwidth]{img/2024-05-15-10-01-35.png}
\end{center}
Se abbiamo la stessa proprieta' ma per le rette orizzontali, allora sono insiemi x-semplici:
\dfn{Insieme x-semplice nel piano xy}{
  Sia $ [c,d] \subseteq \mathbb{R}_y $, siano $ h_1,h_2:[c,d]\to\mathbb{R} $, tali che $ h_1(y) \leq h_2(y) \forall y \in [c,d] $ (con $ h_1,h_2 $ continue). 
  \[
    A = \{(x,y) \in \mathbb{R}^2 | y \in [c,d], h_1(y) \leq x \leq h_2(y)\}
  \]
}
\begin{center}
  \includegraphics[width=0.35\textwidth]{img/2024-05-15-10-02-41.png}
\end{center}
\nt{
  Esistono insiemi che non sono ne' x, ne' y -semplici (corona circolare):
  \begin{center}
    \includegraphics[width=0.25\textwidth]{img/2024-05-15-10-03-29.png}
  \end{center}
}

\section{Formule di riduzione}
Vediamo ora come le proprieta' degli insiemi semplici possano aiutarci a calcolare il volume di un sottografo in tre dimensioni. L'obbiettivo principale e' quello di calcolare l'area sotto il grafico fissando la $ x $ o la $ y $ usando un integrale in una variabile, e poi di sommare tutte queste aree lungo la direzione opposta utilizzando ancora un integrale semplice. Vediamo come applicare questa intuizione formalmente:
\mprop{Riduzione y-semplice}{
  Siano $ g_1,g_2:[a,b]\to\mathbb{R} $ due funzioni continue t.c. $ \forall x \in [a,b]. g_1(x) \leq g_2(x) $. Sia $ A \in \mathbb{R}^2 $ un insieme y-semplice e sia $ f:A\to\mathbb{R} $ continua, allora vale:
  \[
    \int_{A}^{}f(x,y)dxdy = \int_{a}^{b}\left(\int_{g_1(x)}^{g_2(x)}f(x,y)dy\right)dx
  \]
}
Quindi, dato un insieme $ A $ y-semplice e una funzione $ f:A\to\mathbb{R} $ continua, per trovare il volume del sottografo relativo ad $ A $ bisogna prima di tutto trovare la formula che ci da' l'area sotto il grafico dato un valore di $ x $. Quindi consideriamo la $ x $ come costante e applichiamo l'integrale in $ dy $:
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-15-11-29-57.png}
\end{center}
Da notare come questi integrali sono calcolati sull'intervallo $ [g_1(x), g_2(x)] $, che sono sempre gli estremi dell'area dato che $ A $ e' y-semplice. Ora ci basta sommare tutte le aree trovate per tutti gli $ x $ nell'intervallo $ [a,b] $, che possiamo fare con un secondo integrale in $ dx $ questa volta:
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2024-05-15-11-45-00.png}
\end{center}
Se dobbiamo calcolare il volume sopra un'area x-semplice, basta ripetere gli stessi passi trovando prima le aree fissando $ y $ e poi sommarle tutte lungo l'asse $ y $:
\mprop{Riduzione x-semplice}{
  Siano $ h_1,h_2:[c,d]\to\mathbb{R} $ due funzioni continue t.c. $ \forall y \in [c,d]. h_1(y) \leq h_2(y) $. Sia $ A \in \mathbb{R}^2 $ un insieme x-semplice e sia $ f:A\to\mathbb{R} $ continua, allora vale:
  \[
    \int_{A}^{}f(x,y)dxdy = \int_{c}^{b}\left(\int_{h_1(y)}^{h_2(y)}f(x,y)dx\right)dy
  \]
}


\end{document} 
