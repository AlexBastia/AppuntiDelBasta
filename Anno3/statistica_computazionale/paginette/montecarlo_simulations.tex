% \begin{document}
\chapter{Montecarlo Simulations}
1. Simulation  ([[Sampling]])
2. Integration ([[Expectations]])
3. Optimisation

Called "Montecarlo" (casino) because of its use of randomly generated numbers. These are used to solve something that isn't random.

\ex{}{
Given a field with a fixed area (eg. $20km^2$) and a lake inside of it with an irregular shape with its own area.

\begin{center}
  \includegraphics[width=0.3\textwidth]{img/field.png}
\end{center}

How do we estimate (compute/measure is impossible - Coastline Paradox) the area of the lake?
	Note: the area of the lake isn't random, but injecting randomness makes it much easier to solve.

We can shoot cannon balls into the field **at random**, recording if it hit water or not. By repeating this process, we get a long string of binary values which were **randomly generated** (in a weird way).

We can now define a random variable $X$  (in this case a Bernoulli) with $n$ recorded outcomes $X_1,...,X_n$ 

Seen as the area of the lake and field don't change, the probability of hitting water remains the same. So for each random variable $P(X_1=1) = ... = P(X_n=1)$, they are **identically distributed**. 

Even if we keep hitting grass in one direction, we can't decide to move the cannon based on this information, because this wouldn't be random and it would cause dependence between the n random variables, which need to be iid (independent identically distributed). 

Given $A=$"hit water", $P(A) \alpha \frac{\text{Area Lake}}{\text{Area Field}}$.
So if we want to estimate the area of the lake, we can multiply the field area by the probability of hitting water. The measurement has become probabilistic.

To find such probability, we can find the avg. value of $X$ (sample proportion/mean). The more samples we have, the more precise the measurement (thanks to CLT).

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/montecarlo_estimation.png}
\end{center}
}

\section{R}
Suite of pre-implemented functions, use them!

Gamma distribution is a continuous distribution of positive random variables. 

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/gamma_distribution.png}
\end{center}

Use `rexp`, `rgamma`, ect. for preloaded distributions (you can search for the name in the help, selecting from a set of 4 different funcitons, d for density, p for probability, q to get value from probability, r for random selections).

How to sample from *any* distribution? For example, in Baeysian statistics we don't know the shape of the posterior. 

To view data, we can use the `hist` function for a histogram. If we want densities, we can use `freq = FALSE` (this is what we'll use). This is actually an estimator of the distribution of the random variable. We can also use `curve`, which is also an estimator, but a smooth one. 

Be careful about the size of the plots (don't be tricked). Height of the histogram estimates the density, but we can't tell if they're independent. The second plot is used for this case, plotting points in a 2d space with slightly shifted values. The acf (auto correlation funciton) plot measures correlation of a time series, by taking a sequence and shifting it many times, recording the correlation of a vector with itself. The x axis indicates the "lag", so at 0 it's 1 (correlation between two identical vectors). If our values are independent, we should see no correlation (close to 0).

By setting the seed, we can fix the pseudo-random sequence.

\ex{Field example in R}{
  Matrix with ones (lake) and zeros (field). We can plot this now. We then randomly select a matrix element and record if it's a 0 or 1. 

Note that we're using discrete values. We can sample using the `sample` function, with which we can define the set of tuples to sample from, the amount, with or without replacement (in our case the former with `replace=TRUE`).

The accuracy depends on the amount of random variables whose outcome we can observe, at a cost to computational time. 

To define a function, the correct syntax is:
Name <- function(parameters)
}

\section{Probability Integral Transform}
Remember to search for other packages that might implement the funciton

We have a starting funciton of which we take the inverse. We are using the cumulative integral function, thus the "Integral" part.

The PIT can produce any random variable starting from a uniform. Lets say we want a random variable with density $f$ and cdf $F$:
$$
F_X(x) = \int^x_{-\infty} f_X(t)dt
$$
We set $U = F_X(X)$ and solve for $X$. Note that $F_X$ has to be invertible, although this is almost always the case. 

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/PIT.png}
\end{center}

The cdf always (with some exceptions) exists, but the density might not. So the first thing we find is the cumulative, and if it exists we can calc the integral and get the density. 

We use the definition of the generalised inverse:
$$
F^{-1}(u) = inf\{x; F(x) \ge u\}
$$
If $U \sim Unif(0,1)$, then $F_X^{-1}=X$. This only works for the uniform distribution. 

We have that $F_X(x) = P(X \le x) = \int^x_{-\infty} f_X(t)dt$. In general, it's a **monotonic, non-decreasing** function. Outside of weird cases (greater than numerable amount of discontinuous points in $f_X$ i think), it's continuous. 

We can't sample directly from X because we don't know its distribution. But we can select a random value from $F_X(x)$ then find its inverse, but does this truly give the correct distribution? 

The uniform distribution has some cool props:
- $U \sim Unif(a,b) \iff \forall u \in [a,b]. P(U \le u) = F_U(u) = u$

Thus: 
$$P(F^{-1}_X(U) \le x) = P(F_X(F_X^{-1}(U)) \le F_X(x) = P(U \le F_X(x)) = F_X(x)$$

So we can say
$X = F_X^{-1}(U)$

\section{General Transformation Methods}
Used when a distribution is easy to construct using other distributions which are easy to simulate. Given $ X_i \sim Exp(1) $ i.i.d, three standard distributions can be derived as:
\begin{itemize}
  \item $ Y = 2 \sum_{j=1}^{v}X_j \sim \chi^2_{2v} $ (Chi-squared distribution)
  \item $ Y = \beta \sum_{j=1}^{a} X_j \sim G(a, \beta) $ (Gamma distribution)
  \item $ Y = \frac{\sum_{j=1}^{a} X_j}{\sum_{j=1}^{a+b} X_j} \sim Be(a,b) $ (Beta distribution)
\end{itemize}
Where $ v, a, b \in \mathbb{N}^* = \{1,2,3,...\} $.

\subsection{Gaussian}
$ F_X(x) = (x) = P(X \gets x) $ $ X \sim N(\mu, \sigma^2) $, has no closed form solution (no analytical solution), so we can't use PIT (we can't invert it). 

\textbf{Box-Muller algorithm} is a direct transformation method which gives two normals from two uniforms. We don't have to remember the algorithm. It's \textbf{exact}, but not the most efficient.

Why is there $ \pi $ in a Gaussian distr? Looking at the algorithm, we use trigonometric functions, which is weird.

R uses the PIT by using a very close approximation of the cdf which is invertible.

\subsection{Multivariate}
Multivariate Gaussian instead of having a single variable $ X \sim N(\mu, \sigma^2) $, we have a vector:
\[
  () \sim N_p(\mu, \Sigma)
\]
Where $ \mu $ is a vector and $ \Sigma $ is a Matrix that indicates the variances (on the diag) and covariances between each pair of RV. It's called a variance and covariance matrix, and it's simmetric and positive definite (all variances are different to 0).

Each individual RV of a multivariate gaussian is a gaussian (gaussian family is closed under ). Note, the opposite is not true.

If $ \Sigma $ is not diagonal, it means at least two RV are dependent.

This pops up for PCA.

Inverse standardisation for multivariate:
How do we square a matrix? Cholesky! We take the Cholesky decomposition $ \Sigma = A A^T $ (more or less the square root of $ \Sigma $).
\[
  Y \sim N_p(0,1) = AY \sim N_p(0, \Sigma)
\]
There are R packages that replicate these steps, but it's faster and better to sample from a gaussian and then stretch and squeeze the vector the rebuild the covariance structure. 2

\section{General result for transformation of a random variable}
GIven $ X \sim f_X(x) $ and $ Z = g(X): X = g^{-1}(Z) $ (g allows an inverse), then:
\[
  f_Z(z) = f_X(g^{-1}(z))\cdot |\frac{\delta g^{-1}(z)}{\delta z}|
\]
\nt{
  We don't always have a density.
}

\ex{Uniform R.V.}{
  $ U \sim Unif(0,1) \to f_U(u) = \begin{cases}
    0 & u \not\in [0,1]\\
    1 & u \in [0,1]
  \end{cases} $

  $ X = (b-a) \cdot U + a $ (linear transformation of a uniform). What is the distribution of $ X $?

  First we calc the inverse:
  \begin{align*}
    X - a = (b-a)\cdot U\\
    U = \frac{X-a}{b-a} = g^{-1}(\cdot)
  \end{align*}

  Now the derivative:
  \begin{align*}
    \frac{\partial g^{-1}(X)}{\partial x} = \frac{\partial [\frac{X - a}{b-a}]}{\partial x} \\
    \frac{\partial [\frac{X - a}{b-a} - \frac{a}{b-a}]}{\partial x}\\
    \frac{\partial [\frac{X}{b-a}]}{\partial x} = \frac{1}{b-a}
  \end{align*}

  So, applying the general transform:
  \[
    f_X(x) = f_U(\frac{x-a}{b-a}) \cdot \frac{1}{b-a} = \begin{cases}
      0 & f_U = 0 \iff x \not\in [a,b]\\
    \frac{1}{b-a} & 
    \end{cases}
  \]
  Thus $ X \sim Unif(a,b) $
}

\ex{Gaussian}{
  $ X \sim N(\mu, \sigma^2) $ and $ Z = \frac{X-\mu}{\sigma} = g(X) $ (never seen before transformation)

  $ X = Z \cdot \sigma + \mu = g^{-1}(X) $

  So the derivative is simply
  \[
    \frac{\partial g^{-1}(Z)}{\partial z} = \sigma
  \]
  \begin{align*}
    f_Z(z) = f_X(g^{-1}(z))\cdot \sigma \\
    \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sigma} \cdot e^{-\frac{(z \sigma + \mu - \mu)^2}{-2\sigma^2}} \cdot \sigma \\
    \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{z^2}{2}}
  \end{align*}

  This is the density of the standard gaussian distribution $ N(0,1) $, and this is why we standardize in this way.
}

We now look at direct distributions for discrete random variables. After, we'll look at indirect meethods with the accept-reject algorithm.

\section{Discrete Distribution}
Non-continuous cdf.

We have to store the values of the cdf (we have to be able to compute it). Then sample from a uniform and check where the value falls:
\[
X = k \iff p_{k-1} < U < p_k
\]

\nt{
  If we have a very large Poisson, we'll have to store many probability values for computation.
}
% \end{document}
