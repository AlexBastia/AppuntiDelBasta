% \begin{document}
\chapter{Sampling}
\section{Direct Continuous}
\subsection{PIT}
Remember to search for other packages that might implement the funciton

We have a starting funciton of which we take the inverse. We are using the cumulative integral function, thus the "Integral" part.

The PIT can produce any random variable starting from a uniform. Lets say we want a random variable with density $f$ and cdf $F$:
$$
F_X(x) = \int^x_{-\infty} f_X(t)dt
$$
We set $U = F_X(X)$ and solve for $X$. Note that $F_X$ has to be invertible, although this is almost always the case. 

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/PIT.png}
\end{center}

The cdf always (with some exceptions) exists, but the density might not. So the first thing we find is the cumulative, and if it exists we can calc the integral and get the density. 

We use the definition of the generalised inverse:
$$
F^{-1}(u) = inf\{x; F(x) \ge u\}
$$
If $U \sim Unif(0,1)$, then $F_X^{-1}=X$. This only works for the uniform distribution. 

We have that $F_X(x) = P(X \le x) = \int^x_{-\infty} f_X(t)dt$. In general, it's a **monotonic, non-decreasing** function. Outside of weird cases (greater than numerable amount of discontinuous points in $f_X$ i think), it's continuous. 

We can't sample directly from X because we don't know its distribution. But we can select a random value from $F_X(x)$ then find its inverse, but does this truly give the correct distribution? 

The uniform distribution has some cool props:
- $U \sim Unif(a,b) \iff \forall u \in [a,b]. P(U \le u) = F_U(u) = u$

Thus: 
$$P(F^{-1}_X(U) \le x) = P(F_X(F_X^{-1}(U)) \le F_X(x) = P(U \le F_X(x)) = F_X(x)$$

So we can say
$X = F_X^{-1}(U)$

\subsection{Known Distributions}
Used when a distribution is easy to construct using other distributions which are easy to simulate. Given $ X_i \sim Exp(1) $ i.i.d, three standard distributions can be derived as:
\begin{itemize}
  \item $ Y = 2 \sum_{j=1}^{v}X_j \sim \chi^2_{2v} $ (Chi-squared distribution)
  \item $ Y = \beta \sum_{j=1}^{a} X_j \sim G(a, \beta) $ (Gamma distribution)
  \item $ Y = \frac{\sum_{j=1}^{a} X_j}{\sum_{j=1}^{a+b} X_j} \sim Be(a,b) $ (Beta distribution)
\end{itemize}
Where $ v, a, b \in \mathbb{N}^* = \{1,2,3,...\} $.

\subsubsection{Gaussian}
$ F_X(x) = (x) = P(X \gets x) $ $ X \sim N(\mu, \sigma^2) $, has no closed form solution (no analytical solution), so we can't use PIT (we can't invert it). 

\textbf{Box-Muller algorithm} is a direct transformation method which gives two normals from two uniforms. We don't have to remember the algorithm. It's \textbf{exact}, but not the most efficient.

Why is there $ \pi $ in a Gaussian distr? Looking at the algorithm, we use trigonometric functions, which is weird.

R uses the PIT by using a very close approximation of the cdf which is invertible.

\subsubsection{Multivariate Gaussian}
Multivariate Gaussian instead of having a single variable $ X \sim N(\mu, \sigma^2) $, we have a vector:
\[
  () \sim N_p(\mu, \Sigma)
\]
Where $ \mu $ is a vector and $ \Sigma $ is a Matrix that indicates the variances (on the diag) and covariances between each pair of RV. It's called a variance and covariance matrix, and it's simmetric and positive definite (all variances are different to 0).

Each individual RV of a multivariate gaussian is a gaussian (gaussian family is closed under ). Note, the opposite is not true.

If $ \Sigma $ is not diagonal, it means at least two RV are dependent.

This pops up for PCA.

Inverse standardisation for multivariate:
How do we square a matrix? Cholesky! We take the Cholesky decomposition $ \Sigma = A A^T $ (more or less the square root of $ \Sigma $).
\[
  Y \sim N_p(0,1) = AY \sim N_p(0, \Sigma)
\]
There are R packages that replicate these steps, but it's faster and better to sample from a gaussian and then stretch and squeeze the vector the rebuild the covariance structure. 2

\subsection{General Case (Using Derivative)}
GIven $ X \sim f_X(x) $ and $ Z = g(X): X = g^{-1}(Z) $ (g allows an inverse), then:
\[
  f_Z(z) = f_X(g^{-1}(z))\cdot |\frac{\delta g^{-1}(z)}{\delta z}|
\]
\nt{
  We don't always have a density.
}

\ex{Uniform R.V.}{
  $ U \sim Unif(0,1) \to f_U(u) = \begin{cases}
    0 & u \not\in [0,1]\\
    1 & u \in [0,1]
  \end{cases} $

  $ X = (b-a) \cdot U + a $ (linear transformation of a uniform). What is the distribution of $ X $?

  First we calc the inverse:
  \begin{align*}
    X - a = (b-a)\cdot U\\
    U = \frac{X-a}{b-a} = g^{-1}(\cdot)
  \end{align*}

  Now the derivative:
  \begin{align*}
    \frac{\partial g^{-1}(X)}{\partial x} = \frac{\partial [\frac{X - a}{b-a}]}{\partial x} \\
    \frac{\partial [\frac{X - a}{b-a} - \frac{a}{b-a}]}{\partial x}\\
    \frac{\partial [\frac{X}{b-a}]}{\partial x} = \frac{1}{b-a}
  \end{align*}

  So, applying the general transform:
  \[
    f_X(x) = f_U(\frac{x-a}{b-a}) \cdot \frac{1}{b-a} = \begin{cases}
      0 & f_U = 0 \iff x \not\in [a,b]\\
    \frac{1}{b-a} & 
    \end{cases}
  \]
  Thus $ X \sim Unif(a,b) $
}

\ex{Gaussian}{
  $ X \sim N(\mu, \sigma^2) $ and $ Z = \frac{X-\mu}{\sigma} = g(X) $ (never seen before transformation)

  $ X = Z \cdot \sigma + \mu = g^{-1}(X) $

  So the derivative is simply
  \[
    \frac{\partial g^{-1}(Z)}{\partial z} = \sigma
  \]
  \begin{align*}
    f_Z(z) = f_X(g^{-1}(z))\cdot \sigma \\
    \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sigma} \cdot e^{-\frac{(z \sigma + \mu - \mu)^2}{-2\sigma^2}} \cdot \sigma \\
    \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{z^2}{2}}
  \end{align*}

  This is the density of the standard gaussian distribution $ N(0,1) $, and this is why we standardize in this way.
}

We now look at direct distributions for discrete random variables. After, we'll look at indirect meethods with the accept-reject algorithm.


\section{Direct Discrete}
Non-continuous cdf.
\subsection{General Algorithm}
Based on inverse transform, but not the same as PIT. This is not used much, because most discrete distributions are known.

We have to store the values of the cdf (we have to be able to compute it). Then sample from a uniform and check where the value falls:
\[
X = k \iff p_{k-1} < U < p_k
\]

\nt{
  If we have a very large Poisson, we'll have to store many probability values for computation.
}


\[
  P_X(x)=P(X=x)=\binom{10}{x}p^x(1-p)^{10-x}
\]
The problem is calculation (binomial can get very large), expecially for unlimited distributions like poisson.

Given two RV $ X,Y $, then
\[
  X = Y \iff F_X(x) = F_Y(y)
\]

The binomial distribution isn't very used, in contrast to Poisson which was very used in sports analysis seen as it \textbf{counts} "stuff" in a specific time window (highly connected to exponential). It's also known as the "rare distribution", seen as the only parameter $ \lambda $ tells you on avarege the number of events you expect to happen, but this is also the value of the variance (which doesn't happen in real life)
\[
  X \sim Poiss(\lambda) \to \begin{cases}
    E(X) = \lambda & \\
    Var(X) = \lambda & 
  \end{cases}
\]
This is called the \textbf{overdispersion problem} ($ Var(X) \gg E(X) $), and it's the main reason for moving on to more comlex models. So the computation is quite difficult, also because of factorial and exponential terms.

\[
Poisson(\lambda) \to \lambda \to +\infty \to \sim\sim N(\lambda, \lambda)
\]
So we can use a Gaussian to aproximate a Poisson distribution.

When $ \lambda > 50 $ we can say that most of the probability of the RV is
\[
\lambda += 3 \sqrt{\lambda}
\]
So this is the range of values for $ X $ that we should consider to calculate the distribution. This is because of its similarity to the gaussian.

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2025-11-18-09-57-49.png}
\end{center}

So some of the values in the domain of $ X $ are useless to check, seen as they're highly unlykely

\ex{}{
  $ X \sim Poisson(100) $. If we consider the Gaussian aprox. most of the values will be in $ (70, 130) $.

  So if 
}

\section{Indirect}
\subsection{Bayes' Theorem}
Giuro che questo non lo rifaccio

\subsubsection{Continuous}
Given two cont RV $ X,Y $
\[
  f_{X|Y=y}(x)
\]
is the distribution of $ X $ given some info ($ Y $)
\[
  (X|Y=y) ~ \text{random variable}
\]
we're just focusing on a specific cross section of the population (e.g. income at a specific age).

We want to find its density, we can use Bayes
\[
  f_{X|Y=y}(x) = \frac{f_{X,Y}}{f_Y(y)} = \frac{f_{Y|X=x}(y) f_X(x)}{f_Y(y)}
\]
When you want to move a RV behind the conditioning line, you divide by the marginal.

We can use the marginal distribution for the denominator
\[
  \frac{f_{X,Y}(x,y)}{\int_{Y}^{}f_{X,Y}(x, t)dt}
\]
by marginalizing out $ Y $ (same idea as the discrete version).

\subsection{Expectation}
Given $ X $ cont. RV $ X \sim f_X(x) $:
\[
  E_X(X) = \int_{D_X}^{}x f_X(x)dx
\]
\nt{
  Always indicate the distribution used as a subscript. This will become important.
}

\[
  E_Y(f_{X|Y}(X|Y)) = \int_{D_X}^{}f_Y(y) f_{X|Y}(X|Y)dy
\]
This is a product of a conditional times a marginal (moving $ Y $ from the back to the front). So we have the joint
\[
  = \int_{D_Y}^{}f_{X,Y}(X,Y)dy = f_X(x)
\]
So we can use expectation to marginalize out variables. Why is this important?

If we have values sampled from a conditional distribution, taking the expected values (average) we're reming the influence of $ Y $. This is the building block for the Montecarlo-Markov Chain (most widely used Montecarlo method).

\subsection{Accept-Reject Method}
We need to know:
\begin{itemize}
\item When it works
\item Main ingredients
\item No proof!
\item The actual algorithm
\end{itemize}

It's the first indirect method for sampling. It works with two distributions:
\begin{itemize}
\item A \textbf{candidate} dist.
\item A \textbf{targe} dist.
\end{itemize}

Remember the lake? Let's say the area is $ 1 \times 1 = 1km^2 $. We are shooting cannon balls at random, so
\[
  X \sim Unif(0,1)
\]
For every two random values we get a result
\[
  (u_1, u_2) = \text{HIT!}
\]
So we generate a uniform for each side.

By changing the shape of the lake, we can get a more interesting result
\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2025-11-19-09-50-14.png}
\end{center}

We're now randomly testing a \textbf{density function} with two uniform RV.

\subsubsection{Ingredients}
\begin{itemize}
  \item \textbf{Denisty} $ f $ of interest (only functional form) and be able to compute it.
  \item \textbf{Candidate} distribution $ g $.
\end{itemize}

The candidate choice is up to us. Some are smarter than others, but as long as we don't pick functions that don't satisfy a certain condition (we'll see later), it's ok. In the exam we'll be given the candidate.

The candidate generates a value. We need to test
\[
  u \leq \frac{f_X(y)}{M g_Y(y)}
\]
If it's satisied we accept (it comes from $ f_X $). Otherwise, we reject it and try again (similar to the discrete case).

Proposal density constraints:
\begin{itemize}
  \item Target and candidate have to have compatible supports ($ S_X \subseteq S_Y $).
  \item There is a constant $ M $ with $ f(x)/g(x) \leq M \ \forall x $.
\end{itemize}

The last requirement is less trivial. It means that the ratio between the distributions has to be bounded.

If these are satisfied, then
\begin{enumerate}
  \item Generate $ Y \sim g $ and \textbf{independently} generate from $ U \sim Unif(0,1) $.
  \item Check if $ U \leq \frac{1}{M} \frac{f(Y)}{g(Y)} $ then set $ X = Y $ (accept)
  \item If it's not satisfied, discard $ Y $ and $ U $ and start again.
\end{enumerate}

Note that $ M = sup_x \frac{f(x)}{g(x)} $. The crucial requirement that $ M $ exists means that $ M $ can have many different values.\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2025-11-19-10-26-26.png}
\end{center}

The best value for $ M $ is the one that reduces the area where the points fall and don't belong to the density, reducing efficiency
\[
  P(\text{Accept}) = \frac{1}{M}
\]
So in the case of the graph you should choose the blue line.

\subsubsection{Proof}
Does it work? We need to think from a statistical pov: Are the accepted values coming from $ f_X(x) $?

What is the distr. of the accepted values? What is the CDF?

Let's define the event
\[
A = \text{"Accept a value"}
\]
Where $ A $ is basically
\[
  Y | U \leq \frac{1}{M} \frac{f(Y)}{g(Y)}
\]
so a distribution conditional to a test where $ U \sim Unif(0,1) $ and $ Y \sim g(y) $. Lets look at the CDF

  \begin{align*}
  P(Y \leq x | U \leq \frac{1}{M} \frac{f(Y)}{g(Y)}) &= \frac{P(C,D)}{P(D)} \\
  &= \frac{P(Y \leq x, U \leq \frac{1}{M}\frac{f(Y)}{g(Y)})}{P(U\leq \frac{1}{M}\frac{f(Y)}{g(Y)})}\\
  &= \frac{\int_{-\infty}^{x}\left(\int_{-\infty}^{\frac{1}{M}\frac{f(Y)}{g(Y)}} 1 \,du\right) \cdot g_Y(y)\,dy}{\int_{-\infty}^{+\infty}\left(\int_{-\infty}^{\frac{1}{M}\frac{f(Y)}{g(Y)}} 1 \,du\right) \cdot g_Y(y)\,dy} \\
  &= \frac{\int_{-\infty}^{x}\frac{1}{M}\frac{f(Y)}{g(Y)} g_Y(y)\,dy}{\int_{-\infty}^{+\infty}\frac{1}{M}\frac{f(Y)}{g(Y)}g_Y(y)\,dy} \\
  &= \frac{\int_{-\infty}^{x}f(Y)\,dy}{\int_{-\infty}^{+\infty}f(Y)\,dy} \\
  &= \frac{P(X \leq x)}{1} = F_X(x) \\
\end{align*}

So the accepted values have the same cdf as $ X $!

\nt{
  $ g $ and $ M $ are arbitrary and don't matter for the correctness of the algorithm.
}

\subsubsection{Recap}
The \textbf{goal} is to sample from the density of a RV $ X $ $ f_X(x) $ (target). To do this we need $ f_X(x) $ and it needs to be computable, while knowing the support of $ X $ $ S_X $. 
\begin{itemize}
\item assume $ \exists M $ and is a constant $ > 0 $ such that
  \[
    \frac{f_X(x)}{g_Y(x)} \leq M
  \]
\item $ g_Y(y) $ is a candidate/proposal distribution with compatible support with $ f $ $ S(Y) \subseteq S(X) $, and should be easy to sample from.
  \item It works for continuous RV
\end{itemize}

Steps:
\begin{enumerate}
  \item Sample $ y $ from $ g_Y(y) $
  \item Ind. sample $ u $ from $ U(0,1) $
  \item check if $ u \leq \frac{1}{M} \frac{f_X(y)}{g_Y(y)} $
  \item If true, accept. Else, reject.
\end{enumerate}

We can check if $ u M \leq \frac{f_X(y)}{g_Y(y)} $, which is like sayinng $ u' \sim Unif(0, M) $.

% TODO: ha parlato di intuizioni per capire l'algoritmo e al suo legame con distribuzioni non normalizzate

To find the optimal value of M, we can use the optimize function in R. If one or both of the distributions is not normalized, M doesn't represent the true prob of acceptance. We can get the normalising constant from the AR as a byproduct.

Note: the distribution we get is normalised even if the original one wasn't. So the histogram won't match! We can correctly plot the target function by multiplying/dividing by the constant.

\subsubsection{Examples}

\ex{AC- algh}{
  We aim to simulate a random variable $X$ from the Standard Normal distribution using the Accept-Reject method with a Laplace (Double Exponential) candidate distribution.

  \begin{itemize}
    \item Target density ($f$): The Standard Normal distribution $(\mathcal{N}(0,1)$:
    \[
      f(x) = \frac{1}{\sqrt{2\pi}}\exp{(-\frac{x^2}{2})}\quad x\in \mathbb{R}
    \]

    \item Candidate Density ($g$): The Laplace distribution ($Laplace(1)$):
    \[g(y) = \frac{1}{2} \exp(-|y|), \quad y \in \mathbb{R}\]

    We need to find a constant $M$ such that $f(x) \le M g(x)$ for all $x$. This is equivalent to finding the maximum of the ratio $f(x)/g(x)$:
    \[\frac{f(x)}{g(x)} = \frac{\frac{1}{\sqrt{2\pi}} e^{-x^2/2}}{\frac{1}{2} e^{-|x|}} = \sqrt{\frac{2}{\pi}} \exp\left(-\frac{x^2}{2} + |x|\right)\]
    
    To maximize this ratio, we maximize the expression $h(x) = \sqrt{\frac{2}{\pi}} \exp\left(-\frac{x^2}{2} + |x|\right)$, for doing that we have to set the derivate to 0:
    \[
      h'(x) = \sqrt{\frac{2}{\pi}} \exp\left(-\frac{x^2}{2} + |x|\right) (-x + \frac{x}{\abs{x}}) = 0
    \]
    Solving this, we find that the maximum occurs at $x = 1$ and $x = -1$. Evaluating $h(x)$ at these points gives:
    \[M = h(1) = h(-1) = \sqrt{\frac{2}{\pi}} e^{-1/2 + 1} = \sqrt{\frac{2e}{\pi}}\]

    So we have 
    \[
      U \leq \frac{1}{M} \frac{f(Y)}{g(Y)} = \frac{1}{\sqrt{\frac{2e}{\pi}}} \cdot \sqrt{\frac{2}{\pi}} \exp\left(-\frac{Y^2}{2} + |Y|\right) = e^{-1/2} \exp\left(-\frac{Y^2}{2} + |Y|\right)
    \]
  \end{itemize}
} 
\lstinputlisting[language=R]{code/Examples/Sampling/AR/Laplace.R}

An other algh in R:
\lstinputlisting[language=R]{code/Examples/Sampling/AR/Beta.R}

Alternative for ar IF YOU HAVE REDUCED COMPUTATIONAL RESOURCE

TODO: Code giving errors for some reason
% \lstinputlisting[language=R]{code/Examples/Sampling/AR/BetaFromBeta.R}

\ex{Penso esercizio}{
  So we have $f(x) \alpha \exp{\left(-\frac{x^2}{2}\right)} \left[\sin{(6x)}^2+3\cos{(x)}^2\cdot \sin{(4x)}^2+1\right]$
  \begin{enumerate}
    \item Plot $f(x)$ to show $M \cdot g(x)$ bounds it where $g_Y$ is the density of a Std. GAUSSIAN $g_Y(y) = \frac{1}{\sqrt{2\pi}} \cdot \exp\{-\frac{y^2}{2}\}$ and find $M$.
    \item Generate 2500 numbers from $f_X$.
    \item Deduce from the acceptance rate of the AR algorithm, an approximation of the normalizing constant.
  \end{enumerate}

  \lstinputlisting[language=R]{code/Examples/Sampling/AR/Ex2_18.R}
}
% \end{document}
