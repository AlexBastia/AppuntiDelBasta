% \begin{document}
\chapter{Sampling}
\section{Direct Continuous}
\subsection{PIT}
Remember to search for other packages that might implement the funciton

We have a starting funciton of which we take the inverse. We are using the cumulative integral function, thus the "Integral" part.

The PIT can produce any random variable starting from a uniform. Lets say we want a random variable with density $f$ and cdf $F$:
$$
F_X(x) = \int^x_{-\infty} f_X(t)dt
$$
We set $U = F_X(X)$ and solve for $X$. Note that $F_X$ has to be invertible, although this is almost always the case. 

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/PIT.png}
\end{center}

The cdf always (with some exceptions) exists, but the density might not. So the first thing we find is the cumulative, and if it exists we can calc the integral and get the density. 

We use the definition of the generalised inverse:
$$
F^{-1}(u) = inf\{x; F(x) \ge u\}
$$
If $U \sim Unif(0,1)$, then $F_X^{-1}=X$. This only works for the uniform distribution. 

We have that $F_X(x) = P(X \le x) = \int^x_{-\infty} f_X(t)dt$. In general, it's a **monotonic, non-decreasing** function. Outside of weird cases (greater than numerable amount of discontinuous points in $f_X$ i think), it's continuous. 

We can't sample directly from X because we don't know its distribution. But we can select a random value from $F_X(x)$ then find its inverse, but does this truly give the correct distribution? 

The uniform distribution has some cool props:
- $U \sim Unif(a,b) \iff \forall u \in [a,b]. P(U \le u) = F_U(u) = u$

Thus: 
$$P(F^{-1}_X(U) \le x) = P(F_X(F_X^{-1}(U)) \le F_X(x) = P(U \le F_X(x)) = F_X(x)$$

So we can say
$X = F_X^{-1}(U)$

\subsection{Known Distributions}
Used when a distribution is easy to construct using other distributions which are easy to simulate. Given $ X_i \sim Exp(1) $ i.i.d, three standard distributions can be derived as:
\begin{itemize}
  \item $ Y = 2 \sum_{j=1}^{v}X_j \sim \chi^2_{2v} $ (Chi-squared distribution)
  \item $ Y = \beta \sum_{j=1}^{a} X_j \sim G(a, \beta) $ (Gamma distribution)
  \item $ Y = \frac{\sum_{j=1}^{a} X_j}{\sum_{j=1}^{a+b} X_j} \sim Be(a,b) $ (Beta distribution)
\end{itemize}
Where $ v, a, b \in \mathbb{N}^* = \{1,2,3,...\} $.

\subsubsection{Gaussian}
$ F_X(x) = (x) = P(X \gets x) $ $ X \sim N(\mu, \sigma^2) $, has no closed form solution (no analytical solution), so we can't use PIT (we can't invert it). 

\textbf{Box-Muller algorithm} is a direct transformation method which gives two normals from two uniforms. We don't have to remember the algorithm. It's \textbf{exact}, but not the most efficient.

Why is there $ \pi $ in a Gaussian distr? Looking at the algorithm, we use trigonometric functions, which is weird.

R uses the PIT by using a very close approximation of the cdf which is invertible.

\subsubsection{Multivariate Gaussian}
Multivariate Gaussian instead of having a single variable $ X \sim N(\mu, \sigma^2) $, we have a vector:
\[
  () \sim N_p(\mu, \Sigma)
\]
Where $ \mu $ is a vector and $ \Sigma $ is a Matrix that indicates the variances (on the diag) and covariances between each pair of RV. It's called a variance and covariance matrix, and it's simmetric and positive definite (all variances are different to 0).

Each individual RV of a multivariate gaussian is a gaussian (gaussian family is closed under ). Note, the opposite is not true.

If $ \Sigma $ is not diagonal, it means at least two RV are dependent.

This pops up for PCA.

Inverse standardisation for multivariate:
How do we square a matrix? Cholesky! We take the Cholesky decomposition $ \Sigma = A A^T $ (more or less the square root of $ \Sigma $).
\[
  Y \sim N_p(0,1) = AY \sim N_p(0, \Sigma)
\]
There are R packages that replicate these steps, but it's faster and better to sample from a gaussian and then stretch and squeeze the vector the rebuild the covariance structure. 2

\subsection{General Case (Using Derivative)}
GIven $ X \sim f_X(x) $ and $ Z = g(X): X = g^{-1}(Z) $ (g allows an inverse), then:
\[
  f_Z(z) = f_X(g^{-1}(z))\cdot |\frac{\delta g^{-1}(z)}{\delta z}|
\]
\nt{
  We don't always have a density.
}

\ex{Uniform R.V.}{
  $ U \sim Unif(0,1) \to f_U(u) = \begin{cases}
    0 & u \not\in [0,1]\\
    1 & u \in [0,1]
  \end{cases} $

  $ X = (b-a) \cdot U + a $ (linear transformation of a uniform). What is the distribution of $ X $?

  First we calc the inverse:
  \begin{align*}
    X - a = (b-a)\cdot U\\
    U = \frac{X-a}{b-a} = g^{-1}(\cdot)
  \end{align*}

  Now the derivative:
  \begin{align*}
    \frac{\partial g^{-1}(X)}{\partial x} = \frac{\partial [\frac{X - a}{b-a}]}{\partial x} \\
    \frac{\partial [\frac{X - a}{b-a} - \frac{a}{b-a}]}{\partial x}\\
    \frac{\partial [\frac{X}{b-a}]}{\partial x} = \frac{1}{b-a}
  \end{align*}

  So, applying the general transform:
  \[
    f_X(x) = f_U(\frac{x-a}{b-a}) \cdot \frac{1}{b-a} = \begin{cases}
      0 & f_U = 0 \iff x \not\in [a,b]\\
    \frac{1}{b-a} & 
    \end{cases}
  \]
  Thus $ X \sim Unif(a,b) $
}

\ex{Gaussian}{
  $ X \sim N(\mu, \sigma^2) $ and $ Z = \frac{X-\mu}{\sigma} = g(X) $ (never seen before transformation)

  $ X = Z \cdot \sigma + \mu = g^{-1}(X) $

  So the derivative is simply
  \[
    \frac{\partial g^{-1}(Z)}{\partial z} = \sigma
  \]
  \begin{align*}
    f_Z(z) = f_X(g^{-1}(z))\cdot \sigma \\
    \frac{1}{\sqrt{2\pi}} \cdot \frac{1}{\sigma} \cdot e^{-\frac{(z \sigma + \mu - \mu)^2}{-2\sigma^2}} \cdot \sigma \\
    \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{z^2}{2}}
  \end{align*}

  This is the density of the standard gaussian distribution $ N(0,1) $, and this is why we standardize in this way.
}

We now look at direct distributions for discrete random variables. After, we'll look at indirect meethods with the accept-reject algorithm.


\section{Direct Discrete}
Non-continuous cdf.
\subsection{General Algorithm}
Based on inverse transform, but not the same as PIT. This is not used much, because most discrete distributions are known.

We have to store the values of the cdf (we have to be able to compute it). Then sample from a uniform and check where the value falls:
\[
X = k \iff p_{k-1} < U < p_k
\]

\nt{
  If we have a very large Poisson, we'll have to store many probability values for computation.
}


\[
  P_X(x)=P(X=x)=\binom{10}{x}p^x(1-p)^{10-x}
\]
The problem is calculation (binomial can get very large), expecially for unlimited distributions like poisson.

Given two RV $ X,Y $, then
\[
  X = Y \iff F_X(x) = F_Y(y)
\]

The binomial distribution isn't very used, in contrast to Poisson which was very used in sports analysis seen as it \textbf{counts} "stuff" in a specific time window (highly connected to exponential). It's also known as the "rare distribution", seen as the only parameter $ \lambda $ tells you on avarege the number of events you expect to happen, but this is also the value of the variance (which doesn't happen in real life)
\[
  X \sim Poiss(\lambda) \to \begin{cases}
    E(X) = \lambda & \\
    Var(X) = \lambda & 
  \end{cases}
\]
This is called the \textbf{overdispersion problem} ($ Var(X) \gg E(X) $), and it's the main reason for moving on to more comlex models. So the computation is quite difficult, also because of factorial and exponential terms.

\[
Poisson(\lambda) \to \lambda \to +\infty \to \sim\sim N(\lambda, \lambda)
\]
So we can use a Gaussian to aproximate a Poisson distribution.

When $ \lambda > 50 $ we can say that most of the probability of the RV is
\[
\lambda += 3 \sqrt{\lambda}
\]
So this is the range of values for $ X $ that we should consider to calculate the distribution. This is because of its similarity to the gaussian.

\begin{center}
  \includegraphics[width=0.5\textwidth]{img/2025-11-18-09-57-49.png}
\end{center}

So some of the values in the domain of $ X $ are useless to check, seen as they're highly unlykely

\ex{}{
  $ X \sim Poisson(100) $. If we consider the Gaussian aprox. most of the values will be in $ (70, 130) $.

  So if 
}

\section{Indirect}
Accept-reject
% \end{document}
