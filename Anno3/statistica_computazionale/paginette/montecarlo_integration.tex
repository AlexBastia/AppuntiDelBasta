% \begin{document}
\chapter{Montecarlo Integration}

\section{Theory}
\subsection{Introduction}

The goal is computing an integral of the from
\[
  I = \sin_{\mathcal{X}}h(x) f(x) dx
\]

where:
\begin{itemize}
  \item $\mathcal{X} \subseteq \mathbb{R}^n$ is the domain of integration
  \item $f(x)$ is a density function (can be unnormalized)  
  \item $h(x)$ is the function we want to integrate 
\end{itemize}

the core of montecarlo integration is in a interpretation of the integral as an expectation value of a RV. If we consider a RV $X$ with density $f(x)$, then we can write
\[
  I = E_f[h(X)] = \sin_{\mathcal{X}} h(x) f(x) dx
\]

this identity is crucial because it allows us to use the properties of RVs to compute integrals.

\nt{
  although an integral it is not usual to appear in this form (for instance $\int_a^bg(x)dx$), we can usually rewrite it in this form by choosing a suitable density $f(x)$. For example, if we want to compute $\int_a^bg(x)dx$, we can choose $f(x)$ as the uniform density in $[a,b]$:
  \[
    f(x) = \begin{cases}
      \frac{1}{b-a} & x \in [a,b] \\
      0 & otherwise
    \end{cases}
  \]
  so that
  \[
    \int_a^bg(x)dx = \int_a^bg(x) \cdot \frac{1}{b-a} \cdot (b-a) dx = (b-a) \int_a^bg(x)f(x)dx
  \]
}

once we have established this identity the method for approximating $I$ is straightforward:
\begin{itemize}
  \item Generation: generate $n$ samples (i.i.d.) from the density $f(x)$: $X_1, X_2, \dots, X_n \sim f(x)$
  \item Estimation: compute the sample mean of $h(X)$:
  \[
    \hat{I}_n = \frac{1}{n} \sum_{i=1}^{n} h(X_i)
  \]
\end{itemize}
the value $\hat{I}_n$ is our estimate of the integral $I$. 

Okay, gg for this introduction. Now let us to a concept that explain that:


\subsection{The Law of Large Numbers and the Central Limit Theorem}

Consider an infinite sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \dots$ (the independent condition can be relaxed) defined on a probability space $(\Omega, \mathcal{F}, P)$. Let these variables have a finite expectation $\mathbb{E}[X_1] = \mathbb{E}[X_2] = ... = \mu$.

\dfn{Sample mean}{
  it's defined the \textit{sample mean} of the first $n$ variables as:
  \[
  \overline{X}_n = \frac{1}{n} S_n = \frac{1}{n} \sum_{i=1}^n X_i
  \]

  Where $S_n$ is the \textit{partial sum} of the first $n$ values.
}

The Law of Large Numbers (LLN) describes the convergence of $\overline{X}_n$ to the true parameter $\mu$ as the sample size $n$ approaches infinity. There are two versions of this law, distinguished by the type of convergence.

\thm{Weak Law of Large Numbers (WLLN) - Convergence in Probability}{
  The sample mean $\overline{X}_n$ converges to $\mu$ in \textit{probability}. That is, for any margin of error $\epsilon > 0$, the probability that the absolute difference between the sample mean and the true mean exceeds $\epsilon$ tends to zero as $n \to \infty$:
  \[
    \lim_{n \to \infty} P\left( |\overline{X}_n - \mu| > \epsilon \right) = 0
  \]
  Often denoted as: $\overline{X}_n \xrightarrow{P} \mu$.
}

\thm{Strong Law of Large Numbers (SLLN) - Convergence Almost Surely}{
  The sample mean $\overline{X}_n$ converges to $\mu$ \textit{almost surely} (a.s.). That is, the event that the limit of the sequence $\overline{X}_n$ equals $\mu$ occurs with probability 1:
  \[
    P\left( \lim_{n \to \infty} \overline{X}_n = \mu \right) = 1
  \]
  Often denoted as: $\overline{X}_n \xrightarrow{a.s.} \mu$.
}

 \nt{
  If the random variables also have a finite variance (they have to also be independent) $Var(X_i) = \sigma^2 < \infty$, then the variance of the sample mean decreases linearly with $n$:
  \[
    Var(\overline{X}_n) = Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac{1}{n^2}\sum_{i=1}^n Var(X_i) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}
  \]
  }

  \ex{Code in R() for SLLN}{
    \lstinputlisting[language=R]{code/Examples/MC_Int/slln.r}
  }

While the Law of Large Numbers guarantees that the sample mean converges to the true expectation, the Central Limit Theorem (CLT) describes the \textit{distribution} of the error for large sample sizes. This allows us to quantify the uncertainty of our Monte Carlo estimates.

\thm{Central Limit Theorem (CLT)}{
  Let $X_1, X_2, \dots$ be a sequence of independent and identically distributed (i.i.d.) random variables with:
  \begin{itemize}
    \item Finite expectation: $\mathbb{E}[X_i] = \mu$
    \item Finite variance: $Var(X_i) = \sigma^2 < \infty$
  \end{itemize}

  Then, the random variable $\sqrt{n}(\overline{X}_n - \mu)$ (it's a linear transformation of a RV, so still a RV) converges in \textit{distribution} to a Normal random variable with mean 0 and variance $\sigma^2$ as $n \to \infty$:
  \[
    \sqrt{n}(\overline{X}_n - \mu) \xrightarrow{d} \mathcal{N}(0, \sigma^2)
  \]
  Equivalently, the standardized sample mean converges to a Standard Normal distribution:
  \[
    \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} \mathcal{N}(0, 1)
  \]
 
}

For a sufficiently large sample size $n$ (simulation runs), the estimation error $(\overline{X}_n - \mu)$ is approximately Normally distributed:
\[
  (\overline{X}_n - \mu) \approx \mathcal{N}\left(0, \frac{\sigma^2}{n}\right)
\]
This approximation justifies the construction of asymptotic confidence intervals. For instance, a 95\% confidence interval for $\mu$ is given by:
\[
  \left[ \overline{X}_n - 1.96 \frac{\sigma}{\sqrt{n}}, \quad \overline{X}_n + 1.96 \frac{\sigma}{\sqrt{n}} \right]
\]
In practice, since the true variance $\sigma^2$ is unknown, it is replaced by the sample variance estimator $S_n^2$.

We can now say something about the uncertainty of the result by using the gaussian.

\section{Numerical Deterministic Aproach}
Most of the time, this works and should be prefered. In R they are implemented as the functions
\begin{itemize}
  \item \textit{Integrate}: can be unstable (integrand needs to be "well behaved")
  \item \textit{Area}: needs bounded domain (can't use infinity), so additional information about function to integrate. In package MASS
\end{itemize}
 
These only work for one-dimensional funcitons, whereas Montecarlo works for multiple parameters.

\section{Montecarlo Method}
Okay, let's formalize the montecarlo method for integration.

As said before, we want this identity:
\dfn{Monte Carlo Integral Representation}{
  Let $X$ be a random variable with probability density function $f(x)$ defined on a domain $\mathcal{X} \subseteq \mathbb{R}^d$. We define the target integral $I$ as the expected value of the function $h(X)$:
  \[
    I = \mathbb{E}_f[h(X)] = \int_{\mathcal{X}} h(x) f(x) \, dx
  \]
  assuming that the expectation exists and is finite, i.e., $\mathbb{E}_f[|h(X)|] < \infty$.
}
Then it's time for exstimator of montecarlo:
\dfn{Monte Carlo Estimator for Integration}{
  Given $n$ independent and identically distributed (i.i.d.) samples $X_1, X_2, \dots, X_n$ drawn from the probability density function $f(x)$, the Monte Carlo estimator $\overline{h}_n$ for the integral $I$ is defined as:
  \[
    \overline{h}_n = \frac{1}{n} \sum_{i=1}^{n} h(X_i)
  \]
  where each $X_i$ is a sample from the distribution defined by $f(x)$.
}

\ex{3.1}{
  use integrate() to compute
  \[
    \Gamma(\lambda) = \int_{0}^{+\infty} x^{\lambda - 1} exp(-x) dx \ \ D = [0, +\infty[
  \]
  which is the gamma distribution.

\lstinputlisting[language=R]{code/Examples/MC_Int/Ex3_1.R}

Here integrate works correctly.
}

\ex{3.2}{
  Consider a sample of $ n = 10 $ RVs from a Cauchy distribution with location parameter $ \theta = 350 $
  \[
    X_1, X_2, ..., X_{10} \sim Cauchy(\theta = 350)
  \]
  We want to calculate the marginal probability of the data we have conditional to the parameter theta
  \[
    P(X_1,...,X_n) = \int_{\Theta}^{} P(X_1,...,X_n | \theta)P(\theta)d\theta
  \]
  We can reconcile the Bayesian representation with the usual frequentist approach by stating that $ P(\theta) \alpha 1 $ (flat prior) so that no value of $ \theta $ is favoured, so we can say
  \[
    = \int_{-\infty}^{+\infty} P(X_1,...,X_n | \theta)d\theta
  \]
  Assuming iid data, we can simplify as such
  \[
    = \int_{-\infty}^{+\infty} \prod_{i=1}^{n} P(X_i | \theta) d\theta
  \]
  But the Cauchy is a continuous distribution, so we can only use the distribution
  \[
    = \int_{-\infty}^{+\infty}\prod_{i=1}^{n} \frac{1}{\pi(1+(x_i - \theta)^2)} d\theta
  \]

  \textbf{Numerical Approach}
  \begin{itemize}
    \item integrate()
    \item area()
  \end{itemize}

  \lstinputlisting[language=R]{code/Examples/MC_Int/Ex3_2.R}
  
  In this case, integrate doesn't work (as the mean of the distribution isn't defined). Area works, but we need to know where to integrate so it's kinda useless.
}

\subsection{Calculating expectation and variance}
This method is very usefull for approximating the expected value and the variance of a distribution. This can be done by setting $h(x)$ to specific values:

\begin{itemize}
  \item $h(X) = X$: by setting the function as a constant, we just get the formula for the expectation of $f$
  \[
    \overline{X}_n \rightsquigarrow \int_{D} x f(x) dx
  \]
  Which is expected seen as the sample mean is an \textit{unbiased estimator} of the expected value in a population.

  \item $h(X) = (X - E_f\left[X\right])^2$: in this case we get the formula for the variance of $X$:
  \[
    \int_D (x - E\[X\])^2 f(x) dx
  \]
\end{itemize}


\nt{
  It's possible to sample a long sequence and reuse it for different integrals with the same density, seen as integration is not a random operation. Meanwhile, we can't do the same for sampling as it's inherintly random.
}

\subsection{Considerations}
Monte Carlo error $ v_n $ is a bonus, and it's not always a proper estimate (when the function is not square-integrable).
It's possible to assert this visually by looking at the purple lines: if they don't converge or converge too slowly, the error isn't computable.

\ex{Rare Classical Monte Carlo L}{
  Consider the CDF of a normal distribution:
  \[
    \Phi(t) = \int_{-\infty}^{t} \frac{1}{\sqrt{2\pi}} e^{-y^2/2} dy
  \]
  We can set $ h(X) = 1_{X \leq t} $ (indicator function), as it works for all values of $ t $ and doesn't change the value.

  \lstinputlisting[language=R]{code/Examples/MC_Int/Ex_3_4.R}

  If we want a precision of four decimals, we need to set
  \[
    2 \sqrt{\frac{1}{4n}} \leq 10^{-4}
  \]
  Which gives $ n = 10^8 $ needed samples from the distribution, crazy. This is even more problematic as we move from the center towards lower values.

  This is a problem inherent to the formulation of the integral itself.
}

\section{Importance Sampling}
This is still a method to compute integrals (name is counter-intuitive). It still uses the LLN, it's just a chep trick.
\[
  E_f[h(X)] = \int_{D}^{}h(x)f(x) dx = \int_{D}^{}h(x)\frac{f(x)}{g(x)}g(x) dx
\]
where $ g(x) $ is the \textbf{proposal distribution} (like in AR alg). We can now take the first part of the integrand and set it as a new function
\[
  \int_{D}^{}h'(x)g(x)dx = E_g\left[\frac{h(X)f(X)}{g(X)}\right]
\]
The difference is that now we're not sampling from $ f $, but from our proposal $ g $ that we chose.

\subsection{Conditions}
\begin{itemize}
  \item Support of $ (h \times f) $ is contained in the support of $ g $
  \item $ Var(h(X)f(X)/g(X)) < \infty$, which basically means that the tails of $ g $ have to be heavier
\end{itemize}

\subsection{Algorithm}
\begin{enumerate}
  \item Sample from $ g(x) $
  \item Transform as $ \frac{f(x)h(x)}{g(x)} = w h(x) $, where $ w $ is the \textit{importance weight} (so we compute the weighted avg. of $ h(x) $)
\end{enumerate}

\ex{3.4}{
  $ E_f[h(X)] $ when $ f $ is a normal pdf (standard gaussian), $ h(x) = exp \{- \frac{(x-3)^2}{2}\} + exp \{-\frac{(x-6)^2}{2}\} $
  \begin{enumerate}
    \item Show that the expectation can be computed in closed form (not needed)
    \item Build a classical MC approx of the expectation using a sample from $ N(0,1) $ of size $ 10^3 $ and also produce an error evaluation $ v_n $
    \item
  \end{enumerate}


}
