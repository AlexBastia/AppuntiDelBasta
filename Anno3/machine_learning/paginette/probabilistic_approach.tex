\chapter{Probabilistic approach}
\section{core idea}
we have two main points of views:
\begin{itemize}
    \item \textbf{traditional view}: we wanna to approximate a function $f:X\to X$
    \item \textbf{Probabilist view}: we wanna compute probablilities: $p: P( Y\mid  X)$
\end{itemize}
\subsection{Probs basics}
\subsubsection{Random variables}
A random variables $X$ represents an oyt come about which we're ncertain
\ex{Random variables}{
\begin{itemize}
    \item $X=$\texttt{true} if a randomly drawn stdent is male
    \item $X=$ first name of the student
    \item $X=$\texttt{true} if a randomly drawn stdent have the same birthday
\end{itemize}
}
 
Formal def:
\dfn{Probs variables}{
    the set $\Omega$ of the possible outcomes is called the sample space. It is said random variable a measurable function over $\Omega$:
    \begin{itemize}
        \item Discrete: $\Omega \to \{m,f\}$
        \item Continuos: $\Omega\to \mathbb{R}$
    \end{itemize}
}
 
\dfn{Probs def}{
    it is defined $P(X)$ is the fraction of times $X$ is true in repeated runs of the same experiment.
}

\nt{
    The definition requires that all samples 
}

Pay attention:
\wc{
    bad examples
}{
    Sample space, let $\Omega$ be a space made the possibile sum:
    \[
        \Omega = \{2,3,4,\dots, 12\}
    \]
    Problem: not all sums are equally likely! It should be:
    \[
        \begin{array}{c}
            P(sum = 2) = 1/11\\
            P(sum = 7) = 1/11
        \end{array}
    \]
    but in reality:
    \begin{itemize}
        \item Sum = 2: can only happen one way: $(1,1)$
        \item Sum = 7: can happen six ways:$(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)$
    \end{itemize}

    so 
    \[
        P(sum = 2) \neq P(sum = 7)
    \]
}

A correct approach is 
\clm{correct approach}{}{
    Be $\Omega = {(1,1), (1,2), (1,3), ..., (6,5), (6,6)}$, where $|\Omega|=36 $outcomes

    each pair has equally probability = $\frac{1}{36}$
    
    Now here is a correctly computing:
    \[
        \begin{array}{c}
            P(sum = 2) = \frac{|{(1,1)}|}{36} = \frac{1}{36}\\
            P(sum = 7) = \frac{|{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)}|}{36} = \frac{6}{36}
        \end{array}
    \]
}

\subsubsection{The Axioms of Probability Theory}
These are the fundamental rules that make probability a "reasonable theory of uncertainty":

\ax{Axioms of probability theory}{
    \begin{align}
        &\text{(1) Non-negativity: } && 0 \leq P(A) \leq 1 \quad \text{for all events } A. \\
        &\text{(2) Normalization: } && P(\Omega) = 1. \\
        &\text{(3) Countable additivity: } && 
        \text{If } A_1, A_2, \dots \text{ are disjoint, then } 
        P\!\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty P(A_i).
    \end{align}
}

Then:

\cor{consequences of the axioms}{
\begin{itemize}
    \item Monotonicity: If $A \subseteq B$, then $P(A) \le P(B)$
    \item Union rule (for two events): $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    \item $P(True) = 1$
    \item $P(False) = 0$
\end{itemize}
}

\begin{center}
    \includegraphics[width=5cm]{probs_ax.png}
\end{center}

\subsubsection{Derivied theorems}
\cor{Complement Rule
}{
    \[
        P(\lnot A) = 1- P(A)
    \]
}
\pf{Dm}{
\[
P(A \cup \neg A) = P(A) + P(\neg A) - P(A \cap \neg A)
\]

But: 
\[
P(A \cup \neg A) = P(\text{True}) = 1
\quad \text{and} \quad
P(A \cap \neg A) = P(\text{False}) = 0
\]

Therefore:
\[
1 = P(A) + P(\neg A) - 0
\quad \implies \quad
P(\neg A) = 1 - P(A) \qed
\]

}

\cor{Partition Rule}{
    \[
        P(A) = P(A \cap B) + P(A \cap \neg B)
    \]
}
\pf{Proof}{
    \[
\begin{aligned}
A &= A \cap (B \cup \neg B) &\text{[since $B \cup \neg B$ is always True]}\\
  &= (A \cap B) \cup (A \cap \neg B) &\text{[distributive law]}
\end{aligned}
\]

Hence,
\[
\begin{aligned}
P(A) &= P((A \cap B) \cup (A \cap \neg B)) \\
     &= P(A \cap B) + P(A \cap \neg B) - P((A \cap B) \cap (A \cap \neg B)) \\
     &= P(A \cap B) + P(A \cap \neg B) - P(\text{False}) \\
     &= P(A \cap B) + P(A \cap \neg B)
\end{aligned}
\]

}
\subsubsection{Multivalued Discrete Random Variables}
\dfn{
    k-value Discrete Random Variables
}{
    A random variable $A$ is \textit{$k$-valued discrete} if it takes exactly one value from 
    \[
    \{\nu_1, \nu_2, \dots, \nu_k\}.
    \]
}

\mprop{Key proprieties}{
    \begin{enumerate}
    \item \textbf{Mutual exclusivity:} For $i \neq j$,
    \[
        P(A = \nu_i \cap A = \nu_j) = 0
    \]

    \item \textbf{Exhaustiveness:}
    \[
        P(A = \nu_1 \cup A = \nu_2 \cup \dots \cup A = \nu_k) = 1
    \]
\end{enumerate}

}

\subsubsection{Conditional Probability}
\dfn{Conditional probs}{
    The Conditional probs of the event $A$ \textit{given} the event $B$ is defined as the quantity
    \begin{center}
        \begin{math}
            P(A\mid B) = \frac{P(A\cap B)}{P(B)}            
        \end{math}
    \end{center}
}

\cor{Cahin roule}{
    \[
        P(A\cap B) = P(B)P(A\mid  B)= P(A) P(B\mid A)
    \]
}
\subsubsection{Independent Events}
\dfn{Independent Events}{
    Events $A$ and $B$ are independent when:
    \begin{center}
        \begin{math}
            P(A\mid  B) = P(B)
        \end{math}
    \end{center}
}
(Meaning: B provides no information about A.)
\cor{consequences}{
    \begin{itemize}
        \item $P(A\cap b) = P(A)P(B)$ (from chail roule)
        \item $P(B|A) = P(B)$ (symmetry)
    \end{itemize}
}

\subsubsection{Bayes' Rule: The Heart of Probabilistic ML (ok chat... really?)}
\thm{Bayes's roule}{
    Now we have Bayes roule
    \begin{center}
        \begin{math}
            P(A\mid  B) = \frac{P(A)  P(B\mid A)}{P(B)}
        \end{math}
    \end{center}
}
\pf{Proof}{
    It's true by the chain roule that: $P(A \cap B) = P(B)  P(A\mid B)$. It's true also the reverse case $P(A \cap B) = P(B) · P(A\mid B)$.

    Since both expressions equal $P(A \mid B)$, they must equal each other:
    \[
        P(A)  P(B\mid A) = P(B)  P(A\mid B) 
    \]
    that it's equal to
    \[
        P(A\mid B) =\frac{[P(A)  P(B\mid A)]}{P(B)}
    \]
}
\ex{The trousers problem}{
    Setup:
    \begin{itemize}
        \item 60\% of students are boys, 40\% are girls
        \item girls wear in the same number skirt and trousers
        \item boys only wear trousers
    \end{itemize}
    If we see a student wearing trousers, what is the probability that is a girl?
}
\pf{Solution}{
    The probs a priori that a strudent is a girl is
    \[
        P(G) = \frac{2}{5}
    \]
    the probability that a student wears trousers is
    \[
        P(T) = \frac{1}{5} + \frac{3}{5} = \frac{4}{5}
    \]
    the probability that a student wear trousers, given that the student is a girl, is
    \[
        P(T\mid G) = 1/2
    \]
    So 
    \[
        P(G\mid T) =\frac{p(G)p(T\mid G)}{P(T)}=\frac{2/5\cdot 1/2}{4/5} = 1/4
    \]
}

\paragraph{Machine Learning Form}
\paragraph{Machine Learning Form}
For discrete $Y$ with values $\{y_1, y_2, \ldots, y_m\}$ and $X$ with values $\{x_1, x_2, \ldots, x_n\}$:
\[
P(Y = y_i \mid X = x_j) = \frac{P(Y = y_i) \cdot P(X = x_j \mid Y = y_i)}{P(X = x_j)}
\]

\textbf{Expanding the denominator:}
\begin{align*}
P(X = x_j) &= \sum_{i} P(X = x_j, Y = y_i) \quad \text{[sum over all $Y$ values]} \\
           &= \sum_{i} P(Y = y_i) \cdot P(X = x_j \mid Y = y_i) \quad \text{[chain rule]}
\end{align*}

\textbf{Complete Bayes' Rule:}
\[
P(Y = y_i \mid X = x_j) = \frac{P(Y = y_i) \cdot P(X = x_j \mid Y = y_i)}{\sum_{i} P(Y = y_i) \cdot P(X = x_j \mid Y = y_i)}
\]

\textbf{Terminology:}
\[
\underbrace{P(Y \mid X)}_{\text{posterior}} = \frac{\overbrace{P(X \mid Y)}^{\text{likelihood}} \cdot \overbrace{P(Y)}^{\text{prior}}}{\underbrace{P(X)}_{\text{marginal}}}
\]

\begin{itemize}
    \item \textbf{Posterior} $P(Y \mid X)$: What we want -- probability of $Y$ given observed $X$
    \item \textbf{Likelihood} $P(X \mid Y)$: How likely is $X$ if $Y$ is true?
    \item \textbf{Prior} $P(Y)$: What we believed before seeing $X$
    \item \textbf{Marginal} $P(X)$: Overall probability of observing $X$ (normalization constant)
\end{itemize}

\textbf{Alternative form:}
\[
\text{Posterior} = \frac{\text{Likelihood} \cdot \text{Prior}}{\text{Marginal Likelihood}}
\]
where:
\[
\text{Marginal} = \sum_{Y} P(X \mid Y) \cdot P(Y)
\]

The term ``marginal'' means we've \textbf{marginalized} (integrated/summed) over $Y$.

\section{The Joint Distribution}

\dfn{Joint Distribution}{
    Let $X_1, X_2,\dots, X_n$ be discrete random variables. The \textit{joint probability distribution} (or \textit{joint distribution}) of these variables is the function:
    \begin{center}
        \begin{math}
            P(X_1 = x_1, X_2 = x_2, \dots, X_n = x_n)
        \end{math}
    \end{center}

    which assigns to every possible combination of values \( (x_1, x_2, \dots, x_n) \) the probability that the random variables simultaneously take those values.

    Formally, for discrete variables, the joint distribution satisfies:
    \begin{itemize}
        \item \( 0 \leq P(x_1, x_2, \dots, x_n) \leq 1 \) for all \( (x_1, \dots, x_n) \)
        \item \( \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} P(x_1, x_2, \dots, x_n) = 1 \)
    \end{itemize}
}

Let's see an example
\ex{Joint distribution}{
    \begin{itemize}
        \item build a table with all possible combinations of values of random variables (features)
        \item compute the probability for any different combination of values
    \end{itemize}

    \begin{center}
        \includegraphics[width=6pt]{jd_img.png} % the table of the document
    \end{center}

    This table is the "Joint distribution"!

    Having that we may compute the probability of any event expressible as a logical combination of the features, with this formula
    \[
        P(E) = \sum_{row\in E} (row)
    \]
    in words for calculating an event we must add each row that is contained by the event. Let's provide an example (of an example)

        Let us compute the probability $P(M, poor)$
        \begin{center}
            \includegraphics[width=6cm]{jd_ex.png}
        \end{center}
    

    we have: $P(M, poor) = 0.33 + 0.13 = 0.46$
}

\subsection{Inference with the Joint distribution}
Here are with the inference:
\dfn{Contintional probability}{
    Let \( E_1 \) and \( E_2 \) be two events defined as logical conditions over subsets of the random variables (e.g., \( E_1: X_i = a, X_j = b \); \( E_2: Y = y \))

    Then, \textit{conditional probability} of \( E_1 \) given \( E_2 \) is:
    \[
        P(E_1 \mid E_2) = \frac{P(E_1 \land E_2)}{P(E_2)} = 
        \frac{\displaystyle\sum_{{row} \in (E_1 \land E_2)} P({row})}
        {\displaystyle\sum_{{row} \in (E_2)} P({row})}
    \]
}

for instance:
\ex{Conditional probability}{
    Let's compute $P(M|poor ) =\frac{P(M \land poor)}{P(poor)}$. We know that $P(M, poor ) = 0.46$. Let us compute $P(poor )$:
    \begin{center}
        \includegraphics[width=6cm]{joint_inf.png}
    \end{center}
    Easy! $P(poor) = .75 \land P(M|poor) = 0.46/0.75 = 0.61$
}

\subsection{Complexity issues}
Let us build the joint table relative to
\[
    P(Y = wealth|X_1 = gender , X_2 = orelav.)
\]

\begin{center}
    \includegraphics[width=6cm]{comp_issue.png}
\end{center}
To fill the table we need to compute $4=2^2$ parameters

If we have $n$ random variable $X = X_1 \times X_2 ,\dots, X_n$ where each $X_i$ is
boolean, we need to compute $2^n$ parameters. These parameters are \textit{probabilities}: to get reasonable value we would need a huge amount of data.

In particular the The Joint Distribution Requires \textit{Exponential parameters}
\ex{features and params}{
    \begin{itemize}
        \item With just 10 binary features, you need $2^{11}-1=2047$ parameters
        \item With 20 features: over 1 million parameters
        \item With 100 features: $2^{101}$ a number larger than the estimated atoms in the observable universe.
    \end{itemize}
}
This is computationally and statistically infeasible.
\subsubsection{USing Bayes}
for reducing complexity, we can rewrite the formula with the Bayes' rule:
\[
    P(Y=y_i\mid X=x_j) = \frac{P(Y=y_i)\cdot P(X = x_j\mid Y = y_i)}{\sum_i P(Y=y_i)\cdot P(X=x_j \mid Y=y_i)}
\]
generalising:
\[
    P(Y \mid X_1, X_2, \dots, X_n) = \frac{P(Y) \cdot P(X_1, X_2, \dots, X_n \mid Y)}{P(X_1, X_2, \dots, X_n)}
\]

But... there is a problem, it's required to know 
\[
    P(X_1, X_2, \dots, X_n \mid Y)
\]

that is the joint distribution of the features given $Y$, that requires, another time, $2^n$ params

\subsection{Naive Bayes}
For atteniung the complexity, it's possible assume an indipendencies conditional hypotesis, called "Naïve Bayes":
\[
    P(X_1,X_2,\dots,X_n|Y)=\prod_{i}P(X_i |Y)
\]
So given $Y$, $X_i$ and $X_j$ are independent from each other. In other therms:
\[
    P(X_i|X_j,Y)=P(X_i|Y)
\]
\nt{
    This means: onece we know $Y$, the feature $X_i \forall i$ are independents between each others
}
\ex{example 1}{
    A box contains two coins: a regular coin and a fake two-headed coin ($P(H) = 1$).Choose a coin at random, toss it twice and consider the following events:

    \begin{itemize}
        \item $A =$ First coin toss is H
        \item $B =$ Second coin toss is H
        \item $C =$ First coin is regular
    \end{itemize}
}
\ex{example 2}{
    For individuals, height and vocabulary are not independent, but they are if age is given.
}

\subsubsection{Giga formula with naive bayes}
\thm{Bayes rule}{
    \[
       P(Y = y_i \mid X_1, \dots, X_n) = \frac{P(Y = y_i) \cdot P(X_1, \dots, X_n \mid Y = y_i)}{P(X_1, \dots, X_n)}
    \]
}
\pf{Proof}{
    Left to mesco as exercice
}

\thm{Naïve Bayes}{
    \[
    P(Y = y_i \mid X_1, \dots, X_n) = \frac{P(Y = y_i) \cdot \prod_j P(X_j \mid Y = y_i)}{P(X_1, \dots, X_n)}
    \]
}
\pf{Proof}{
    Left to Bonzo as exercice
}

\thm{Classification of a new sample $x^{\text{new}} = \langle x_1, \dots, x_n \rangle$}{
    \label{thm:cne}
    Given a new instance represented by the feature vector $x^{\text{new}} = (x_1, x_2, \dots, x_n)$, 
    the predicted class is obtained as:
    \[
        Y^{\text{new}} = \arg\max_{y_i} \, P(Y = y_i) \cdot \prod_{j} P(X_j = x_j \mid Y = y_i)
    \]
}

\pf{Proof}{
    Left to Bastiality as an exercise.
}

\nt{
    Theorem~\ref{thm:cne} expresses the decision rule of the Naïve Bayes classifier.
    Given a new vector of features $x^{\text{new}} = (x_1, x_2, \dots, x_n)$, 
    we estimate the most probable class $y_i$ by maximizing the posterior probability 
    $P(Y = y_i \mid X_1 = x_1, \dots, X_n = x_n)$, 
    which—under the conditional independence assumption—reduces to the product of the prior $P(Y = y_i)$ 
    and the individual likelihoods $P(X_j = x_j \mid Y = y_i)$.
}

\section{Learning algorithm}
Given discrete Random Variables $X_i, Y$, there are two phases
\begin{itemize}
    \item \textbf{Training}: in this phases the maching learn from the data of training set, estimating two types of probs:
    \begin{itemize}
        \item \textbf{Prior} (prob of the classes). For any possible value $y_k$ of $Y$ , estimate
        \[
            \pi_k = P(Y = y_k)
        \]
        example: if 9 out of 14 matches are "Play = Yes", then $\pi_{yes}= \frac{9}{14},\quad \pi_{no}= \frac{5}{14} $

        \item \textbf{Likelihoods}:(conditional probabilities of features). for any possible value $x_{ij}$ of $X_i$ estimate:
        \[
            \theta_{ijk} = P(X_i = x_{ij}\mid Y = y_k)
        \]
        It's the probability that a certain feature $X_i$ assumes the value $X_{ij}$, given $y_k$.

        example: $P(Outlook=Sunny\mid Play=Yes)=\frac{2}{9}$
    \end{itemize}
    \item \textbf{Classification of $a^{new} = \langle a_1 , \dots a_n \rangle$} (a vector with $n$ observed values (one for each feature)). We want to establish which class it belong to
    
    decision-making formula:
    \[
    \begin{aligned}
    Y^{\text{new}} &= \arg \max_{y_k} P(Y = y_{k}) \cdot \prod_{i} P(X_{i} = a_{i} \mid Y = y_{k}) \\
    &= \arg \max_{k} \pi_{k} \prod_{j} \theta_{ijk}
    \end{aligned}
    \]

    where:
    \begin{itemize}
        \item $P(Y=y_k)$: prior
        \item $P(X_i=a_i\mid Y=y_k)$: likelihood for each features
        \item the prod $\prod_{i}$ is given by Naive assumption
    \end{itemize}
\end{itemize}

\ex{a good day to play tennis?}{
    we wanna build a model that, given certain weather conditions, predict whether it is a good day to play tennis or not 
    
    Our class variable is:
    \[
        Y=Play\in\{Yes,No\}
    \]
    and the features observed are:
    \[
        X_1 = Outlook \quad X_2 = Temp \quad X_3=Humidity \quad X_4=Wind
    \]

    Here we have the dataset:
    \begin{table}[H]
\centering
\caption{Dataset for the \textit{Play Tennis} classification problem}
\label{tab:play_dataset}
\begin{tabular}{ccccc}
\toprule
Outlook & Temp & Humidity & Wind & Play \\
\midrule
Sunny & Hot & High & Weak & No \\
Sunny & Hot & High & Strong & No \\
Overcast & Hot & High & Weak & Yes \\
Rain & Mild & High & Weak & Yes \\
Rain & Cool & Normal & Strong & No \\
Overcast & Cool & Normal & Strong & Yes \\
Sunny & Mild & High & Weak & No \\
Sunny & Cool & Normal & Weak & Yes \\
Rain & Mild & Normal & Weak & Yes \\
Sunny & Mild & Normal & Strong & Yes \\
Overcast & Mild & High & Strong & Yes \\
Overcast & Hot & Normal & Weak & Yes \\
Rain & Mild & High & Strong & No \\
\bottomrule
\end{tabular}
\end{table}

TODO: TABELLA FATTA FARE DA UN LLM NON È VENUTA BENISSIMO

    \textbf{Calculating the prior}

    From the dataset we can compute the prior probabilities of the class variable $Y$:

    \[
    P(Y = \text{Yes}) = \frac{9}{14}, \qquad
    P(Y = \text{No}) = \frac{5}{14}.
    \]

    These represent the empirical frequencies of the two possible outcomes of $Y$.

    \medskip
    \textbf{Calculating the likelihoods}

    For each feature $X_i$ and each class $Y = y_k$, we estimate the conditional probabilities
    \[
    P(X_i = x_{ij} \mid Y = y_k),
    \]
    that is, the probability of observing a certain feature value $x_{ij}$ given that the class is $y_k$.

    For example:

    \[
    P(\text{Outlook} = \text{Sunny} \mid Y = \text{Yes}) = \frac{2}{9}, \qquad
    P(\text{Outlook} = \text{Sunny} \mid Y = \text{No}) = \frac{3}{5}.
    \]

    These values are computed as the relative frequencies in the dataset.

    \medskip
    \textbf{Classification of a new instance}

    Suppose we want to classify the new day
    \[
    x^{\text{new}} = (\text{Outlook} = \text{Sunny},\ 
    \text{Temp} = \text{Cool},\ 
    \text{Humidity} = \text{High},\ 
    \text{Wind} = \text{Strong}).
    \]

    We apply the Naïve Bayes decision rule:
    \[
    Y^{\text{new}} = 
    \arg\max_{y_i} P(Y = y_i) 
    \cdot 
    \prod_j P(X_j = x_j \mid Y = y_i).
    \]

    \medskip
    For $Y = \text{Yes}$:
    \[
    P(\text{Yes}) \cdot P(\text{Sunny}|\text{Yes}) \cdot P(\text{Cool}|\text{Yes}) 
    \cdot P(\text{High}|\text{Yes}) \cdot P(\text{Strong}|\text{Yes})
    = \frac{9}{14} \cdot \frac{2}{9} \cdot \frac{3}{9} \cdot \frac{3}{9} \cdot \frac{3}{9} 
    \approx 0.0053
    \]

    For $Y = \text{No}$:
    \[
    P(\text{No}) \cdot P(\text{Sunny}|\text{No}) \cdot P(\text{Cool}|\text{No}) 
    \cdot P(\text{High}|\text{No}) \cdot P(\text{Strong}|\text{No})
    = \frac{5}{14} \cdot \frac{3}{5} \cdot \frac{1}{5} \cdot \frac{4}{5} \cdot \frac{3}{5} 
    \approx 0.0205
    \]

    \medskip
    \textbf{Decision:}

    Since
    \[
    P(Y = \text{No} \mid x^{\text{new}}) > P(Y = \text{Yes} \mid x^{\text{new}}),
    \]
    the predicted class is
    \[
    Y^{\text{new}} = \text{No}.
    \]

    \medskip
    Therefore, according to the Naïve Bayes model, it is \textbf{not a good day to play tennis}.
}

\section{Generative techniques}
When we want to classify data (determine which category Y something belongs to given its features X), there is a fundamentally philosophical approaches, the \textbf{Generative Approach} that "learn how each class generates data", here is a sketch:
\begin{itemize}
    \item Ask: "How does each class produce its characteristic data?"
    \item Model: $P(X|Y)$ (probability of features given category)
    \item Then use Bayes' Rule to reverse it and get $P(Y|X)$
\end{itemize}

The term "Generative" come from the fact we're modeling the data generation process. We're essentially saying: "if I knew the category $Y$, I could generate/simulate typical data $X$ from that category"

\subsection{Big example: the visual intuition}
We want to calssify images into categories $\{0,1,\dots,9\}$ The generative approach says: "for each digit, learn waht imgs form that category typically look like. Then given a new img, see which category would most naturally produce such an img"

\begin{center}
    \includegraphics[width=6cm]{digit_distribution.png}
\end{center}

Okay, now we want to classify a new img:
\begin{center}
    \includegraphics[width=6cm]{digit_seven.png}
\end{center}
\textbf{Which of these distributions would most likely have generated this image?}

Mathematically, for each category k:
\[
    \begin{aligned}
        \text{Score for category k} &= P(Y = k) \cdot P(X = \text{image} | Y = k)\\
                     &= \text{Prior} \cdot \text{Likelihood}
    \end{aligned}
\]

You pick the category with the highest score. You're asking which generative mode (which category's distribution) best explains the observed data

\nt{
    The "score" is just the numerator of byes rule:
    \[
        P(Y = k | X=\text{image}) = [P(Y = k) \cdot P(X=\text{image} | Y = k)] / P(X=\text{image})
    \]
}

\subsubsection{Joint Distribution vs. Naïve Bayes}
Ideally we'd want to model the Complete joint distribution:
\[
    P(X_1, X_2, \dots, X_n | Y = y_i)
\]
For MNIST, this would be the distribution of all $784$ pixels ($28\times 28$ image) for images of each digit. This distribution would capture all the correlations between pixels - how pixel 1 relates to pixel 2, how groups of pixels form edges and curves, etc. Howver, modeling the full joint distribution for $784$ dimensions is impossibly complex. We'd need:
\begin{itemize}
    \item $2^{784}$ parameters just for binary pixels (more than atoms in the universe!)
    \item An astronomical amount of training data
\end{itemize}

So we play the card "Naïve Bayes" with 2500 atk and 1000 def:
\[
    P(X_1,\dots, X_n \mid y=y_i) \approx \prod_{j} P(X_j\mid Y=y_i)
\]

The assumption (effect) is: Given the category $Y$, all pixels are \textit{independent}. This means we model each pixel separately:
\[
    \begin{aligned}
        &P(X_1 | Y = 0), P(X_2 | Y = 0), ..., P(X_{784} | Y = 0)  \\
        &P(X_1 | Y = 1), P(X_2 | Y = 1), ..., P(X_{784} | Y = 1)  
    \end{aligned}
\]
This is computationally feasible, but we've lost all information about how pixels relate to each other!
\subsection{Caution 1: The Zero Probability Problem}

From a previous example if we have $P(Play = No|Outlook = Overcast)$ the result is $\theta_Overcast,No = 0$, This happened because in our training data, we never observed a "No" (don't play tennis) when the outlook was overcast

\nt{
    Remember the classification formula:
    \[
        \text{Score} = \pi_k \cdot \prod_{i} \theta_{ijk}
    \]
    If any single $\theta_{ijk}= 0$, the entire product becomes zero. So A single feature value you've never seen in training can completely eliminate a category from consideration, even if all other features strongly support it!
}

\subsection{Caution 2: The Independence Assumption}
Naïve Bayes assumes events are independent from each other (given Y). What if this is not the case?

\subsubsection{The XOR Problem: A Fatal Limitation}

Consider random binary images where pixels are either 0 or 1. We classify based on two pixels: $p_1$ and $p_2$.

\textbf{Classification rule:}
\begin{itemize}
    \item \textbf{Category A:} if $p_1 = p_2$ (both same) — Images: $\{(0,0), (1,1)\}$
    \item \textbf{Category B:} if $p_1 \neq p_2$ (different) — Images: $\{(0,1), (1,0)\}$
\end{itemize}

This is an \textbf{XOR (exclusive OR)} relationship — a simple logical rule.

\textbf{What Naïve Bayes learns:}

For Category A (training: $(0,0)$ and $(1,1)$):
\begin{align*}
P(p_1 = 1 \mid A) &= \frac{1}{2} \quad \text{[one out of two has } p_1=1\text{]} \\
P(p_2 = 1 \mid A) &= \frac{1}{2} \quad \text{[one out of two has } p_2=1\text{]}
\end{align*}

For Category B (training: $(0,1)$ and $(1,0)$):
\begin{align*}
P(p_1 = 1 \mid B) &= \frac{1}{2} \quad \text{[one out of two has } p_1=1\text{]} \\
P(p_2 = 1 \mid B) &= \frac{1}{2} \quad \text{[one out of two has } p_2=1\text{]}
\end{align*}

\textbf{Result:} All probabilities are identical! For any test image $(a,b)$:
\begin{align*}
\text{Score}_A &= P(A) \cdot P(p_1=a \mid A) \cdot P(p_2=b \mid A) = 0.5 \cdot 0.5 \cdot 0.5 = 0.125 \\
\text{Score}_B &= P(B) \cdot P(p_1=a \mid B) \cdot P(p_2=b \mid B) = 0.5 \cdot 0.5 \cdot 0.5 = 0.125
\end{align*}

\textbf{Naïve Bayes cannot distinguish the categories!} It achieves only 50\% accuracy (random guessing) despite the trivially simple classification rule.

\textbf{Why?} The features $p_1$ and $p_2$ are \textbf{not independent} given the category — they're perfectly correlated:
\begin{itemize}
    \item In Category A: if $p_1 = 0$ then $p_2 = 0$ (with certainty)
    \item In Category B: if $p_1 = 0$ then $p_2 = 1$ (with certainty)
    \item $P(p_2 = 1 \mid p_1 = 1, A) = 1 \neq P(p_2 = 1 \mid A) = 0.5$ — violates independence!
\end{itemize}

\textbf{General lesson:} Naïve Bayes cannot learn relationships between features. It only learns how common each individual feature value is within each class, not how features combine, interact, or correlate.

\section{About Maximum Likelihood Estimation (MLE)}

\subsection{Problem definition}
In words, the Maximum Likelihood Estimation (MLE) is the parameter value that maximizes the probability of observing the given data. For instance, if we have a model where all possible outcomes are $0$ or $1$, the parameter to estimate is computed using the \textit{Bernoulli distribution}\footnote{If you don't know this, please read the \textit{Basta - Giolapalma notes for probability}} $P(w) = \theta^w (1-\theta)^{(1-w)}$ where $w\in\{0,1\}$ represents all possible outcomes. However, normally we have a dataset (in this case a sequence of observations $D = \{w_1, w_2, \ldots, w_n\}$) and we don't know $\theta$. The goal is to find the value of $\theta$ that makes our observed sequence most probable.

For independent observations, the likelihood is:
\[
    L(\theta|D) = P(D|\theta) = \prod_{i=1}^n P(w_i|\theta) = \prod_{i=1}^n \theta^{w_i}(1-\theta)^{1-w_i}
\]

Let $\alpha_0 = \sum_{i=1}^n w_i$ be the number of times we observed $1$ (successes), and $\alpha_1 = n - \alpha_0$ be the number of times we observed $0$ (failures). Then:
\[
    L(\theta|D) = \theta^{\alpha_0}(1-\theta)^{\alpha_1}
\]

The MLE is obtained by maximizing this likelihood (or equivalently its logarithm) with respect to $\theta$.

\dfn{MLE}{
    Given:
    \begin{itemize}
        \item A parametric probability model with parameter(s) $\theta$
        \item Observed data $D=\{x_1, \dots, x_n\}$
        \item A likelihood function $L(\theta|D) = P(D|\theta)$ (probability of data given parameters)
    \end{itemize}

    The \textit{Maximum Likelihood Estimator} is defined as:
    \begin{center}
        \begin{math}
           \hat{\theta}_{MLE} = \arg\max_\theta{L(\theta\mid D)} = \arg \max_\theta P(D|\theta)
        \end{math}
    \end{center}

    In words: $\hat{\theta}_{MLE}$ is the parameter value that maximizes the probability of observing the given data.
}

\subsection{Results for Bernoulli}



\thm{MLE for Bernoulli}{
    Given a set of $n$ i.i.d. (independent and identically distributed) observations $D = \{w_1, \ldots, w_n\}$ from a Bernoulli distribution with parameter $\theta$. The Maximum Likelihood Estimate (MLE) for $\theta$ is the sample frequency of successes.
    
    If $\alpha_0$ is the number of successes (observations equal to 1), then the estimate is given by:
    \[
        \hat{\theta}_{MLE} = \frac{\alpha_0}{n}
    \]
}


\pf{Derivation for Bernoulli}{
    For independent Bernoulli trials with outcomes $w_i \in \{0,1\}$, the likelihood is:
    \[
        L(\theta|D) = \prod_{i=1}^n P(w_i|\theta) = \prod_{i=1}^n \theta^{w_i}(1-\theta)^{1-w_i}
    \]
    
    Let $\alpha_0 = \sum_{i=1}^n w_i$ (number of $1$'s) and $\alpha_1 = n - \alpha_0$ (number of $0$'s). Then:
    \[
        L(\theta|D) = \theta^{\alpha_0}(1-\theta)^{\alpha_1}
    \]
    
    Taking the logarithm (which preserves the maximum since $\log$ is monotonically increasing):
    \[
        \ell(\theta) = \log L(\theta|D) = \alpha_0 \log(\theta) + \alpha_1 \log(1-\theta)
    \]
    
    To find the maximum, take the derivative and set to zero:
    \[
        \frac{d\ell}{d\theta} = \frac{\alpha_0}{\theta} - \frac{\alpha_1}{1-\theta} = 0
    \]
    
    Solving:
    \begin{align*}
        \frac{\alpha_0}{\theta} &= \frac{\alpha_1}{1-\theta} \\
        \alpha_0(1-\theta) &= \alpha_1\theta \\
        \alpha_0 &= \theta(\alpha_0 + \alpha_1) \\
        \hat{\theta}_{MLE} &= \frac{\alpha_0}{\alpha_0 + \alpha_1} = \frac{\alpha_0}{n}
    \end{align*}
    
    To verify this is a maximum, check the second derivative:
    \[
        \frac{d^2\ell}{d\theta^2} = -\frac{\alpha_0}{\theta^2} - \frac{\alpha_1}{(1-\theta)^2} < 0
    \]
    Since the second derivative is negative for $\theta \in (0,1)$, this confirms a maximum
}

\subsection{Multivalued case}
Now I presented the formula just for two possible cases using the Bernoulli distribution, and for multivalued cases? WE HAVE THE \textit{multinomial distribution}

\nt{Multinomial distribution is $P(X^n = \alpha_i\mid \theta) = c_{\alpha_i} \prod_{i}\theta_{i}^{\alpha_i}$. where $\alpha_i$ is the number of i in the sequence and $ c_{a_i}$ is a combinatorial constant not depending on $\theta$
}

\thm{MLE for Discrete Distributions}{
    The Maximum Likelihood Estimate (MLE) for the probability $\theta_i$ of each outcome is its observed sample frequency.

    If $\alpha_i$ is the number of times the i-th outcome has been observed in the $n$ trials, then the estimate for its probability is:
    \[
        \hat{\theta}_i = \frac{\alpha_i}{n}
    \]
}
\pf{Proof}{
    Basta exercice
}

\cor{MLE for Naïve Bayes Parameters}{
    As a direct consequence of the main theorem, the Maximum Likelihood Estimates for the parameters of a Naïve Bayes classifier are also given by their sample frequencies:

    \begin{enumerate}
        \item ($\pi_k$) The MLE for the prior probability of a class $y_k$ is its relative frequency in the dataset. 
        \[
            \hat{\pi}_k = P(Y = y_k) = \frac{\#\mathcal{D}\{Y = y_k\}}{|\mathcal{D}|}
        \]

        \item ($\theta_{ijk}$) The MLE for the conditional probability of a feature $X_i$ taking the value $x_{ij}$ given a class $y_k$, is the relative frequency of that feature value within the subset of data belonging to class $y_k$.
        \[
            \hat{\theta}_{ijk} = P(X_i = x_{ij} | Y = y_k) = \frac{\#\mathcal{D}\{X_i = x_{ij} \land Y = y_k\}}{\#\mathcal{D}\{Y = y_k\}}
        \]
    \end{enumerate}
}
\section{Document classification with Bag of Words (BoW) approach}
The Bag-of-Words ($BoW$) model is a technique for document classification, which involves assigning a document to a predefined category (like Sport, politics, Tech, ect...). The core idea is to treat a doc not like a sorted sequence of phrases but like a simple Bag where words have no order or grammar, in this technique the primary focus is on capturing the occurrence frequency of each word within the document.

\subsection{Training and classification}
Event $X_i=$ i-th word in the document: a discrete random variable assuming as many values as words in the language
\[
    \theta_{i,\text{\texttt{word}},\ell} = P(X_i = \text{\texttt{word}} \mid Y = \ell)
\]
In words: "the probability that in a document of the category $\ell$ the word
\texttt{word} appears at position $i$", but for the BoW technique we assume that we have a distribution independent from the position and that all event are independents so 
\[
     \theta_{i,\text{\texttt{word}},\ell} =  \theta_{j,\text{\texttt{word}},\ell}= \theta_{\text{\texttt{word}},\ell}
\]

Then with discrete random variables $X_i$, $Y$ let's defining the training and classification:
\begin{itemize}
    \item \textbf{Training}: 
    \begin{itemize}
        \item for any possible value $y_k$ of $Y$, estimate
        \[
            \pi_k=P(Y=y_k)
        \]
        \item for any possible value $x_ij$ of the attribute $X_i$ estimate
        \[
            \theta_{ijk} = P(X_i = x_{ij} \mid Y = y_k )
        \]
        (the condition prob that a certain word $x_{ij}$ appears in a doc, knowing that that doc belongs to category $y_k$)
    \end{itemize}
    \item \textbf{Classification of} $a^{\texttt{new}} = \langle a_1,\dots,a_n \rangle$ (new sequence of words):
    \[
    \begin{align*}
        Y^{\texttt{new}} &=\arg\max_{y_k} P(Y = y_k) \cdot \prod_{i} P(X_i = a_i | Y = y_k)\\
        & = \arg\max_{k}\pi_k\cdot \prod_{i}\theta_{ijk}
    \end{align*}
    \]
    where $x_{ij} = a_i$
\end{itemize} 

The probabilities needed for the classifier are estimated from the dataset using Maximum Likelihood Estimates (MLE's):
\begin{itemize}
    \item The MLE for the prior probability of a class, $\pi_k$, is the fraction of the documents in the training set that belong to category $y_k$
    \item The MLE for the conditional probability of a word, $\theta_{\text{\texttt{word}},k}$, is the frequency of that word within all documents belonging to category $y_k$
\end{itemize}

The classification formula involves multiplying many small probabilities, which can lead to numerical instability (underflow). To solve this, we can maximize the logarithm of the likelihood instead, since the logarithm is a monotonically increasing function and will not change the location of the maximum.

The classification rule becomes:
\[
    Y^{\texttt{new}} = \arg\max_{y_k} \left( \log(\pi_k) + \sum_i \log(\theta_{ijk}) \right)
\]
This can be further simplified by grouping identical words. If $n_j$ is the number of occurrences of the unique word $w_j$ in the new document, the sum becomes a weighted sum:
\[
    \sum_i \log(\theta_{ijk}) = \sum_j n_j \cdot \log(\theta_{jk})
\]
This final expression can be elegantly interpreted as a dot product in a high-dimensional vector space where each word of the vocabulary is a dimension