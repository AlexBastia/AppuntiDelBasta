% \begin{document}
\chapter{Neural Networks}
Only known way to implement deep learning. 

\section{The Artificial Neuron}
A \textit{Neural Network} is a structure that closly mimics the brain, as it's composed by interconnected layers of \textit{artificial neurons} that work together to create an output.

An artificial neuron has a set of inputs that are summed with a bias, and the output is calculated using an activation function. The function mustn't be linear, because we want to sequencially compose neurons and the composition of linear functions is just another linear function and doesn't add any complexity (could be calculated with just one neuron). 

There are different activation functions (threshold, logistic, hyperbolic, rectified linear (newest used in AlexNet, big jump)). 

The name neuron comes from the biological counterpart (obviously) which works in a similar way:

\begin{center}
  \includegraphics[width=0.4\textwidth]{cortical-neuron.png}
\end{center}

\begin{itemize}
  \item Dendritic tree: connections with other neurons (synapses)
    \item In the body the inputs are summed and passed through the axon hilock, which performs a sort of thresholding before being passed to other neurons
\end{itemize}

Comparing artificial neural networks to our brains, the number of neurons can be similar ($ 2\cdot 10^{10} $for the biggest models), but the sheer size doesn't necessarily correlate to a more intelligent system (other animals have bigger brains), we actually don't really know what else there is. The switching time for real neurons is actually slower than artificial ones, seen as it's a chemical reaction and not electrical. Each neuron is connected to many other neurons in the brain ($ 10^{4-5} $), reaching reaction times $ < 100 $, so the brain isn't very deep (number of intermediate nodes) and it's very parallelised.

\section{Topologies}
\dfn{Feed-forward}{
  Acyclic networks with unidirectional data flux.
}
\nt{
  Our brains are cyclic networks (also called \textit{recurrent}), but we won't be studying this case.
}

\subsection{Layers}
The network is built by sets of structured neurons that are combined to build the whole.

\begin{center}
  \includegraphics[width=0.4\textwidth]{img/layers}
\end{center}

Based on the number of layers, we have that a NN is:
\begin{itemize}
  \item \textbf{Shallow}: if there is only one hidden layer.
  \item \textbf{Deep}: if there are multiple hidden layers.
\end{itemize}

\subsubsection{Dense layers}
\dfn{Dense layer}{
  A layer in a NN is called \textbf{dense} when neurons in adjacent layers are all interconnected.
}

For each dense layer, each neuron has an input $I^n$, weights $W^n$ and a constant bias $B^1$ from which an output $O^1$ is calculated.
\[
    I^n \cdot W^n + B^1 = O^1
\]
This operation can be parallelised using the whole layer with a matrix of weights $W^{n \times m}$ (one array of weights for each neuron) and a vector of biases, using the same inputs over all the neurons. The input is a vector in multiple dimensions (tensor), and after algebraic manupulations a new tensor is returned. 

\nt{
  Dense layers usually work on \textit{flat} (unstructured) inputs, where the order of the elements is irrelevant.
}

\section{Features and deep features}

TODO: manca sta parte qua che fa jumpscare alla fine della lezione del 24 ottobre ma che il 6 novembre non la fa
TODO: boh ma sta roba qua non so dove sta, forse non l'ha fatta
TODO: @bastini plz finiscie te, che altrimenti questi mi diventano ApuntiDelGiolaBambas
Re: Yes yes ora mi sta mettendo a fare

\section{Successful Applications} % slides2 - 6 Nov
\subsection{Image Processing}

\subsubsection{ImageNet}
High resolution labeled image dataset covering 22k object classes.

A competition is held each year to classify given images.
\\
\textbf{Relevance}

Classfication models are typically composed of two parts:
\begin{itemize}
\item \textbf{Front-end}: extracts features from the input image and convers them into a vector
\item \textbf{Back-end}: uses the extracted features for classification
\end{itemize}
ImageNet is still importand because a front-end, \textbf{pre-trained over ImageNet}, can be reused for a lot of different applications (\textit{transfer learning}).

\subsubsection{Object Detection}
YOLO (You Only Look Once) is a real-time object detection system. It's a smilar problem to image classification, but has the extra functionality of localizing the object using a bounding box (smallest possible rectangle containing the object) inside the image.

The model actually returns many thousands of possible bounding boxes from an image, each with its own weight. It's up to another piece of software to select which ones to keep.

\subsubsection{Image Segmentation}
Segmentation adds even more detail to object detection by classifying each pixel (so basically object detection but with the bounding box being the actual outline of the object).

\subsubsection{Key-points detection}
Finds specific areas of an object to track (e.g. eyes, joints, ecc. to analyse human position/movement).

\subsubsection{Applications}
\begin{itemize}
\item Medical imaging
  \item Autonomous driving
  \item Pose estimation
  \item Activity recognition
  \item Video surveillance
  \item ...
\end{itemize}

\subsection{Generative Modeling}
The objective of generative modeling is to learn the \textbf{distribution} $ P(X) $ of training data - that is how points are distributed inside the feature space thay inhabit.

Typically, we aim to build a \textbf{generator} able to \textbf{sample} points according to the learned distribution.

But NN are \textbf{deterministic systems}, so how can we simulate a stochastic sampling procedure?

\subsubsection{The generator}
We know how to build \textbf{pseudo-random generators} for simple, known distributions (e.g. Gaussian).

So the problem reduces to learn a \textbf{transformation} mapping the known distribution to the actual distribution $ P(X) $.

Morally, the generator learns $ P(X|z) $, where $ z $ is the "\textbf{latent}" rapresentation of $ X $.

\subsubsection{Ancestral sampling}
Generation is thus a two stage process:
\begin{itemize}
\item Sample $ z $ according to a known prior distribution
\item Pass $ z $ as input to the generator, and process it to get a significant output.
\end{itemize}

Generative models (GANs, VAEs, Diffusion, ...) differ in the way the generatoris trained.

\subsubsection{The latent space}
The source space is the so called latent space.

Each latent point $ z $ contains \textbf{all information} needed to generate a complete sample, hence it can be seen as an \textbf{internal encoding} (latent representation) of the given sample.

Latent values must be disseminated with a \textbf{known, regular distribution} in their space (the so called prior distribution).

Important things to keep in mind:
\begin{itemize}
\item \textbf{Any} face is somewhere in the latent space
\item Generation is a \textbf{continuous} process: small modifications of the encoding produce small modifications of the output
\end{itemize}

\nt{
  Latent space points are also often compressed versions of the explicit representation, meaning they take up less phisical space.
}

\subsubsection{Conditional generation}
The generator tries to model $ P(X|z,c) $ where $ c $ is a condition integrating the latent encoding $ z $. This condition can be:
\begin{itemize}
\item A label
\item A segmentation
\item A text prompt
\item Another image
\item A sequence of frames
\item ...
\end{itemize}

\subsection{Natural Language Processing}
Main subfields in NLP
\begin{itemize}
\item \textbf{Natural language Understanding}: focus on interpreting and extracting meaning rom human language, includes
  \begin{itemize}
  \item sentiment analysis
  \item named entity recognition (NER)
  \item question answering
  \end{itemize}
\item \textbf{Natural Language Modeling}: focus on producing human-like text, includes
  \begin{itemize}
  \item text summarization
  \item dialogue generation
  \item story writing
  \item retrieval augmented generation
  \end{itemize}
\end{itemize}
\textbf{Speech recognition/generation} is an additional topic bridging text and audio.

\subsubsection{Embeddings}
NLU instruct models to understand context, ambiguity and nuances in language.

All previous tasks require a meaningful representation of words, provided by \textbf{embeddings}.

\dfn{Embeddings}{
  Mappings of words/sentences into a high-dimensional vector space. They help models understand relationships beween words, such as similarity, analogy and context.
}

These embeddings form the latent space of text representation, where the same transformations (e.g. going from singular to plural) have the same transformation vector.

\subsubsection{Bridging text and images}
CLIP (Contrastive Language-Image Pretraining) is a recent technology that allows the joint analysis of text and images.

It's trained on a dataset of text-image pairs, both mapped to a shared latent space where similar pairs have closer embeddings.

\subsubsection{Language modeling}
Images involve spatial relationships, often modeled \textbf{holistically} or in parallel.

NLP operates on \textbf{sequences} of tokens (words or subwords) where order is critical.

Given a sequence $ x $ of tokens $ x_1x_2...x_n $, we are interested in modeling the mechanisms underlying their concatenation.

Generative models estimate the joint probability of a text sequence
\[
  P(x_1x_2...x_n)
\]
This can be broken into \textbf{conditional probabilities} using the \textbf{chain rule}
\[
  P(x_1)P(x_2|x_1)P(x_3|x_1x_2)...P(x_n|x_1...x_{n-1})
\]
In practice, you can train the model to guess the next token completing the given sequence, enabling \textbf{sequential generation}(each token depends on its predecessor.

Different possible approaches are:
\begin{itemize}
  \item \textbf{N-grams}: estimate probabilities based on fixed window size (ex. Bigram model uses $ P(x_n|x_{n-1}) $). The main limitation is that it can't capture long-range dependencies.
  \item \textbf{Deep NN}:
    \begin{itemize}
    \item \textbf{Recursive NN}: introduce sequence processing but strugle with long term dependencies
    \item \textbf{Transformers}: with \textbf{self-attention} they can handle long-range context efficiently and are the foundation for modern NLP
    \end{itemize}
\end{itemize}


\section{Expressiness}
This part begins with an important question: \textit{Can we compute any function by means of a Neural Network?}.
If so, every function must have a corresponding set of weights that, when applied to a Neural Network, induce the calculation of said function.
But before that, we must first ask: \textit{Can we compute any function with a single neuron?}

These questions are related to the \textbf{Expressiveness} of Neural Networks, that is the capacity of a Neural Network to approximate any function
\subsection{Perceptron, the single layer case}
A single neuron is called a \textbf{Perceptron}. It is the simplest form of a Neural Network.

\dfn{Perceptron with binary threshold}{
  \label{dfn:perceptron}
  
  \begin{center}
    \includegraphics{./paginette/imgs/perceptron.png}
  \end{center}
  A perceptron is a function $ f: \mathbb{R}^n \rightarrow \{0, 1\} $ defined as:
  \[
    f(x) = \begin{cases}
      1 & \text{if } \sum_{i=1}^n w_i \cdot x_i + b \geq 0 \\
      0 & \text{otherwise}
    \end{cases}
  \]
  where $ x \in \mathbb{R}^n $ is the input vector, $ w \in \mathbb{R}^n $ is the weight vector and $ b \in \mathbb{R} $ is the bias.
}

\nt{
  It's easy to notice the the the bias $b$ set the position of threshold
}
\subsubsection{Hyperplanes}
For major information about hyperplanes see the notes of "Ottimizzazione combinatoria di Alex Basta e Qua Qua dancer"

\dfn{Hyperplane}{
  We can define a hyperplane in $ \mathbb{R}^n $ as the set of points $ x \in \mathbb{R}^n $ that satisfy the equation:
  \[ \sum_{i=1}^n w_i \cdot x_i + b = 0 \]
  where $ w \in \mathbb{R}^n $ is the normal vector to the hyperplane and $ b \in \mathbb{R} $ is the bias
}

\ex{Hyperplane in two dimensions}{
  We can consider this equation in $ \mathbb{R}^2 $:
  \[
    -\frac{1}{2}x_1 + x_2 + 1 = 0
  \]
  The graph is a line in the plane with normal vector $ w = \left(-\frac{1}{2}, 1\right) $ and bias $ b = 1 $.

  \begin{center}
    \includegraphics{./paginette/imgs/hyperplan_in_r2.png}
  \end{center}
}
As we can see from the example, a general property of hyperplanes in $ \mathbb{R}^n $, the hyperplans divedes the space in two half-spaces. By the definition of perceptron \ref{dfn:perceptron} ($\sum_i x_ix_i+b=0 $) this gives value $1$ to all the points in one half-space and value $0$ to all the points in the other half-space.

\nt{
  "above" and "below" can be inverted by just inverting the parameters:
  \[
    \sum_{i=1}^n w_i \cdot x_i + b \geq 0 \iff \sum_{i=1}^n -w_i \cdot x_i + (-b) \leq 0 
  \]
}
\subsubsection{Computing logical connectives}
For investingating the expressiveness of a single perceptron, a logical way is to see if it can compute logical connectives.

\paragraph{NAND case}
Ramarking NAND:
\begin{center}
  \begin{table}
    \begin{tabular}{c|c|c}
      $ x_1 $ & $ x_2 $ & $ \text{NAND}(x_1, x_2) $ \\
      \hline
      0 & 0 & 1 \\
      0 & 1 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 0 \\
    \end{tabular}
  \end{table}
\end{center}
  
For computing NAND with a perceptron, we need to find weights $ w_1, w_2 $ and bias $ b $ such that:
\[
  \text{\texttt{nand}}(x_1, x_2) = \begin{cases}
    1 & \text{if } w_1 \cdot x_1 + w_2 \cdot x_2 + b \geq 0 \\
    0 & \text{otherwise}
  \end{cases}
\]

\nt{
  This is same as asking if we can a straight line to separate green and red points
  \begin{center}
    \includegraphics{./paginette/imgs/separate_gerrn_and_red.png}
  \end{center}
  Remember: doing that means putting one to one side of the line all the green points and on the other side all the red points

  Well, the answare is yes! We can choose for example:
  % 1.5 − x1 − x2 = 0 or 3 − 2x1 − 2x2 = 0
  \[
    w_1 = -2, \quad w_2 = -2, \quad b = 3 \Rightarrow 3 - 2x_1 - 2x_2 = 0
  \]
  \begin{center}
    \includegraphics[width=6cm]{./paginette/imgs/nand_solution.png}
  \end{center}
}

Watching the notes what we have is the NAND-perceptron:
\begin{center}
  \includegraphics[width=6cm]{./paginette/imgs/nand_perceptron.png}
\end{center}

\[
  \text{\texttt{nand}}(x_1, x_2) = \begin{cases}
    1 & \text{if } -2 \cdot x_1 - 2 \cdot x_2 + 3 \geq 0 \\
    0 & \text{otherwise}
  \end{cases}
\]

\nt{
  The set of logical operators that contains only the NAND gate is actually logically complete\footnote{A complete set of logical operators can calculate any boolean function.}, but it needs to be composed to do so. Meaning a single layer being able to simulate such a function does not imply that the perceptron is logically complete, as we'll now see.
}

\paragraph{XOR case}
XOR is defined as:
\begin{center}
  \begin{table}
    \begin{tabular}{c|c|c}
      $ x_1 $ & $ x_2 $ & $ \text{XOR}(x_1, x_2) $ \\
      \hline
      0 & 0 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 0 \\
    \end{tabular}
  \end{table}
\end{center}

\nt{Same as asking: Can we draw a straight line separating red and green points?
  \begin{center}
    \includegraphics[width=6cm]{./paginette/imgs/XOR_green_and_red.png}
  \end{center}
  Solution? No solution :(
}
The XOR function is not linearly separable, so it cannot be computed by a single perceptron. This is a limitation of linear methods (like logistic regression), seen as it's impossible to compare features with one another.

In conclusion, single layer perceptrons are not logically complete. 

\subsection{Multi-layer perceptrons}
A perpteptron, as it is demonstrated before, can compute NAND and it is known that NAND is logically complete, meaning that any logical function can be computed by a combination of NAND functions.
So why it is not possible to compute XOR or others with a single perceptron? why why perceptrons are not \textit{complete}? The answer is that a single perceptron can only compute \textit{linearly separable functions} and they need to be combined in order to compute more complex functions.

\ex{Multi-layer perceptron for XOR}{
  \begin{center}
    \includegraphics[width=8cm]{./paginette/imgs/multi_layer_perceptron_for_XOR.png}
  \end{center}
  The multi-layer perceptron above computes XOR using 3 perceptrons that compute NAND, OR and AND respectively.
}

So how deep does a neural network need to be to calculate a certain logic expression? We can look at the depth as the number of nested logical connectors, or as the depth of the tree generated by the expression. In conjunctive or disjunctive normal form, all trees have depth of 3, and each logic formula can be transformed in one of these forms. In reality, we can remove the negation layer and use only 2 layers by using more expressive connectors. 

\nt{
  It's thanks to the activator function that the composition of layers adds expressivity, as a simple composition of linear functions is still linear and doesn't add complexity.
}

A significant theoretical result, known as the universal approximation theorem, states that even \textbf{shallow networks} (those with a single hidden layer) are \textbf{already complete}.
This means that a shallow network can, in principle, approximate any continuous function to an arbitrary degree of accuracy, given a sufficient number of neurons in its hidden layer.

This naturally raises a critical architectural question: Why go for deep networks?
If a shallow architecture is theoretically sufficient, what is the practical advantage of stacking multiple layers?

\subsection{Deep Networks}
The answer lies not in \textit{capability} but in \textit{efficiency}. Research has demonstrated that with deep nets, the same function may be computed with less neural units Deep networks build a hierarchical representation of features---where each layer learns progressively more complex abstractions based on the previous---which is a far more parameter-efficient way to represent complex functions compared to the "brute force" approach of a single, massive hidden layer.

In fact, there are some cases where transforming a logical expression into normal form can create an exponentially longer expression, making it much less efficient.

\paragraph{The essential role of Activator Functions}
It is crucial to understand that this expressive power---whether in shallow or deep networks--- originates from a single source. \textbf{Activation functions play an essential role},
as they are the \textbf{only source of nonlinearity} in the model.

A neural layer is composed of a linear transformation (the weighted sum and bias) followed
by a non-linear activation. If this non-linear step were removed, the network would collapse.
\textbf{Composing linear layers not separated by nonlinear activations makes no sense}, as the composition of any number of linear functions is, itself, just a single linear function.
Therefore, it is the activation function that enables the network to warp and fold the input space, allowing it to learn the complex, non-linear relationships required to solve problems like the XOR example.

\section{Training}
Training a neural network involves adjusting its weights and biases to minimize the difference between its predicted outputs and the actual target values. The training process can be described in three main steps:

\begin{itemize}
  \item \textbf{Forward Pass}: Suppose to have a neural network with some configurations of the parameters $\theta$ (weights and biases)
  \begin{center}
    \includegraphics[width=6cm]{paginette/imgs/pass_trough.png}
  \end{center}
  The foreard pass is defined as the process of passing the input data $x$ through the network to obtain the output predictions then calculating the current loss relative to $\theta$
  \item \textbf{Backward Pass}: The backward pass is the process of computing the gradients of the loss function with respect to each weight and bias in the network (next chapter will cover this in detail)
  \begin{center}
    \includegraphics[width=6cm]{paginette/imgs/backprop.png}
  \end{center}
  The algorithm for computing parameters updates is known as \textit{backpropagation algorithm}
  \item \textbf{Parameters Update}: For decreasing the loss, the params need to be adjustent in different ways. The tool that allows us to establish in which way parameters should be updated is the gradient calculated during the backward pass
  \begin{center}
    \includegraphics[width=6cm]{paginette/imgs/params_upd.png}
  \end{center}
\end{itemize}

\nt{
The cost of both the passes is relatively similar, but during the forward pass each tensor's output is memorised in order to calculate the gradients later on (as we'll see). Because of this there's a higher memory use during the training of a Neural Network.
}

\subsection{The Backpropagation algorithm}

For calculating this so called gradient (a vector of partial derivatives of the loss function with respect to each parameter in the network) it good remember that a neural network is a complex function resulting from the composition of many simpler functions (the layers). So a mathematical tool that is useful for calculating derivatives of composed functions is the \textbf{chain rule}

\subsubsection{Chain roule}
In order to calculate the gradients needed for the gradient descent algorithm, we need to use the \textbf{chain rule}:
\dfn{Chain Roule}{
  Given two functions $ f: \mathbb{R} \rightarrow \mathbb{R} $ and $ g: \mathbb{R} \rightarrow \mathbb{R} $, the derivative of their composition $ h(x) = f(g(x)) $ is given by:
  \[
    h'(x) = f'(g(x)) \cdot g'(x)
  \]
  Eqyivalently, letting $ y = g(x) $, we have:
  \[ h'(x) = f'(y) \cdot g'(x)\]
}

The derivative of a composition of a sequence of functions is the \textbf{product of the derivatives of the individual functions}. This function is iterable, so for each layer we multiply its derivative by the derivative of its forward input (which is why we need to memorise it).

\nt{
Binary thresholding has null derivative everywhere (its a step function), so it would ruin the chain.
}

\subsubsection{The Network as a Composite Function}
Before applying we must formally define the function computed by the neural network.

\dfn{Activation Vector at layer $l$}{
  \label{dfn:compositefunction}
  The activation vector at layer $l$ is defined as the following function computed by the layer $l$:
  \[ a^l = \sigma(b^l + w^l \cdot x^l) \]
  where:
  \begin{itemize}
    \item $\sigma$ is the activation function
    \item $z^l = b^l + w^l \cdot x^l$ is the weighted input at layer $l$
    \item $x^{l+1}=a^l, x^1=x$ (in fact the output of layer $l$ is the input of layer $l+1$)
  \end{itemize} 
}

So by the definition \ref{dfn:compositefunction} the neural network with $L$ layers computes the following function:
\[
  \sigma(b^L + w^L \cdot \sigma(b^{L-1} + w^{L-1} \cdot \sigma( \dots \sigma(b^1 + w^1 \cdot x^1) )))
\]

\nt{
  The dimension of $w^l$ and $b^l$ depend on the number of neurons at layer $l$ (and $l-1$)
}

\nt{
All of them are \textit{parameters} of the models
}
\subsubsection{Backpropagation Rules in Vectorial Notation}
The Backpropagation algorithm applies the chain rule to this composite function to efficiently compute the gradient of an error function $E$ (e.g., Euclidean distance) with respect to all parameters. But first of all it's useful to define the \textbf{error at layer $l$} defined as the vector of partial derivatives of the error $E$ with respect to the weighted input $z^l$. Formally:

\dfn{
  Error derivative at layer $l$
}{
  The error derivative at layer $l$ is defined as:
  \[
    \delta^l = \frac{\partial E}{\partial z^l}
  \]
  where $ z^l = b^l + w^l \cdot x^l $ is the weighted input at layer $l$
}

This $\delta$ term represents the error signal at that layer. The algorithm is then defined by the following four equations:

\begin{enumerate}
  \item \textbf{Error for the output layer (L):}
  \[ \delta^L = \nabla_{a^L} E \odot \sigma'(z^L) \]
  This calculates the initial error signal by combining the gradient of the loss function ($\nabla_{a^L} E$) with the derivative of the final activation function ($\sigma'(z^L)$)

  \item \textbf{Error backpropagation:}
  \[ \delta^l = ((W^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \]
    This is the core rule that \textit{propagates} the error backward. The error from
    the next layer ($\delta^{l+1}$) is passed back through the weights ($W^{l+1}$)
    and combined with the derivative of the current layer's activation function
  \item \textbf{Gradient for the bias:}
    \[ \frac{\partial E}{\partial b_j^l} = \delta_j^l \]
    The gradient for any bias is simply the error signal $\delta$ at that
    neuron.

  \item \textbf{Gradient for the weight:}
    \[ \frac{\partial E}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l \]
    The gradient for any weight is the activation from the source neuron ($a_k^{l-1}$)
    multiplied by the error signal at the destination neuron ($\delta_j^l$)
\end{enumerate}

\subsubsection{The vanishing gradient problem}
By the chain rule, the derivative is a long sequence of factor,where these factors are, alternately
\begin{itemize}
  \item Derivatives of activation functions
  \item Derivate of linear functions (the weights)
\end{itemize}

Backpropagation alghorithm uses the chain rule to compute gradients efficiently, so the gradient used for an initial state is the result of a lot of factors. These factors include the activateion functions. For many years the activation function used was the sigmoid (continuous version of a binary threshold), whose derivative is always less than 1. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=6cm]{paginette/imgs/sigmoid_actfun.png}
  \caption{Blu line is logistic function (sigmoid), green line is its derivative}
  \label{fig:sigmoid_actfun}
\end{figure}

This graph is crucial for understanding the vanishing gradient problem. As we can see from the graph, the derivative of the sigmoid is always not greater than 0.25 and it's flat. The backpropagation multiplies a lot of these very small factors together, so the result is that the gradient becomes very small (ex. $gradiente = (\dots \times 0.25 \times 0.23 \times 0.25 \times 0.21 \times \dots)$) as it propagates backward through the layers. This means that the weights in the earlier layers receive very small updates during training, making it difficult for the network to learn effectively. If the gradient is close to zero, learning is impossible.

In contrast, the Relu function doesn't have this problem as its derivative is either 0 or 1.


\section{Filter and convolution}

The convulution is an operation between two functions that produces a third function that expresses how the shape of one is modified by the other. In image processing, the convolution is used to apply filters to images, such as blurring, sharpening, edge detection, and more. In machine learning, convolutional neural networks (CNNs) use convolutional layers to automatically learn and extract features from images preserving spatial relationships. Mathematically

\dfn{convulution operation}{
  Fir a bidimensional input image $ I $ and a filter (or kernel) $ K^{m\times n} $, the convolution operation is defined as:
  \[
    S(i,j) = (I * K)(i,j) = \sum_{m} \sum_{n} I(i-m, j-n) K(m,n)
  \]
}

\begin{center}
  \includegraphics[width=8cm]{paginette/imgs/convolution.png}
\end{center}

\ex{}{
 \begin{center}
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_1.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_2.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_3.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_4.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_5.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_6.png}
\end{center}
}

\nt{
  In a convolutional layer, each neuron produces a single value of the output feature map, which appears as one “pixel" in the resulting matrix.
}

In the neural networks, each neruon is linked to the first hidden layer. If we had a $1000 \times 1000$ pixel image ($1M$ inputs) and a hidden layer of $1000$ neurons, we would need one billion weights ($10^9$ connections)— impossible to manage computationally. One of the main carateristics of convutional layers is the \textit{local connectivity}, each neuron is connected only to a small region of the input image, called the receptive field. This drastically reduces the number of parameters and computations required, making it feasible to process high-dimensional inputs like images. 
Another important characteristic is \textit{weight sharing}, where the same set of weights (the filter or kernel) is used across different spatial locations of the input. This means that the same feature can be detected regardless of its position in the image, enhancing the model's ability to generalize and reducing the number of unique parameters that need to be learned.

With a cascade of convolutional filters and pooling layers, the network can learn hierarchical features, from low-level edges and textures in the early layers to high-level object parts and entire objects in the deeper layers. This hierarchical feature learning is crucial for tasks like image classification, object detection, and segmentation.

\subsection{About the relevance of convolutions for image processing}
An image is coded as a numerical matrix (array) grayscale (0-255) or rgb (triple 0-255)

\begin{center}
  \includegraphics[width=8cm]{paginette/imgs/model.png}
\end{center}

\subsubsection{Img as surfaces}
We can see an image as a surface in 3D space, where the $x$ and $y$ coordinates represent the pixel positions, and the $z$ coordinate represents the intensity (brightness) of the pixel. In grayscale images, this intensity ranges from 0 (black) to 255 (white). In color images, each pixel has three intensity values corresponding to the red, green, and blue channels.

\begin{center}
  \includegraphics[width=8cm]{paginette/imgs/img_as_surface.png}
\end{center}

\nt{
  Edges, angles, ...: points where there is a discontinuity, i.e. a fast variation of the intensity
  \begin{center}
    \includegraphics[width=8cm]{paginette/imgs/conv_img_2.png}
  \end{center}
  More generally, are interested to identify \textit{patterns} inside the image. The key idea is that the kernel of the convolution expresses the pattern we are looking for.
}

\ex{Finite Derivate}{
  Suppose we want to find the positions inside the image where there is a sudden horizontal passage from a dark region to a bright one. The pattern we are looking has that kernel:
  \[
    \begin{bmatrix}
      -1 & -1
    \end{bmatrix}
  \]
  or, varying the distance between pixels:
  \[
    \begin{bmatrix}
      -1 & 0 & 1
    \end{bmatrix}
  \]
  \begin{center}
    \includegraphics[width=8cm]{paginette/imgs/kernel.png}
  \end{center}
}

\subsection{Discovering patterns}

instead of using human designed pre-defined patterns, let the net learn them. This is particularly useful in deep networks:
\begin{itemize}
  \item stacking kernels we can learn more complex patterns 
  \item adding non-linear activations we synthesize complex, non-linear kernels
\end{itemize}

\subsubsection{Receptive field}
\dfn{receptive field}{
  The receptive field of a neuron in a convolutional layer is the specific region of the input image that influences the neuron's output. It defines the spatial extent of the input data that the neuron "sees" and processes.
}
It is equal to the dimension of an input image producing (without padding) an output with dimension 1

\nt{
  A neuron cannot see anything outside its receptive field
}

\section{CNNs and Transfer Learning}
TODO: 21 Nov (non registrata) - slides 5 (ma non so se sono giuste / cosa ha davvero fatto)
TODO: Potrebbe anche aver finito le slide 4, chi lo sa

\section{How to fool a Neural Network}
TODO: 28 Nov - slides6
\subsection{Adversarial attacks}
\subsection{Data Manifolds}
\subsection{Autoencoders}

\section{Generative Models}
TODO: 4 Dic - slides7

\section{Generative Adversarial Networks}
TODO: 5 Dic - slides8
