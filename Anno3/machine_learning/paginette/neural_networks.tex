% \begin{document}
\chapter{Neural Networks}
Only known way to implement deep learning. 

An artificial neuron has a set of inputs that are summed with a bias, and the output is calculated using an ctivation function. invented in the '50 with the name 'perceptron', the activation function was discrete (0 or 1). This method is obsolete because it doesn't permit training. The function musn't be linear, because we want to sequencially compose neurons and the composition of linear functions is just another linear function and deosn't add any complexity (could be calculated with just one neuron). 

There are different activation functions (threshol, logistic, hyperbolic, rectified linear (newest used in AlexNet, big jump)). 

The name neuron comes from the biological counterpart (obviously) which works in a similar way:
\begin{itemize}
  \item Dendritic tree: connections with other neurons (synapses)
    \item In the body the inputs are summed and passed through the axon hilock, which performs a sort of thresholding before being passed to other neurons
\end{itemize}

Comparing artificial neural networks to our brains, the number of neurons can be similar ($ 2\cdot 10^{10} $for the biggest models), but the sheer size doesn't necessarily correlate to a more intelligent system (other animals have bigger brains), we actually don't really know what else there is. The switching time for real neurons is actually slower than artificial ones, seen as it's a chemical reaction and not electrical. Each neuron is connected to meny other neurons in the brain ($ 10^{4-5} $), reaching reaction times $ < 100 $, so the brain isn't very deep (number of intermediate nodes) and it's very parallelised.

\section{Topologies}
\dfn{Feed-forward}{
  Acyclic networks with unidirectional data flux.
}
\nt{
  Our brains are cyclic networks.
}

\subsection{Layers}
The network is built by sets of structured neurons that are combined to build the whole.

For each dense layer, each neuron has an input, weights, a constant bias from which an output is calculated. This operation can be parallelised using the whole layer with a matrix of weights and a vector of biases, using the same inputs over all the neurons. The input is a vector in multiple dimensions (tensor), and after algebraic manupulations a new tensor is returned. 

\section{Features and deep features}

TODO: manca una lezione intera
TODO: @bastini plz finiscie te, che altrimenti questi mi diventano ApuntiDelGiolaBambas

\section{Expressiness}
This part begins with an important question: \textit{Can we compute any function by means of a Neural Network?}.
If so, every function must have a corresponding set of weights that, when applied to a Neural Network, induce the calculation of said function.
But before that, we must first ask: \textit{Can we compute any function with a single neuron?}

These questions are related to the \textbf{Expressiveness} of Neural Networks, that is the capacity of a Neural Network to approximate any function
\subsection{Perceptron, the single layer case}
A single neuron is called a \textbf{Perceptron}. It is the simplest form of a Neural Network.

\dfn{Perceptron with binary threshold}{
  \label{dfn:perceptron}
  
  \begin{center}
    \includegraphics{./paginette/imgs/perceptron.png}
  \end{center}
  A perceptron is a function $ f: \mathbb{R}^n \rightarrow \{0, 1\} $ defined as:
  \[
    f(x) = \begin{cases}
      1 & \text{if } \sum_{i=1}^n w_i \cdot x_i + b \geq 0 \\
      0 & \text{otherwise}
    \end{cases}
  \]
  where $ x \in \mathbb{R}^n $ is the input vector, $ w \in \mathbb{R}^n $ is the weight vector and $ b \in \mathbb{R} $ is the bias.
}

\nt{
  It's easy to notice the the the bias $b$ set the position of threshold
}
\subsubsection{Hyperplanes}
For major information about hyperplanes see the notes of "Ottimizzazione combinatoria di Alex Basta e Qua Qua dancer"

\dfn{Hyperplane}{
  We can define a hyperplane in $ \mathbb{R}^n $ as the set of points $ x \in \mathbb{R}^n $ that satisfy the equation:
  \[ \sum_{i=1}^n w_i \cdot x_i + b = 0 \]
  where $ w \in \mathbb{R}^n $ is the normal vector to the hyperplane and $ b \in \mathbb{R} $ is the bias
}

\ex{Hyperplane in two dimensions}{
  We can consider this equation in $ \mathbb{R}^2 $:
  \[
    -\frac{1}{2}x_1 + x_2 + 1 = 0
  \]
  The graph is a line in the plane with normal vector $ w = \left(-\frac{1}{2}, 1\right) $ and bias $ b = 1 $.

  \begin{center}
    \includegraphics{./paginette/imgs/hyperplan_in_r2.png}
  \end{center}
}
As we can see from the example, a general property of hyperplanes in $ \mathbb{R}^n $, the hyperplans divedes the space in two half-spaces. By the definition of perceptron \ref{dfn:perceptron} ($\sum_i x_ix_i+b=0 $) this gives value $1$ to all the points in one half-space and value $0$ to all the points in the other half-space.

\nt{
  "above" and "below" can be inverted by just inverting the parameters:
  \[
    \sum_{i=1}^n w_i \cdot x_i + b \geq 0 \iff \sum_{i=1}^n -w_i \cdot x_i + (-b) \leq 0 
  \]
}
\subsubsection{Computing logical connectives}
For investingating the expressiveness of a single perceptron, a logical way is to see if it can compute logical connectives.

\paragraph{NAND case}
Ramarking NAND:
\begin{center}
  \begin{table}
    \begin{tabular}{c|c|c}
      $ x_1 $ & $ x_2 $ & $ \text{NAND}(x_1, x_2) $ \\
      \hline
      0 & 0 & 1 \\
      0 & 1 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 0 \\
    \end{tabular}
  \end{table}
\end{center}
  
For computing NAND with a perceptron, we need to find weights $ w_1, w_2 $ and bias $ b $ such that:
\[
  \text{\texttt{nand}}(x_1, x_2) = \begin{cases}
    1 & \text{if } w_1 \cdot x_1 + w_2 \cdot x_2 + b \geq 0 \\
    0 & \text{otherwise}
  \end{cases}
\]

\nt{
  This is same as asking if we can a straight line to separate green and red points
  \begin{center}
    \includegraphics{./paginette/imgs/separate_gerrn_and_red.png}
  \end{center}
  Remember: doing that means putting one to one side of the line all the green points and on the other side all the red points

  Well, the answare is yes! We can choose for example:
  % 1.5 − x1 − x2 = 0 or 3 − 2x1 − 2x2 = 0
  \[
    w_1 = -2, \quad w_2 = -2, \quad b = 3 \Rightarrow 3 - 2x_1 - 2x_2 = 0
  \]
  \begin{center}
    \includegraphics[width=6cm]{./paginette/imgs/nand_solution.png}
  \end{center}
}

Watching the notes what we have is the NAND-perceptron:
\begin{center}
  \includegraphics[width=6cm]{./paginette/imgs/nand_perceptron.png}
\end{center}

\[
  \text{\texttt{nand}}(x_1, x_2) = \begin{cases}
    1 & \text{if } -2 \cdot x_1 - 2 \cdot x_2 + 3 \geq 0 \\
    0 & \text{otherwise}
  \end{cases}
\]

\nt{
  The set of logical operators that contains only the NAND gate is actually logically complete\footnote{A complete set of logical operators can calculate any boolean function.}, but it needs to be composed to do so. Meaning a single layer being able to simulate such a function does not imply that the perceptron is logically complete, as we'll now see.
}

\paragraph{XOR case}
XOR is defined as:
\begin{center}
  \begin{table}
    \begin{tabular}{c|c|c}
      $ x_1 $ & $ x_2 $ & $ \text{XOR}(x_1, x_2) $ \\
      \hline
      0 & 0 & 0 \\
      0 & 1 & 1 \\
      1 & 0 & 1 \\
      1 & 1 & 0 \\
    \end{tabular}
  \end{table}
\end{center}

\nt{Same as asking: Can we draw a straight line separating red and green points?
  \begin{center}
    \includegraphics[width=6cm]{./paginette/imgs/XOR_green_and_red.png}
  \end{center}
  Solution? No solution :(
}
The XOR function is not linearly separable, so it cannot be computed by a single perceptron. This is a limitation of linear methods (like logistic regression), seen as it's impossible to compare features with one another.

In conclusion, single layer perceptrons are not logically complete. 

\subsection{Multi-layer perceptrons}
A perpteptron, as it is demonstrated before, can compute NAND and it is known that NAND is logically complete, meaning that any logical function can be computed by a combination of NAND functions.
So why it is not possible to compute XOR or others with a single perceptron? why why perceptrons are not \textit{complete}? The answer is that a single perceptron can only compute \textit{linearly separable functions} and they need to be combined in order to compute more complex functions.

\ex{Multi-layer perceptron for XOR}{
  \begin{center}
    \includegraphics[width=8cm]{./paginette/imgs/multi_layer_perceptron_for_XOR.png}
  \end{center}
  The multi-layer perceptron above computes XOR using 3 perceptrons that compute NAND, OR and AND respectively.
}

So how deep does a neural network need to be to calculate a certain logic expression? We can look at the depth as the number of nested logical connectors, or as the depth of the tree generated by the expression. In conjunctive or disjunctive normal form, all trees have depth of 3, and each logic formula can be transformed in one of these forms. In reality, we can remove the negation layer and use only 2 layers by using more expressive connectors. 

\nt{
  It's thanks to the activator function that the composition of layers adds expressivity, as a simple composition of linear functions is still linear and doesn't add complexity.
}

A significant theoretical result, known as the universal approximation theorem, states that even \textbf{shallow networks} (those with a single hidden layer) are \textbf{already complete}.
This means that a shallow network can, in principle, approximate any continuous function to an arbitrary degree of accuracy, given a sufficient number of neurons in its hidden layer.

This naturally raises a critical architectural question: Why go for deep networks?
If a shallow architecture is theoretically sufficient, what is the practical advantage of stacking multiple layers?

\subsection{Deep Networks}
The answer lies not in \textit{capability} but in \textit{efficiency}. Research has demonstrated that with deep nets, the same function may be computed with less neural units Deep networks build a hierarchical representation of features---where each layer learns progressively more complex abstractions based on the previous---which is a far more parameter-efficient way to represent complex functions compared to the "brute force" approach of a single, massive hidden layer.

In fact, there are some cases where transforming a logical expression into normal form can create an exponentially longer expression, making it much less efficient.

\paragraph{The essential role of Activator Functions}
It is crucial to understand that this expressive power---whether in shallow or deep networks--- originates from a single source. \textbf{Activation functions play an essential role},
as they are the \textbf{only source of nonlinearity} in the model.

A neural layer is composed of a linear transformation (the weighted sum and bias) followed
by a non-linear activation. If this non-linear step were removed, the network would collapse.
\textbf{Composing linear layers not separated by nonlinear activations makes no sense}, as the composition of any number of linear functions is, itself, just a single linear function.
Therefore, it is the activation function that enables the network to warp and fold the input space, allowing it to learn the complex, non-linear relationships required to solve problems like the XOR example.

\section{Training}
Training a neural network involves adjusting its weights and biases to minimize the difference between its predicted outputs and the actual target values. The training process can be described in three main steps:

\begin{itemize}
  \item \textbf{Forward Pass}: Suppose to have a neural network with some configurations of the parameters $\theta$ (weights and biases)
  \begin{center}
    \includegraphics[width=6cm]{paginette/imgs/pass_trough.png}
  \end{center}
  The foreard pass is defined as the process of passing the input data $x$ through the network to obtain the output predictions then calculating the current loss relative to $\theta$
  \item \textbf{Backward Pass}: The backward pass is the process of computing the gradients of the loss function with respect to each weight and bias in the network (next chapter will cover this in detail)
  \begin{center}
    \includegraphics[width=6cm]{paginette/imgs/backprop.png}
  \end{center}
  The algorithm for computing parameters updates is known as \textit{backpropagation algorithm}
  \item \textbf{Parameters Update}: For decreasing the loss, the params need to be adjustent in different ways. The tool that allows us to establish in which way parameters should be updated is the gradient calculated during the backward pass
  \begin{center}
    \includegraphics[width=6cm]{paginette/imgs/params_upd.png}
  \end{center}
\end{itemize}

\nt{
The cost of both the passes is relatively similar, but during the forward pass each tensor's output is memorised in order to calculate the gradients later on (as we'll see). Because of this there's a higher memory use during the training of a Neural Network.
}

\subsection{The Backpropagation algorithm}

For calculating this so called gradient (a vector of partial derivatives of the loss function with respect to each parameter in the network) it good remember that a neural network is a complex function resulting from the composition of many simpler functions (the layers). So a mathematical tool that is useful for calculating derivatives of composed functions is the \textbf{chain rule}

\subsubsection{Chain roule}
In order to calculate the gradients needed for the gradient descent algorithm, we need to use the \textbf{chain rule}:
\dfn{Chain Roule}{
  Given two functions $ f: \mathbb{R} \rightarrow \mathbb{R} $ and $ g: \mathbb{R} \rightarrow \mathbb{R} $, the derivative of their composition $ h(x) = f(g(x)) $ is given by:
  \[
    h'(x) = f'(g(x)) \cdot g'(x)
  \]
  Eqyivalently, letting $ y = g(x) $, we have:
  \[ h'(x) = f'(y) \cdot g'(x)\]
}

The derivative of a composition of a sequence of functions is the \textbf{product of the derivatives of the individual functions}. This function is iterable, so for each layer we multiply its derivative by its forward input (which is why we need to memorise it).

\nt{
Binary thresholding has null derivative everywhere (its a step function), so it would ruin the chain.
}

\subsubsection{The Network as a Composite Function}
Before applying we must formally define the function computed by the neural network.

\dfn{Activation Vector at layer $l$}{
  \label{dfn:compositefunction}
  The activation vector at layer $l$ is defined as the following function computed by the layer $l$:
  \[ a^l = \sigma(b^l + w^l \cdot x^l) \]
  where:
  \begin{itemize}
    \item $\sigma$ is the activation function
    \item $z^l = b^l + w^l \cdot x^l$ is the weighted input at layer $l$
    \item $x^{l+1}=a^l, x^=x$ (in fact the output of layer $l$ is the input of layer $l+1$)
  \end{itemize} 
}

So by the definition \ref{dfn:compositefunction} the neural network with $L$ layers computes the following function:
\[
  \sigma(b^L + w^L \cdot \sigma(b^{L-1} + w^{L-1} \cdot \sigma( \dots \sigma(b^1 + w^1 \cdot x^1) ))
\]

\nt{
  The dimension of $w^l$ and $b^l$ depend on the number of neurons at layer $l$ (and $l-1$)
}

\nt{
All of them are \textit{parameters} of the models
}
\subsubsection{Backpropagation Rules in Vectorial Notation}
The Backpropagation algorithm applies the chain rule to this composite function to efficiently compute the gradient of an error function $E$ (e.g., Euclidean distance) with respect to all parameters. But first of all it's useful to define the \textbf{error at layer $l$} defined as the vector of partial derivatives of the error $E$ with respect to the weighted input $z^l$. Formally:

\dfn{
  Error derivative at layer $l$
}{
  The error derivative at layer $l$ is defined as:
  \[
    \delta^l = \frac{\partial E}{\partial z^l}
  \]
  where $ z^l = b^l + w^l \cdot x^l $ is the weighted input at layer $l$
}

This $\delta$ term represents the error signal at that layer. The algorithm is then defined by the following four equations:

\begin{enumerate}
  \item \textbf{Error for the output layer (L):}
  \[ \delta^L = \nabla_{a^L} E \odot \sigma'(z^L) \]
  This calculates the initial error signal by combining the gradient of the loss function ($\nabla_{a^L} E$) with the derivative of the final activation function ($\sigma'(z^L)$)

  \item \textbf{Error backpropagation:}
  \[ \delta^l = ((W^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \]
    This is the core rule that \textit{propagates} the error backward. The error from
    the next layer ($\delta^{l+1}$) is passed back through the weights ($W^{l+1}$)
    and combined with the derivative of the current layer's activation function
  \item \textbf{Gradient for the bias:}
    \[ \frac{\partial E}{\partial b_j^l} = \delta_j^l \]
    The gradient for any bias is simply the error signal $\delta$ at that
    neuron.

  \item \textbf{Gradient for the weight:}
    \[ \frac{\partial E}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l \]
    The gradient for any weight is the activation from the source neuron ($a_k^{l-1}$)
    multiplied by the error signal at the destination neuron ($\delta_j^l$)
\end{enumerate}

\subsubsection{The vanishing gradient problem}
By the chain rule, the derivative is a long sequence of factor,where these factors are, alternately
\begin{itemize}
  \item Derivatives of activation functions
  \item Derivate of linear functions (the weights)
\end{itemize}

Backpropagation alghorithm uses the chain rule to compute gradients efficiently, so the gradient used for an initial state is the result of a lot of factors. These factors include the activateion functions. For many years the activation function used was the sigmoid (continuous version of a binary threshold), whose derivative is always less than 1. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=6cm]{paginette/imgs/sigmoid_actfun.png}
  \caption{Blu line is logistic function (sigmoid), green line is its derivative}
  \label{fig:sigmoid_actfun}
\end{figure}

This graph is crucial for understanding the vanishing gradient problem. As we can see from the graph, the derivative of the sigmoid is always not greater than 0.25 and it's flat. The backpropagation multiplies a lot of these very small factors together, so the result is that the gradient becomes very small (ex. $gradiente = (\dots \times 0.25 \times 0.23 \times 0.25 \times 0.21 \times \dots)$) as it propagates backward through the layers. This means that the weights in the earlier layers receive very small updates during training, making it difficult for the network to learn effectively. If the gradient is close to zero, learning is impossible.

In contrast, the Relu function doesn't have this problem as its derivative is either 0 or 1.

\section{Roba che devo ancora controllare}
\begin{itemize}
\item $ a^l $ is the \textbf{activation vector}
\item $ z^l $ is the \textbf{weighted input}
\item $ x^{l+1} = a^l, x^1 = x $
\end{itemize}

Partial derivative
\[
\delta^l = \frac{\delta E}{\delta z^l}
\]

We have to traverse the dense part. The parameters we're interested in are the parameters of the linear transformation. We transpose the matrix to calculate the layer at level $ l+1 $. 

We can calculate all partial derivatives with respect to $ z $, but we don't want this, we want $ b $. But in reality we just need to multiply by the derivative with respect to $ b $ of $ z = ... $, which is one. So the two values are the same. We can do the same for $ w $, which has derivative $ x^l $, which is the last layer's output ??.

We don't really need to know all the formulas, just the main concept.

% \end{document}

\section{Filter and convolution}

The convulution is an operation between two functions that produces a third function that expresses how the shape of one is modified by the other. In image processing, the convolution is used to apply filters to images, such as blurring, sharpening, edge detection, and more. In machine learning, convolutional neural networks (CNNs) use convolutional layers to automatically learn and extract features from images preserving spatial relationships. Mathematically

\dfn{convulution operation}{
  Fir a bidimensional input image $ I $ and a filter (or kernel) $ K^{m\times n} $, the convolution operation is defined as:
  \[
    S(i,j) = (I * K)(i,j) = \sum_{m} \sum_{n} I(i-m, j-n) K(m,n)
  \]
}

\begin{center}
  \includegraphics[width=8cm]{paginette/imgs/convolution.png}
\end{center}

\ex{}{
 \begin{center}
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_1.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_2.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_3.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_4.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_5.png} \\
  \includegraphics[width=4cm]{paginette/imgs/conv/conv_6.png}
\end{center}
}

\nt{
  In a convolutional layer, each neuron produces a single value of the output feature map, which appears as one “pixel" in the resulting matrix.
}

in the neural networks, each neruon is linked to the first hidden layer. If we had a $1000 \times 1000$ pixel image ($1M$ inputs) and a hidden layer of $1000$ neurons, we would need one billion weights ($10^9$ connections)— impossible to manage computationally. One of the main carateristics of convutional laters is the \textit{local connectivity}, each neuron is connected only to a small region of the input image, called the receptive field. This drastically reduces the number of parameters and computations required, making it feasible to process high-dimensional inputs like images. 
Another important characteristic is \textit{weight sharing}, where the same set of weights (the filter or kernel) is used across different spatial locations of the input. This means that the same feature can be detected regardless of its position in the image, enhancing the model's ability to generalize and reducing the number of unique parameters that need to be learned.

with a cascade of convolutional filters and pooling layers, the network can learn hierarchical features, from low-level edges and textures in the early layers to high-level object parts and entire objects in the deeper layers. This hierarchical feature learning is crucial for tasks like image classification, object detection, and segmentation.

\subsection{About the relevance of convolutions for image processing}
An image is coded as a numerical matrix (array) grayscale (0-255) or rgb (triple 0-255)

\begin{center}
  \includegraphics[width=8cm]{paginette/imgs/model.png}
\end{center}

\subsubsection{Img as surfaces}
We can see an image as a surface in 3D space, where the $x$ and $y$ coordinates represent the pixel positions, and the $z$ coordinate represents the intensity (brightness) of the pixel. In grayscale images, this intensity ranges from 0 (black) to 255 (white). In color images, each pixel has three intensity values corresponding to the red, green, and blue channels.

\begin{center}
  \includegraphics[width=8cm]{paginette/imgs/img_as_surface.png}
\end{center}

\nt{
  Edges, angles, ...: points where there is a discontinuity, i.e. a fast variation of the intensity
  \begin{center}
    \includegraphics[width=8cm]{paginette/imgs/conv_img_2.png}
  \end{center}
  More generally, are interested to identify \textit{patterns} inside the image. The key idea is that the kernel of the convolution expresses the pattern we are looking for.
}

\ex{Finite Derivate}{
  Suppose we want to find the positions inside the image where there is a sudden horizontal passage from a dark region to a bright one. The pattern we are looking has that kernel:
  \[
    \begin{bmatrix}
      -1 & -1
    \end{bmatrix}
  \]
  or, varying the distance between pixels:
  \[
    \begin{bmatrix}
      -1 & 0 & 1
    \end{bmatrix}
  \]
  \begin{center}
    \includegraphics[width=8cm]{paginette/imgs/kernel.png}
  \end{center}
}

\subsection{Discovering patterns}

instead of using human designed pre-defined patterns, let the net learn them. This is particularly useful in deep networks:
\begin{itemize}
  \item stacking kernels we can learn more complex patterns 
  \item adding non-linear activations we synthesize complex, non-linear kernels
\end{itemize}

\subsubsection{Receptive field}
\dfn{receptive field}{
  The receptive field of a neuron in a convolutional layer is the specific region of the input image that influences the neuron's output. It defines the spatial extent of the input data that the neuron "sees" and processes.
}
It is equal to the dimension of an input image producing (without padding) an output with dimension 1

\nt{
  A neuron cannot see anything outside its receptive field
}

