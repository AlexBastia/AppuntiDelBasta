% \begin{document}
\chapter{Neural Networks}
Only known way to implement deep learning. 

An artificial neuron has a set of inputs that are summed with a bias, and the output is calculated using an ctivation function. invented in the '50 with the name 'perceptron', the activation function was discrete (0 or 1). This method is obsolete because it doesn't permit training. The function musn't be linear, because we want to sequencially compose neurons and the composition of linear functions is just another linear function and deosn't add any complexity (could be calculated with just one neuron). 

There are different activation functions (threshol, logistic, hyperbolic, rectified linear (newest used in AlexNet, big jump)). 

The name neuron comes from the biological counterpart (obviously) which works in a similar way:
\begin{itemize}
  \item Dendritic tree: connections with other neurons (synapses)
    \item In the body the inputs are summed and passed through the axon hilock, which performs a sort of thresholding before being passed to other neurons
\end{itemize}

Comparing artificial neural networks to our brains, the number of neurons can be similar ($ 2\cdot 10^{10} $for the biggest models), but the sheer size doesn't necessarily correlate to a more intelligent system (other animals have bigger brains), we actually don't really know what else there is. The switching time for real neurons is actually slower than artificial ones, seen as it's a chemical reaction and not electrical. Each neuron is connected to meny other neurons in the brain ($ 10^{4-5} $), reaching reaction times $ < 100 $, so the brain isn't very deep (number of intermediate nodes) and it's very parallelised.

\section{Topologies}
\dfn{Feed-forward}{
  Acyclic networks with unidirectional data flux.
}
\nt{
  Our brains are cyclic networks.
}

\subsection{Layers}
The network is built by sets of structured neurons that are combined to build the whole.

For each dense layer, each neuron has an input, weights, a constant bias from which an output is calculated. This operation can be parallelised using the whole layer with a matrix of weights and a vector of biases, using the same inputs over all the neurons. The input is a vector in multiple dimensions (tensor), and after algebraic manupulations a new tensor is returned. 

\section{Features and deep features}

TODO: manca una lezione intera

\section{Expressivness}
What functions can we calculate? Does a set of weights exist for a neural network to calculate a function? Does it have to be deep?

\subsection{Single Layer}
Can we implement a NAND function with a perceptron? If so, we can calculate all logical expressions, or simulate all circuits.

\nt{
  NAND is a complete set of logical operators, but they need to be composed to do so. Meaning a single layer being able to calculate this function does not imply that the perceptron is complete, as we'll later see.
}

Can we find two weights $ w_1, w_2 $ and a bias $ b $ such that
\[
  nand(x_1, x_2) = \begin{cases}
  1 & \sum_{i} w_ix_i + b \geq 0\\
  0 & 
  \end{cases}
\]

We can do this graphically looking at a plane. We want to be able to draw a hyperplane (a line in $ R^2 $) that divides the plane in two different areas, one for values that should equal 0 ($ (1,1) $) or 1 ($ (0,0), (0,1), (1,0) $).

TODO: drawing

A possible equation for such line is
\[
-2x_1 -2x_2 + 3 \geq 0
\]

So the output is
\[
output = \begin{cases}
  1 & -2x_1 -2x_2 + 3 \geq 0\\
 0 & 
\end{cases}
\]

What we can't calculate is the \textbf{XOR} or its complementary (equality). This is clear when looking at the plane
TODO: drawing

This is a limitation in linear methods (like logistic regression), because we need to compare both input features.

In conclusion, single layer perceptrons are not logically complete.

\subsection{Multi-layer perceptron}
Can we compute XOR by stacking perceptrons? Yessir, look at the image
TODO: ahhhh

How deep does a logic network need to be to calculate a certain expression? We can look at the depth as the number of nested logical connectors, or the depth of the tree generated by the expression. In conjunctive or disjunctive normal form, all trees have depth of 3, and each logic formula can be transformed in one of these forms. To be honest, we can remove the negation layer and use only 2 layers with cooler gates. 

So the single layer is cringe, but two layers can express any continuous function.

\nt{
  It's thanks to the activator function that the composition of layers adds expressivity, as composition of linear functions is always linear.
}

\subsection{Deep layers}
So why go deep? Two layers are already complete! 
\begin{itemize}
  \item Convolution layers (TODO: non ho capito)
\item Transformers arent actually that deep, as transformer blocks are essentially made up of 3-4 layers, creating a total depth of 30-40 layers, which isn't actually that deep.
\end{itemize}

\nt{
  In some cases, transforming a logical expression into normal form can create an exponentially longer expression, making it a lot less efficient.
}

In short, deep layers can be more efficient than shallow ones in some cases

\section{Training}

\subsection{Current loss}
Suppose we have a neural network with some configurations of trhe parameters $ \theta $.

We can pass a \textbf{batch} of data in input to be processed in parallel and get an output that can be compared to an expected output. This is a \textbf{forward pass}.

Using gradient descent, we can minimize the loss function by changing certain parameters. This is called \textbf{backward pass} because we need to calculate the parameter's gradinets from back to front.

The cost of both passes is similar. During the forward pass, we need to memorise each tensor output in order to calculate gradients later on. So during training there's a higher memory use.

\subsection{Backpropagation}
To calculate the gradients we need, we use the \textbf{chain rule}:
\[
  h'(x) = f'(g(x))*g'(x)
\]
The derivative of a composition of a sequnce of function is the product of the derivatives of the individual functions. This function is iterable, so for each layer we multiply its derivative on their forward input (this is why we need to memorize it).

Binary thresholding has null derivative everywhere (its a step function), so it would ruing the chain.

Given linear layers combined by activator functions, at layer $ \ell $ we have
\[
  a^l = \sigma(b^l + w^l \cdot x^l)
\]

\begin{itemize}
\item $ a^l $ is the \textbf{activation vector}
\item $ z^l $ is the \textbf{weighted input}
\item $ x^{l+1} = a^l, x^1 = x $
\end{itemize}

Partial derivative
\[
\delta^l = \frac{\delta E}{\delta z^l}
\]

We have to traverse the dense part. The parameters we're interested in are the parameters of the linear transformation. We transpose the matrix to calculate the layer at level $ l+1 $. 

We can calculate all partial derivatives with respect to $ z $, but we don't want this, we want $ b $. But in reality we just need to multiply by the derivative with respect to $ b $ of $ z = ... $, which is one. So the two values are the same. We can do the same for $ w $, which has derivative $ x^l $, which is the last layer's output ??.

We don't really need to know all the formulas, just the main concept.

\subsubsection{Vanishing gradient problem}
When going backwards, its possible for the gradient to become 0. In this case we don't have any movement. This is a problem for certain activator function. The sigmoid is the continuous version of a binary threshold with a very flat derivative. After many levels of backpropagation being multiplied by numbers less than 0.25, we can quickly go to 0 ???. The Relu function doesn't have this problem (derivative is 0 or 1)

% \end{document}
