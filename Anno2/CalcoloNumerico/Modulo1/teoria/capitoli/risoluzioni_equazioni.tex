\chapter{Risoluzione di equazioni (generale)}
\section{Bisezione}
Come primo metodo per risolvere (approssimativamente) equazioni, usiamo il metodo della \textbf{bisezione}. Questo sfrutta il teorema degli zeri per trovare uno dei possibili zeri di una funzione continua su un intervallo chiuso con valori discordi di segno agli estremi. Come esempio, prendiamo questa equazione:
\[
x = cos x
\]
Che possiamo riscrivere come:
\[
  (f(x) = x-cos x), \quad x \in \mathbb{R}. f(x) = 0
\]
Dato l'intervallo $ [-1, 1] $, otteniamo il seguente grafico:
\begin{center}
    no puede soportar este sufrimento
  %\includegraphics[width=0.5\textwidth]{img/2024-09-23-12-11-58.png}
\end{center}

Ci si chiede se esistono tali implicazioni:
\begin{itemize}
    \item Esistenza e unicità della soluzione
    \item Esistenza di algoritmi per calcolare soluzione
\end{itemize}

Innanzi tutto si tenga conto di tale teorema 
\teorema{
    \textbf{Teorema di Bolzano}: \\
        Sia $F:[a,b] \in \mathbb{R} \rightarrow \mathbb{R}$ continua in $[a,b]$ e $f(a)\cdot f(b)<0 \Rightarrow \exists x^*:F(x^*)=0$
}
Per la dimostrazione si vedano gli appunti/libri di analisi

\subsubsection{Primo algoritmo di bisezione: n-iterativo}
Intuitivamente l'algoritmo di bisezione ha due fasi:
\begin{enumerate}
    \item se $f(a)\cdot f(c)=0$ la radice è nell'intervallo $[a,c]$ che diventa la nuova versione dell'intervallo $[a,b]$
    \item altrimenti, se  $f(c) \cdot f(b)=0$ la radice è nell'intervallo $[c,b]$ che diventa la nuova versione dell'intervallo $[a,b]$
\end{enumerate}

Ecco qui descritto l'algoritmo di bisezione in pseudocodice:

\begin{algorithm}[H]
    \caption{primo algoritmo di Bisezione (Function f, int a, int b, int n)}
    \KwData{Funzione continua $f(x)$, continua in $a$ e $b$ con $f(a) \cdot f(b) < 0$, un numero N di iterazioni}
    \KwResult{Approssimazione della radice}



    \For{1 to n}{
        $c \leftarrow \frac{a + b}{2};$\\
        \If{$f(c) = 0$}{
            \Return $c$\ \tcp*{Trovata la radice esatta}
        }
        \If{$f(a) \cdot f(c) < 0$}{
            $b \leftarrow c$\ \tcp*{La radice è nell'intervallo $[a, c]$}
        }
        \Else{
            $a \leftarrow c$\ \tcp*{La radice è nell'intervallo $[c, b]$}
        }
    }
    \Return $c$\tcp*{Approssimazione della radice}
\end{algorithm}

Questo tipo di soluzione per la bisezione ha un numero $n$ di iterazioni per approssimare la soluzione, questa soluzione è molto semplice ma non la migliore. Vi è tuttavia una soluzione migliore

\subsubsection{Secondo algoritmo di bisezione: tolleranza d'errore}
Nella pratica è molto più utile definire una costante $\epsilon$ che vuole rappresentare l'errore di approssimazione massimo, di modo che un'approssimazione $\overline{r}$ della radice reale $r$ grantisca che $|r- \overline{r}| \leq \epsilon$. In altre parole si vuole garantire che $r\in [\overline{r}-\epsilon,\overline{r}+\epsilon]$. \\ Nel nostro caso $\overline{r}$ è il valore di $c=(b-a)/2$ mentre impostiamo $\epsilon = (b-a)/2$. Si imposti inoltre un \textbf{errore di tolleranza $e_{tol}$}  per iterare le operzione fino a che non si incontri. Di seguito è riportato l'algoritmo:

%todo RIFARLO
\begin{algorithm}[H]
    \caption{Algoritmo di Bisezione}
    \KwData{Funzione continua $f(x)$, estremi $a$ e $b$ con $f(a) \cdot f(b) < 0$, tolleranza $\epsilon$}
    \KwResult{Approssimazione della radice $x^*$ con $|f(x^*)| <Tocca la rivoluzione perle sighe del che a 2 euro
 \epsilon$}
    \While{$|b - a| > \epsilon$}{
        $c \leftarrow \frac{a + b}{2}$;
        \If{$f(c) = 0$}{
            \Return $c$\; \tcp*{Trovata la radice esatta}
        }
        \If{$f(a) \cdot f(c) < 0$}{
            $b \leftarrow c$\; \tcp*{La radice è nell'intervallo $[a, c]$}
        }
        \Else{
            $a \leftarrow c$\; \tcp*{La radice è nell'intervallo $[c, b]$}
        }
    }
    \Return $\frac{a + b}{2}$\; \tcp*{Approssimazione della radice}
\end{algorithm}

%! MANCANO DEI PEZZI

\section{Iterazione di punto fisso}
Presa la funzione $ f $ di cui vogliamo trovare lo zero, possiamo definire una funzione $ g(x) = x-w(x)f(x) $ dove $ w $ e' una funzione peso \textit{positiva} e \textit{limitata}. Quindi:
\[
  f(x) = 0 \iff g(x) = x
\]
\begin{center}
  \input{./PGF/PuntoFisso1.pgf}
\end{center}

\begin{center}
  \input{./PGF/PuntoFisso2.pgf}
\end{center}

Quindi siamo passati dal cercare uno zero di una funzione a trovare il \textbf{punto fisso}:
\dfn{Punto fisso}{
  Data $ g:D \to C $, si chiamano \textbf{punti fissi} tutti i punti $ x \in D: $
  \[
    g(x) = x
  \]
}
Per trovare il punto fisso, bisogna quasi sempre usare un metodo \textit{iterativo}, ovvero di approssimazioni successive, seguendo questa procedura:
\begin{itemize}
\item Iniziare con una prima approssimazione $ x_0 $
\item Iterare con la seguente formula: $ x_{k+1} = g(x_{k}) $
\end{itemize}

\mprop{}{
  Data una funzione $ g:D\to C $ continua e una successione $ x_n \in D. x_{k+1} = g(x_k) $ tale che $ x_n \xlongrightarrow{n \to +\infty} p $, allora:
  \[
    g(p) = p
  \]
  Ovvero $ p $ e' un \textbf{punto fisso} di $ g $.
}
\pf{Dimostrazione}{
  Per proprieta' delle successioni e per ipotesi, sappiamo che $ \lim_{k\to+\infty}x_k = p $. Quindi, per continuita' di $ g $:
  \[
    \lim_{k\to+\infty} g(x_k) = g(p)
  \]
  Inoltre, per costruzione della successione $ x_n $, sappiamo che $ g(x_k) = x_{k+1} $, quindi:
  \[
    \lim_{k\to +\infty} x_{k+1} = \lim_{k\to+\infty} g(x_k)
  \]
  \[
    p = g(p)
  \]
}
\subsection{Mappe contrattive}
Ma come facciamo ad assicurarci che la successione converga? Ecco che entrano in gioco le \textbf{mappe contrattive}.
\dfn{Mappa}{
  Una \textbf{mappa} e' una funzione $ g: D \to D $ dove $ D $ e' un intervallo chiuso $ [a,b] $.
}
\mprop{}{
  Presa una mappa continua su un intervallo $ [a,b] $, allora:
  \[
    \exists p \in [a,b]. p \text{ e' un punto fisso}
  \]
}
\pf{Dimostrazione}{
  Consideriamo la funzione $ f(x) = x - g(x) $, dove $ g $ e' una mappa continua sull'intervallo $ [a,b] $. Quindi $ f(a) = a - g(a) \leq 0 $ e $ f(b) = b - g(b) \geq 0 $, dato che per definizione di mappa $ g(a), g(b) \in [a,b] $. Quindi per teorema degli zeri $ \exists p \in [a,b]. f(p) = 0 $. Quindi $ g(p) = p $ e' un punto fisso.
}
\begin{center}
  \input{./PGF/PuntoFisso3.pgf}
\end{center}

Come si vede dal grafico della mappa continua $ g(x) $ su $ [-4, 4] $, e' possibile che una mappa abbia piu' di un punto fisso. Per assicurarci l'esistenza di una soluzione unica, dobbiamo aggiugere un'ultima condizione:
\dfn{Mappa contrattiva}{
  Una mappa $ g: D \to D $ si dice \textbf{contrattiva} se esiste $ C < 1 $ tale che:
  \[
    \forall x, y \in D. |g(x)-g(y)| \leq C|x-y|
  \]
  $ C $ e' detta costante contrattiva 
}
Questo significa che per ogni due punti che prendiamo dal dominio, la loro distanza sara' sempre maggiore (o uguale) alla distanza fra i due punti corrispondenti nel codominio. Graficamente (in $ \mathbb{R}^2 $):
\begin{center}
 \input{./PGF/PuntoFisso4.pgf}
\end{center}
\thm{}{
  Data una mappa contrattiva $ g $ su un intervallo chiuso $ D $, esiste un solo punto fisso $ p $ a cui converge la successione $ x_n $ dove $ x_{k+1} = g(x_k) $, scegliendo $ x_0 $ come qualunque punto in $ D $.
}
\pf{Dimostrazione (singolo punto fisso)}{
Assumiamo che $ g $ abbia piu' di un punto fisso e dimostriamo l'assurdo. Consideriamo il seguente grafico:
\begin{center}
 \includegraphics[width=0.5\textwidth]{img/2024-09-28-17-27-59.png}
\end{center}
Prendendo due dei punti fissi, $ p_1, p_2 $, notiamo che $ \frac{|g(p_1) - g(p_2)|}{|p_1 - p_2|} = \frac{|p_1 - p_2|}{|p_1 - p_2|} = 1 $, ma dato che la costante $ C $ deve essere minore di $ 1 $, abbiamo dimostrato l'assurdo. 
}
Manca la dimostrazione della garanzia dell'esistenza del punto fisso, che vedremo piu' avanti. (\ref{dimConvergPtFisso})

\subsubsection{Come controllare se una funzione differenziabile e' una mappa contrattiva}
Nella dimostrazione precedente e' apparsa una forma che somigliava molto a una derivata. Infatti, se la funzione $ g $ e' derivabile possiamo usare il seguente teorema:
\thm{}{
  Se una funzione $ g: [a,b] \to [a,b] $ e' differenziabile e esiste una costante $ C < 1 $ tale che $ \forall x \in [a,b]. |g'(x)|<C $, allora $ g $ e' una \textbf{mappa contrattiva}
}
\pf{Dimostrazione}{
  Per il teorema di Lagrange, $ \forall x < y \in [a,b]. \exists c \in [x,y]. g(y)-g(x) = g'(c)(y-x) $. Quindi, aggiungendo valori assoluti, $|g(y)-g(x)| = |g'(c)|\cdot |y-x| \leq C \cdot |y-x|$ per ipotesi. Quindi, per definizione, $ g $ e' una mappa contrattiva.
}

\subsection{Velocita' del miglioramento dell'approssimazione}
Chiamiamo l'errore assoluto $ |E_k| $ la differenza assoluta fra $ x_k $ e il valore effettivo del punto fisso $ p $.
\mprop{}{
  $ |E_{k+1}| \leq C|E_k| $, quindi 
  \[
  |E_k| \leq C^k|E_0|
  \]
  Dove $ E_0 = x_0 - p $.
}
Quindi per ogni iterazione, l'errore iniziale diminuisce almeno di un fattore $ C $ (che ricordo e' sempre $ <1 $ quindi e' una cosa buona). Cio' implica che piu' e' piccolo $ C $, piu' ci avviciniamo in fretta a $ p $. 
\pf{Dimostrazione}{
  Per definizione di errore $ E_{k+1} = x_{k+1} - p $, che possiamo riscrivere come $ g(x_k) - g(p) $, quindi per definizione di mappa contrattiva $ |E_{k+1}| = |g(x_k) - g(p)| \leq C|x_k - p| = C|E_k| $. Se sostituiamo $ k = 0 $ abbiamo il caso base:
  \[
  |E_1| \leq C|x_0 - p|
  \]
  Da cui ricorsivamente: 
  \[
    |E_k| \leq C|E_{k-1}| \leq C \cdot C|E_{k-2}| \leq ... \leq C \cdot ... \cdot C|E_0| = C^k|x_0 - p|
  \]
}

Ora siamo in grado di finire la dimostrazione di prima:
\pf{Diostrazione (convergenza)}{ \label{dimConvergPtFisso}
  Dato che $ C<1 $, se $ k \to +\infty $ allora $ C^k \to 0 $, quindi $ \lim_{k \to +\infty} |E_k| = \lim_{k \to +\infty} C^k|x_0 - p| = 0 $. Allora:
  \[
    x_k \xlongrightarrow{k \to +\infty} p
  \]
}

\section{Metodo di Newton}
Vediamo ora il metodo di Newton, che rispetto alla bisezione e' generalmente piu' veloce (ma non abbiamo la garanzia). Puo' essere dimostrato ricorrendo alle mappe contrattive o come approssimazione a linee tangenti, vediamo la prima:\\\\
Prendiamo una funzione differenziabile $ f $ e costruiamole una mappa contrattiva $ g $. Per migliorare l'efficenza, vogliamo fare in modo che la costante $ C $ abbia valore minimo dato un intorno della radice $ r $ di $ f $, come facciamo? Dato che $ C $ assume il valore della derivata massima della mappa, vogliamo fare in modo che la derivata di $ g $ sia minima per quell'intorno. Un modo per fare cio' e' assicurarci che $ g'(r) = 0 $, vediamo come:
\[
  g'(r) = 1 - w'(r)f(r) - w(r)f'(r) = 1 - w(r)f'(r)
\]
dato che $ f(r) = 0 $ per definizione, quindi:
\[
  w(r) = \frac{1}{f'(r)}
\]
Dato che non sappiamo il valore di $ r $, poniamo $ w(x) = \frac{1}{f'(x)} $ per ogni $ x $ nel dominio. Da notare che $ w(x) $ non esiste quando $ f'(x) = 0 $. Facendo cosi', la formula di iterazione diventa:
\[
  x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
\]
che e' la formula per il metodo di Newton.

