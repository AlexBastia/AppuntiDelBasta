\chapter{Sistemi lineari}

\section{Algoritmi per il calcolo di sistemi lineari}
\begin{center}
    \begin{math}
        \begin{cases}
            a_{21}x_1+ a_{12}x_2 +\dots + a_{1n}x_n = b_2\\
            a_{11}x_1+ a_{12}x_2 +\dots + a_{1n}x_n = b_1\\
            \vdots \\
            a_{n1}x_1+ a_{12}x_2 +\dots + a_{1n}x_n = b_n
        \end{cases}
    \end{math}
\end{center}

Con $m$ equazioni ed $n$ incognite. E sia $m=n$, corrispondente ad un sistema quadrato

Sia $Ax= b$ con
\[
    A = \begin{pmatrix}
        a_{11} && \dots && a_{1n} \\
        a_{21} && \dots && a_{2n} \\ 
        \vdots && \vdots && \vdots \\
        a_{n1} && \dots && a_{nn}
    \end{pmatrix}
\]
e 
\[
    x \in \mathbb{R}^n =\begin{pmatrix}
        x_1 \\
        x_2 \\
        \vdots \\
        x_n
    \end{pmatrix},
    b \in \mathbb{R}^n = \begin{pmatrix}
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{pmatrix}
\]
Allora su ha che il sistema $Ax=b$ ha \red{una ed una sola soluzione sse $A$ non è singolare} ($detA\neq 0$)

\[
    Ax = b \iff A^{-1}Ax = A^{-1} b \iff x =A^{-1}b    
\]

Dove \red{il Calcolo di $A^{-1}$ complessità computazionale $O(n^3)$}

A questo proposito ci vengono in aiuto diversi metodi per il calcolo:
\begin{itemize}
    \item \textbf{Metodi Diretti}: la soluzione viene calcolata in un numero finito di passi modificando la matrice del problema in modo da rendere piú agevole il calcolo della soluzione. 
    
    Es. 
    \begin{itemize}
        \item Matrici triangolari: Metodi di Sostituzione
        \item Fattorizzazione LU
        \item  fattorizzazione di Cholesky, 
    \end{itemize}

    \item \textbf{Metodi Iterativi}: Calcolo di una soluzione come limite di una successione di approssimazioni $x_k$, senza modificare la struttura della matrice $A$. Adatti per sistemi di grandi dimensioni con matrici sparse (pochi elementi non nulli)
\end{itemize}
\subsection{Metodi diretti}
Iniziamo con la definizione di matrici triangolari:
\dfn{Matrice triangolare inferiore}{
    Una matrice $M\in \mathbb{R}^{n\times n}$ è detta \textbf{triangolare inferiore} se:
    \[
        L = \begin{pmatrix}
            l_{11} & 0 & 0 & \dots & 0 \\
            l_{21} & l_{22} & 0 & \dots & 0 \\
            l_{31} & l_{32} & l_{33} & \dots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            l_{n1} & l_{n2} & l_{n3} & \dots & l_{nn}
        \end{pmatrix}
    \]

    Dove $l_{ij}$ per $i < j$, ossia tutti gli elementi sopra la diagonale (con $j>i$) sono nulli
}
Da cui discende la definizione di \textbf{sistema triangolare inferiore}:
\dfn{sistema triangolare inferiore}{
    Un \textbf{sistema triangolare inferiore} è un sistema lineare di equazioni in cui la matrice dei coefficienti è una matrice triangolare inferiore, e si presenta nella seguente forma:
    \[
        \begin{pmatrix}
            a_{1,1} & 0 & 0 & \dots & 0 \\
            a_{2,1} & l_{2,2} & 0 & \dots & 0 \\
            \vdots & \vdots & \ddots & \ddots & \vdots \\
            a_{n-1,1} & a_{n-1,2} & \dots & a_{n-1,n-1} & 0 \\
            a_{n,1} & a_{n,2} & a_{n,3} & \dots & a_{n,n}
        \end{pmatrix}    
        \begin{pmatrix}
            x_1 \\ x_2 \\ \vdots \\x_{n-1} \\ x_{n}
        \end{pmatrix}
        = 
        \begin{pmatrix}
            b_1 \\ b_2 \\ \vdots \\b_{n-1} \\ b_{n}
        \end{pmatrix}
    \]

    e in un sistema lineare:
    \[
        \begin{cases}
            a_{1,1} x_1 = b_1 \\
            a_{2,1} x_1 + a_{2,2}x_2 = b_2 \\
            a_{3,1} x_1 + a_{3,2} x_2 + a_{3,3}x_3 = b_3 \\
            \vdots \\
            a_{n,1} x_1 + a_{n,2} x_2 + \dots + a_{n,n}x_n = b_n   
        \end{cases}
    \]
}

Adesso c'è contrapposto con il \textbf{sistema triangolare superiore}
\dfn{matrice triangolare superiore}{
    Una matrice \( M \in \mathbb{R}^{n \times n} \) è detta triangolare superiore se:
    \[
        L = \begin{pmatrix}
        a_{11} & a_{12} & a_{13} & \cdots & a_{1,n} \\
        0 & a_{2,2} & a_{2,3} & \cdots & a_{2,n} \\
        0 & 0 & a_{3,3} & \cdots & a_{3,n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & a_{n,n} \\
        \end{pmatrix}
    \]
    Dove \( u_{ij} = 0 \) per \( i > j \), ossia tutti gli elementi sotto la diagonale (con \( i > j \)) sono nulli.

}
Da cui discende la definizione di \textbf{sistema triangolare superiore}
\dfn{sistema triangolare superiore}{
    Un \textbf{sistema triangolare superiore} è un sistema lineare di equazioni in cui la matrice dei coefficienti è una matrice triangolare superiore, e si presenta nella seguente forma:

    \[
        \begin{pmatrix}
            a_{1,1} & a_{1,2} & a_{1,3} & \cdots & a_{1,n-1} & a_{1,n} \\
            0 & a_{2,2} & a_{2,3} & \cdots & a_{2,n-1} & a_{2,n} \\
            0 & 0 & a_{3,3} & \cdots & a_{3,n-1} & a_{3,n} \\
            \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
            0 & 0 & 0 & \cdots & a_{n-1,n-1} & a_{n-1,n} \\
            0 & 0 & 0 & \cdots & 0 & a_{n,n} \\
        \end{pmatrix}
        \begin{pmatrix}
            x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_{n-1} \\ x_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_1 \\ b_2 \\ b_3 \\ \vdots \\ b_{n-1} \\ b_n
        \end{pmatrix}
    \]

    e in un sistema lineare:

    \[
    \begin{cases}
        a_{1,1} x_1 + a_{1,2} x_2 + \cdots + a_{1,n} x_n = b_1 \\
        a_{2,2} x_2 + a_{2,3} x_3 + \cdots + a_{2,n} x_n = b_2 \\
        a_{3,3} x_3 + \cdots + a_{3,n} x_n = b_3 \\
        \ \ \ \vdots \\
        a_{n,n} x_n = b_n
        \end{cases}
    \]
}

\subsubsection{algortimo delle sostituzioni}
Quando si ha a che fare con matrici triangolari la soluzione può essere facilmente calcolata attraverso \textbf{l'algoritmo delle sostituzioni}, che si differenzia nel caso di matrici triangolari inferiori o superiori

\teorema{
    \begin{itemize}
    \item Nel caso di matrici triangolari superiori l'algoritmo prende il nome di \textbf{sostituzioni all'indietro} ed è descritto dal sistema qui sotto:
    \[
        \begin{cases}
            x_n = \frac{b_n}{a_{nn}}\\
            x_i = \frac{b_i - \sum_{k=i+1}^{n}a_{ik}x_k}{a_{ii}} & i = n-1, n-2, \dots 1
        \end{cases}    
    \]
    \item Nel caso di matrici triangolari inferiori l'algoritmo prende il nome di \textbf{sostituzioni in avanti} ed è descritto dal sistema qui sotto:
    \[
        \begin{cases}
            x_1 = \frac{b_1}{a_{11}}\\
            x_i = \frac{b_i - \sum_{k=1}^{i-1} a_{ik}x_k }{a_{ii}} & i = 2,3,\dots,n
        \end{cases}    
    \]

\end{itemize}    
    In entrambi i casi il numero di operazioni richiesto è $\simeq \frac{n(n+1)}{n} = O(n^2)$
}

\subsubsection{Eliminazione di Gauss}

Si eliminano le incognite in modo sistematico per trasformare i sistema lineare in uno equivalente con matrice a struttura triangolare superiore, più precisamente si introduce una successione di matrici tale che:
\[
            A^{(k)} = (a_{ij}^{(k)}), \quad k = 1,\dots,n
\]
Con $A^{(1)} = A$ e $A^{(n)} = U$ e dove la matrice $A^{(k+1)}$ ha tutti gli elementi $\{a_{ij}^{(k+1)}, j+1 \leq i\leq n, 1\leq j \leq k\}$ nulli. Ovvero essa differisce da $A^{(k)}$ per avere \red{gli elementi sottodiagonali della colonna k-esima nulli}. Risparmio ulteriori spiegazioni di come funziona e passo direttamente alla formula:
\[
    \begin{array}{l}
        m_{ik} = \frac{q_{ik} ^ {(k)}}{q_{kk} ^ {(k)}} \\
        a_{ij}^{(k+1)} = a_{ij}^{(k)} - m_{ik} a_{kj} ^ {(k)}
    \end{array}
\]


Inoltre è possibile che bisogni scambiare le righe, ad esempio quando $det(A_k) = 0$ e pertanto il $a_{kk} = 0$, Perciò occorre scambiarli con altre righe. Si noti che lo scambio di righe $i,j$ di una matrice $A$ equivale a moltiplicarla per una matrice $P^{ij}$ tale che:
\[
    (P^{(ik)})_{km} =
        \begin{cases} 
            \delta_{km} & \text{se } k \neq i \text{ e } k \neq j, \\ 
            0 & \text{se } k = m \text{ e } (k = i \text{ o } k = j), \\ 
            1 & \text{se } k = i \text{ e } m = j, \\ 
            1 & \text{se } k = j \text{ e } m = i.
        \end{cases}
\]


La quale verrà chiamata \textbf{matrice di permutazione}

\subsubsection{algoritmo di gauss rivisto come mettodo di fattorizzazione LU}
Si può dimostrare che l'Algoritmo di Gauss realizza la seguente fattorizzazione della matrice $A$ in due matrici triangolari:
\[
    A = LU = \begin{pmatrix}
    1 & 0 & 0 & 0\\
    * & 1 & 0 & 0\\
    * & * & 1 & 0\\
    * & * & * & 1\\
    \end{pmatrix} \cdot \begin{pmatrix}
    * & * & * & *\\
    0 & * & * & *\\
    0 & 0 & * & *\\
    0 & 0 & 0 & *\\
    \end{pmatrix}
\]

Passiamo dunque, direttamente al teorema esplicativo

\teorema{
    Sia $A\in\mathbb{R^{n\times n}}$ una matrice, è possibile fattorizzarla nel seguente modo:
    \[
        A = L U
    \]
    Dove:
    \begin{itemize}
        \item $U=A^{(n)}$ è una matrice triangolare superiore (ovvero la matrice posto sotto l'eliminazione di gauss)
        \item $L$ è la matrice triangolare inferiore dei moltiplicatori, ovvero:
        \[
            L = \begin{cases}
                L_{ij} = m_{ij} & i<J\\
                L_{ij} = 1 & i=j\\
                L_{ij} = 0 & j>i
            \end{cases}    
        \]
    \end{itemize}

    Mentre se si necessita di uno scambio di righe durante l'eliminazione di gauss la fattorizzazione sarà:
    \[
        LU = PA
    \]
    Dove $P$ è la matrice di permutazione vista prima
}

\dimostrazione{
    Sia $M_k = I - m_k e^T_k$, dove:
    \begin{itemize}
        \item $(e_k)_j=\delta_{kj}$ quindi $e_k^T = [0,0,\dots,1,\dots,0]$ Dove 1 è alla posizione 
        
        \item $m_k = \begin{pmatrix}
            0\\
            \dots \\
            m_{k+1,k}\\
            m_{k+2,k}\\
            \dots\\
            m_{nk}
        \end{pmatrix}$
    \end{itemize}
    Che equivale in termini di componenti è uguale a:
    \[
        (M_k)_{ip} = \delta_{ip} - (m_k e^T_k)_{ip} = \delta_{ip} - m_{ik}\delta_{kp}    
    \]
    Dalle formule dell'algoritmo di gauss si ha:
    \[
        a_{ij}^{(k+1)} = a^{(k)}_{ij}- m_{ij}^{(k)}a_{ij}^{(k)} = a_{ij}^{(k)} - m_{ik}\delta_{kk}a^{(k)}a_{kj}^{(k)} = \sum_{p}(\delta_{ip}-m_{ik}\delta_{kp})a_{pj}^{(k)}
    \]   
    e quindi è facile verificare che:
    \[
        A^{(k+1)} = M_kA^{(k)}    
    \]
    Si prenda osservi, intatti, che moltiplicare $M_k$ a sinistra di $A$ ha come effetto la sottrazione da ogni riga $R_i, (i=k+1, \dots, n)$ della riga $m_{ik}$. Allora:
    \[
        U = A^{(n)} = \underbrace{M_{n-1}M_{n-2}\dots,M_1}_{M}A    
    \] 
    Ovvero: 
    \[
        A = M^{-1} U    
    \]
    E dato che il prodotto di matrici triangoli e l'inversa di una matrice triangolare sono ancora matrici triangolare, si ha che $M^{-1}$ è triangolare, quindi definiamo $M^{-1}=L$.
    
    Adesso dimostriamo la parte del teorema con la matrice di permutazione, ricordando quindi la formula relativa alla matrice di permutazione si ha:
    \[
        A^{(k+1)} = M_k P_k A^{(k)}
    \]

    Per $U$ quindi si avrà la seguente espressione:
    \[
        U = M_{n-1}P_{n-1}\dots M_1P_1A    
    \]
    posto 
    \[
        P = P_{n-1}\dots P_1\text{ e }L = P(M_{n-1}P_{n-1}\dots M_1 P_1)^{-1}
    \]
    Si ha allora che $L$ è triangolare inferiore e si ottiene:
    \[
        LU = P(M_{n-1}P_{n-1\dots M_1P_1})^{-1} (M_{n-1}P_{n-1}\dots M_1P_1)A = PA
    \]
}

\esempio{
    Supponiamo di voler fattorizzare la matrice:
    \[
        A = \begin{pmatrix}
            2&3\\
            4&7
        \end{pmatrix}   
    \]
    Poi inizializzo $L$ come una matrice identità e$U=A$
    \[
        L=\begin{pmatrix}
            1&0\\
            0&1
        \end{pmatrix}
        \quad
        A = \begin{pmatrix}
            2&3\\
            4&7
        \end{pmatrix}   
    \]
    Poi eseguo l'algoritmo di Gauss in $U$ avendo come pivot $2$, in questo caso occorre solo modificare solo la seconda in quanto la matrice ha solo 2 righe. Il fattore moltiplicativo nella riga 2 e colonna 1 è $l_{2,1} =  \frac{4}{2} = 2$, si può procedere così con l'eliminazione Gaussiana ottenendo:
    \[
        U =\begin{pmatrix}
            1&0\\
            2&1
        \end{pmatrix} 
    \]
    Si inserisca, inoltre, $l_{2,1}$ nella matrice $L$:
    \[
        L=\begin{pmatrix}
            1&0\\
            2&1
        \end{pmatrix}    
    \]
    La fattorizzazione, così, è finita e si ha $A=L\times U$:
    \[
        \begin{pmatrix}
            2&3\\
            4&7
        \end{pmatrix} = 
        \begin{pmatrix}
            1&0\\
            2&1
        \end{pmatrix} 
        \begin{pmatrix}
            1&0\\
            2&1
        \end{pmatrix} 
    \]
}

A che serve tutto questo? Perché tutto sto ambaradam? Restando calmi e senza farci prendere dalla disperazione dobbiamo prendere atto che il nostro bellissimo potere fattorizzante ci permette di ridurci a risolvere sistemi lineari triangolari per poi risolverli con il potere dell'algoritmo delle sostituzioni (evviva). Si noti il seguente corollario


EOEOE
\cor{sistemi triangolari}{
    Nel caso in cui non si ricorra ad uno scambio di righe si ha
    \[
        Ax = b \iff LU x = b \iff \begin{cases}
            Ly = b\\
            Ux = y
        \end{cases}
    \]

    Nel caso in cui si ricorre ad uno scambio di righe si ha:
    \[
        Ax = b\iff PAx = Pb \iff LU x = Pb \iff \begin{cases}
            Ly = Pb\\
            Ux = y
        \end{cases}
    \]
}



\nt{
  Se $ A $ e' invertibile, la fattorizzazione LU esiste sse tutti i minimi principali (leading principal minor??) sono non nulli. Nel caso in cui $ A $ e' singolare ($ det(A) = 0 $), se i primi $ k = rk(A) $ minimi principali sono non nulli allora siamo sicuri che la fattorizzazione LU esiste, ma potrebbe esistere anche se ce ne sono di nulli. 
}

\begin{algorithm}[H]
    \SetAlgoNlRelativeSize{-1}
    \SetNlSty{textbf}{}{}
    \caption{Fattorizzazione LU}
    \KwIn{Intero $n$, Matrice $A[1..n,1..n]$}
    \KwOut{Array $B[1,2]$ contenente le matrici $L$ e $U$}
    
        Let $B$ be a new Array[2]\;
        Let $L$ be a new Matrix[1..n, 1..n]\;
        Let $U$ be a new Matrix[1..n, 1..n]\;
    
        % Inizializza L come matrice identità
        \For{$i = 1$ \KwTo $n$}{
            \For{$j = 1$ \KwTo $n$}{
                \If{$i == j$}{
                    $L[i, j] \gets 1$\;
                }
                \Else{
                    $L[i, j] \gets 0$\;
                }
            }
        }
        Let $p$ be a new Real\tcp*{dichiaro il pivot}
        \tcp{colonne per pivot}
        \For{$i = 1$ \KwTo $n$}{
            $p\gets A[i,i]$\tcp*{aggiorno il pivot}
            \tcp{righe}
            \For{$j = i+1$ \KwTo $n$}{
                $L[j,i] \gets \frac{A[j,i]}{p}$\tcp*{fattore moltiplicativo in L}
                \For{$k=i$ \KwTo $n$}{
                    $A[j,k] \gets A[j,k] - L[j,i] \times A[i,k]$\;
                }
            }
        }
    
        $B[1] \gets L$\;
        $B[2] \gets A$\;
        \KwRet{$B$}\;
    \end{algorithm}
\subsection{Fattorizzazione di Cholesky}
Si puo' usare solo nel caso speciale in cui $ A $ sia \textbf{definita semi-positiva}:
\dfn{Matrice semi-definita positiva}{
  $ A $ $ n \times n $ simmetrica e' \textbf{semi-definita positiva} se $ \forall x \in \mathbb{R}^n: $
  \[
  x^T A x \geq 0
  \]
}

Per cui vale:
\mprop{Autovalori di matrici semi-definite positive}{
 Data una matrice $ A $ semi-definita positiva, ogni autovalore di $ A $ e' positivo o nullo
}

Grazie a queste proprieta', possiamo dire che $ \exists L: $
\[
A = L \cdot L^T
\]
e trovarlo e' circa due volte piu' efficente rispetto a LU.



\subsection{metodo Doolittle e Fattorizzazione di Cholesky}
Allora sempre restando sulla cuazzo di fattorizzazione $LU$ esiste un bellissimo metodo in grado di fattorizzare agilmente la matrice di partenza $A$ attraverso formule ben preciso, ebbene date il benvenuto al \textbf{metodo Doolittle}:
\thm{
    metodo Doolittle
}{
    Sia $A\in\mathbb{R}^{n\times n}$, il metodo di Doolittle permette di decomporre la matrice $A$ come il prodotto di una matrice triangolare inferiore $L$ (dove $L_{ii}=1, \;\forall i \in\{1,\dots, n\}$) e una matrice triangolare superiore $U$ del tipo 
    \[
        A = LU
    \]
    attraverso le seguenti formule
    \begin{enumerate}
        \item \[
            U_{pj} = A_{pj}- \sum_{k=1}^{p-1} L_{pk} U_{kj} \quad j\geq p
        \]
        \item \[
            L_{ip} = \frac{1}{U_{pp}}\left(A_{ip}-\sum_{k=1}^{p-1}L_{ik}U_{kp}\right) \quad i>p
        \]
    \end{enumerate}
}
\dimostrazione{
    Si noti innanzi tutto che per il normale prodotto riga per colonna dobbiamo trovare due matrici $L$ e $U$ tale che
    \[
        A_{ij} = \sum_{k=1}^{r}L_{ik}U_{kj}\quad r=\min(i,j)   
    \]

    che equivale a dire (per ovvia dimostrazione):
    \[
        \begin{array}{lll}
            1)&A_{pj} = \sum_{k=1}^{p}L_{pk}U_{kj}& j\geq p\\
            2)&A_{ip} = \sum_{k=1}^{p} L_{ik} U_{kp} & i>p
        \end{array}
    \]
    Da queste formule mi riduco a dimostrare l'algoritmo:
    \begin{enumerate}
        \item \[
            A_{pj} = \sum_{k=1}^{p}L_{pk}U_{kj}=\sum_{k=1}^{p-1}L_{pk}U_{kj} + L_{pp}U_{pj}
        \]
        Dato che gli elementi della diagonale nella matrice $L$ sono uguale a 1 si ha:
        \[
             A_{pj} = \sum_{k=1}^{p-1}L_{pk}U_{kj} + L_{pp}U_{pj} = \sum_{k=1}^{p-1}L_{pk}U_{kj} + U_{pj} \implies  U_{pj} =  A_{pj} - \sum_{k=1}^{p-1} L_{pk}U_{kj}
        \]
        ricordandosi $j\geq p$
        \item \[
            A_{ip} = \sum_{k=1}^{p} L_{ik} U_{kp} =  \sum_{k=1}^{p-1}L_{ik}U_{kp} + L_{ip}U_{pp}
        \]
        banalmente si ha che:
        \[
            L_{ip} = \frac{1}{U_{pp}}\left(A_{ip}-\sum_{k=1}^{p-1}L_{ik}U_{kp}\right)
        \]
        ricordandosi $i>p$
    \end{enumerate} 
}

Si ha quindi una fattorizzazione che si ottiene formando le matrici sequenzialmente dalla 1 alla 2
\esempio{
    
    Consideriamo la matrice \( A \):
    \[
    A = \begin{bmatrix}
            2 & -1 & 1 \\
            3 &  3 &  9 \\
            3 &  3 &  5
        \end{bmatrix}.
    \]

    Passo 1: Inizializzazione delle matrici \( L \) e \( U \):
    \[
        L = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}, \quad
        U = \begin{bmatrix}
                0 & 0 & 0 \\
                0 & 0 & 0 \\
                0 & 0 & 0
            \end{bmatrix}.
    \]

    Passo 2: Calcolo degli elementi di \( U \) e \( L \) riga per riga.

    Riga 1:
    Gli elementi della prima riga di \( U \) si calcolano direttamente come:
    \[
        U_{11} = A_{11} = 2, \quad
        U_{12} = A_{12} = -1, \quad
        U_{13} = A_{13} = 1.
    \]

    Aggiorniamo \( U \):
    \[
        U = \begin{bmatrix}
            2 & -1 & 1 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix}.
    \]

    Gli elementi di \( L \) nella colonna 1 si calcolano come:
    \[
        L_{21} = \frac{A_{21}}{U_{11}} = \frac{3}{2} = 1.5, \quad
        L_{31} = \frac{A_{31}}{U_{11}} = \frac{3}{2} = 1.5.
    \]

    Aggiorniamo \( L \):
    \[
    L = \begin{bmatrix}
            1 & 0 & 0 \\
            1.5 & 1 & 0 \\
            1.5 & 0 & 1
        \end{bmatrix}.
    \]

    Riga 2:
    Gli elementi della seconda riga di \( U \) si calcolano sottraendo il contributo della prima riga (già calcolata) da \( A \):
    \[
        U_{22} = A_{22} - L_{21}U_{12} = 3 - (1.5)(-1) = 4.5,
    \]
    \[
        U_{23} = A_{23} - L_{21}U_{13} = 9 - (1.5)(1) = 7.5.
    \]

    Aggiorniamo \( U \):
    \[
    U = \begin{bmatrix}
            2 & -1 & 1 \\
            0 & 4.5 & 7.5 \\
            0 & 0 & 0
        \end{bmatrix}.
    \]

    Gli elementi di \( L \) nella colonna 2 si calcolano come:
    \[
        L_{32} = \frac{A_{32} - L_{31}U_{12}}{U_{22}} = \frac{3 - (1.5)(-1)}{4.5} = \frac{4.5}{4.5} = 1.
    \]

    Aggiorniamo \( L \):
    \[
        L = \begin{bmatrix}
            1 & 0 & 0 \\
            1.5 & 1 & 0 \\
            1.5 & 1 & 1
        \end{bmatrix}.
    \]

    Riga 3:
    L'elemento \( U_{33} \) si calcola come:
    \[
        U_{33} = A_{33} - L_{31}U_{13} - L_{32}U_{23} = 5 - (1.5)(1) - (1)(7.5) = 5 - 1.5 - 7.5 = -4.
    \]

    Aggiorniamo \( U \):
    \[
        U = \begin{bmatrix}
            2 & -1 & 1 \\
            0 & 4.5 & 7.5 \\
            0 & 0 & -4
        \end{bmatrix}
    \]

    Risultato finale
    La matrice \( A \) è stata scomposta come:
    \[
    L = \begin{bmatrix}
        1 & 0 & 0 \\
        1.5 & 1 & 0 \\
        1.5 & 1 & 1
    \end{bmatrix}, \quad
    U = \begin{bmatrix}
        2 & -1 & 1 \\
        0 & 4.5 & 7.5 \\
        0 & 0 & -4
    \end{bmatrix}.
    \]

    Verifica:\\
    Moltiplicando \( L \) e \( U \), si ottiene \( A \):
    \[
        A = LU = \begin{bmatrix}
            1 & 0 & 0 \\
            1.5 & 1 & 0 \\
            1.5 & 1 & 1
        \end{bmatrix}
        \begin{bmatrix}
            2 & -1 & 1 \\
            0 & 4.5 & 7.5 \\
            0 & 0 & -4
        \end{bmatrix}
        =
        \begin{bmatrix}
            2 & -1 & 1 \\
            3 & 3 & 9 \\
            3 & 3 & 5
        \end{bmatrix}
    \]
}

Adesso possiamo passare alla fattorizzazione di Cholesky, non prima di aver presentato un'importante definizione. Infatti si può usare solo nel caso speciale in cui $ A $ sia \textbf{definita semi-positiva}:
\dfn{Matrice semi-definita positiva}{
  $ A $ $ n \times n $ simmetrica e' \textbf{semi-definita positiva} se $ \forall x \in \mathbb{R}^n: $
  \[
    x^T A x \geq 0
  \]
}

Per cui vale:
\mprop{Autovalori di matrici semi-definite positive}{
 Data una matrice $ A $ semi-definita positiva, ogni autovalore di $ A $ e' positivo o nullo
}


\nt{
    Grazie a questa proposizione si la matrice $A$ da fattorizzare può essere scomposta senza ricorrere alla pivotazione (scambio di righe e/o colonne)
}

tramite questa osservazione ci possiamo ricondurre al seguente teorema
\thm{Metodo di Cholesky}{
    Sia $A\in\mathbb{R}^{n\times n}$ una matrice \textit{simmetrica e definita semi-positiva} allora esiste una matrice $H$ triangolare inferiore tale che:
    \[
        A= H H^T
    \]
    Che si può calcolare attraverso tale algoritmo
    \[
        \text{1)}\quad H_{pp} =\sqrt{ A_{pp}- \sum_{k=1}^{p}H^2_{pk}} 
    \]
    \[
        \text{2)}\quad H_{ip} = \left(A_{ip}-\sum_{k=1}^{p-1}H_{ik}H_{pk}\right) \frac{1}{H_{pp}}, \quad i\geq p + 1
    \]

    Con una complessità computazionale di $O\left(\frac{n^3}{6}\right)$
}
\dimostrazione{
    Sia $A\in\mathbb{R}^{n\times n}$ una matrice \textit{simmetrica e definita semi-positiva}, devo dimostrare che $\exists H : A = HH^T$, ovvero che $\exists H : a_{ij} = \sum_{k=1}^{i}h_{ik}h^T_{kj}$ dove $a\in K$ e $h\in H$ 

    Procedimento nel costruire tale matrice:
    \begin{itemize}
        \item elementi nella diagonale:
        \[
            a_{pp} = \sum_{k=1}^{p} h_{pk}h_{kp}^T = \sum_{k=1}^{p-1} h_{pk}h_{kp}^T +  h_{pp}h_{pp}^T 
        \]
        Per definizione di matrice trasposta si ha che $ h_{pp} = h_{pp}^T$, il che implica
        \[
            h_{pp} = \sqrt{a_{pp}- \sum_{k=1}^{p-1}h^2_{pk}}
        \]
        \item elementi sotto la diagonale:
        \[
            a_{ip} = \sum_{k=1}^{p} h_{ik}h_{kp}^T = \sum_{k=1}^{p-1} h_{ik}h_{kp}^T+ h_{ip}h_{pp}^T
        \]
        quindi
        \[
            h_{pp} = \left(a_{ip}-\sum_{k=1}^{p-1}h_{ik}h_{pk}\right)\frac{1}{h_{pp}}
        \]
    \end{itemize}
    Inoltre si ha, per definizione di matrice trasposta e matrice triangolare inferiore, che:
    \[
        a_{ip} = \sum_{k=1}^{p} h_{ik}h_{kp}^T = \sum_{k=1}^{p} h_{ik}h_{pk} =\sum_{k=1}^{p} h_{pk}h_{ik} = \sum_{k=1}^{p} h_{pk}h_{ki}^T = a_{ip}
    \]
    Dato che per ipotesi la matrice $A$ è simmetrica si ha che questa proposizione è corretta
}

Come per la fattorizzazione $LU$ ci troviamo a discutere del sistema seguente:
\cor{}{
    La suluzione $x$ si ottiene attraverso la soluzione di due sistemi triangolari:
    \[
        \begin{cases}
            Hy= b\\
            H^Tx=y
        \end{cases}
    \] 
}

\subsection{metodi iterativi}
I metodi iterativi si basano sull'idea di calcolare la soluzione del sistema 
\[
    Ax = b
\]

Come limite di una sucessione di vettori:
\[
    x = \lim_{k\to\infty}x^k
\]
e dove ci si fermerà al minimo $n$ per cui si abbia 
\[
    \|x^n-x\|<\epsilon 
\]

Una strategia generale per costruire una sucessiano $\{x^k\}$ è basata sulla tecnica dello "splitting" $A = P- N $ dove $P$ ed $N$ sono due matrici e $P$ è non singolare. Precisamente si ottiene
\[
    Px = Nx + b
\]

A questo punto, possiamo riscrivere il sistema in forma iterativa. L'idea è di utilizzare una stima iterativa \( x^{(k)} \) al passo \( k \) per sostituire \( x \) nel termine dipendente \( N x \), ottenendo una formula iterativa:

\[
P x^{(k+1)} = b + N x^{(k)}.
\]

Scomponendo ulteriormente:

\[
x^{(k+1)} = P^{-1}(b + N x^{(k)}).
\]

Questa è la \textbf{formula iterativa} per calcolare una nuova stima \( x^{(k+1)} \) a partire dalla stima precedente \( x^{(k)} \), adesso formalizziamo tutto in un teorema non prima di aver dichiarato un bel lemma

\mlenma{}{
    Sia $B = P^{-1}N$, allora si ha che 
    \[
        \lim_{k\to\infty} B^k = 0 \iff \|B\|<1
    \]
}

\thm{}{
    Sia $A = P - N$, si ha che assegnato $x^0$ si ottiene $x^k$ risolvendo tali sistemi 
    \[
        Px^k = Nx^{k-1} + b, \quad k\geq 1
    \]
    e si ha che $x^k$ è un'iterazione più vicina alla soluzione sse la matrice $\lim_{k\to\infty}{(P^{-1}N)}^k$ 
}
\dimostrazione{
    Sia 
    \[Px^k = Nx^{k-1}+b\]

    Posto 
    \[
        e^k = x^k -x   
    \]

    usando \( e^k \), possiamo scrivere:
    \[
        x^{(k)} = x + e^k.
    \]

    Per il passo successivo \( x^{(k+1)} \), avremo:

    \[
        x^{(k+1)} = x + e^{(k+1)}.
    \]
    Sostituendo nella formula iterativa \( x^{(k+1)} = P^{-1}(b - N x^{(k)}) \), otteniamo:
    \[
        x + e^{k+1} = P^{-1} \left(b - N (x + e^k)\right) \iff x + e^{k+1} = P^{-1} \left(b - Nx + Ne^k\right)
    \]

    Poiché \( A x = b \), possiamo sostituire \( b = A x \), e quindi otteniamo:
    \[
        x + e^{k+1} = P^{-1} \left(A x - N x - N e^k \right) \iff x + e^{k+1} = P^{-1} \left(Px  - N x - N e^k \right)
    \]



    


    
}
presentiamo qui il \textbf{metodo Jacobi}



