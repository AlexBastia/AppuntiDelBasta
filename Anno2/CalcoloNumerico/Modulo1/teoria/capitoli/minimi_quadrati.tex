\chapter{Problema dei minimi quadrati}
Si consideri il seguente sistema:
\[
    Ax = b    
\]
Dove $A\in \mathbb{R}^{m\times n},b\in \mathbb{R}^m, x\in \mathbb{R}^n$ e $m > n$. È facile verificare che risulta indeterminato, dato che il numero di equazioni è superiore al numero di incognite, non ammettendo, quindi, alcuna soluzione. 

Consci di questo fatto alquanto deprecabile e accettata la sua inalterabilità, si vuole però cercare la $x$ tale che l'espressione $Ax-b$ non sia $0$ ma "quasi", ovvero il più piccolo valore possibile. Si è così giunti al concetto di problema ai minimi quadrati:
\definizione{
    Si definisce il \textbf{problema ai minimi quadrati (LSQ)}, che consiste nel determinare il vettore $x \in R$ n che minimizzi il vettore dei residui $r = Ax - b$:
    \[
        \min_{x \in \mathbb{R}^n} \|Ax - b\|_2^2 = \min_{x \in \mathbb{R}^n} \|r\|_2^2
    \]
    dove $r = (r_1 , r_2 , \dots , r_n ) = (Ax_1 - b_1 , \dots , Ax_n - b_n)$
}

Si giunge così al seguente teorema:

\teorema{
    Sia $A\in \mathbb{R}^{m\times n}$ con $m>n$ e $rg(A)=k \leq n$, allora il problema
    \[
        \min_{x \in \mathbb{R}^n} \|Ax - b\|_2^2 = \min_{x \in \mathbb{R}^n} \|r\|_2^2    
    \]
    \textbf{Ammette sempre almeno una soluzione}, inltre:
    \begin{itemize}
        \item \textbf{$k=n$ (quindi ha rango massimo)}: il problema ha una ed una sola soluzione
        \item \textbf{$k < n$}: il problema ha infinite soluzioni e tali soluzioni formano un sottospazio di $\mathbb{R}^n$ di dimensione $n-k$
    \end{itemize}
}
\section{Equazioni  normali}
Come si fa, quindi, a minimizzare questo di residuo? Bhe bisogna trovare un "punto di minimo" che ovviamente va trovato con le derivate (nel nostro caso dato che siamo in $\mathbb{R}^n$ il gradiente). Intanto sappiamo che:
\begin{center}
    \begin{math}
        \|Ax - b\|_2^2 = (Ax - b)^T (Ax - b) \\
        = \left( -b^T + x^T A^T \right) (Ax - b) \\
        = -b^T A x + b^T b + x^T A^T A x \\
        \text{(sommando i termini simili)} \\
        b^T A x = x^T A^T b \\
        = x^T A^T A x - 2 x^T A b + b^T b
    \end{math}
\end{center}

Si pone:
\[
    f(x) = x^T A^T Ax - 2x^T Ab + b^T b
\]
Questa è un'equazione $f(x): \mathbb{R}^n -> \mathbb{R}$, condizione necessaria (ma non sufficiente) che un punto $x^*$ sia di minimo è che $\nabla f(x^*) = 0$ dove:
\[
    \nabla f = (\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n}
    )
\]
Dato che $f$ non è altro che una sommatoria di tre addendi, non mi resta che fare il gradiente di questi per poi addizionarli. Quindi:
\begin{itemize}
    \item $\nabla(x^TAx)=2A^TAx$
    \item $\nabla(2x^T Ab)=2A^Tb$
    \item $\nabla( b^T b)=0$
\end{itemize}
Allora:
\[
    \nabla f(x)=2A^TAx-2A^Tb
\]
Imponendo che il gradiente sia nullo si ottiene il cosiddetto \textbf{Sistema delle equazioni normali}
\[
    A^T Ax = A^T b    
\]

Se $ A^T A$ è simmetrica e definita positiva allora $A$ ha rango massimo, è possibile quindi risolvere il sistema con un opportuno metodo adatto alla matrice $A^T A$, ad esempio utilizzando la fattorizzazione di Cholesky, si ottiene una matrice $L$ t.c. $A^T A = LL^T$ e si risolvono nell’ordine i due sistemi:
\[
    \begin{cases}
        Ly = A^T b \\
        L^Tx = y  
    \end{cases}
\]
Tuttavia, il calcolo di $A^TA$ può \red{rendere il problema eccessivamente mal condizionato} poiché $K(A^TA) = K(A)^2$, ovvero \red{il numero di condizionamento della matrice $A^T A$ è approssimativamente il quadrato del numero di condizionamento della matrice $A$}. Ricordiamo che un problema mal condizionato è più difficile da risolvere numericamente, poiché anche piccoli errori di arrotondamento nei calcoli possono causare errori significativi nella soluzione finale

\section{Decomposizione in valori singolari (SVD)}
Se il rango $k<\min(m,n)$ allora vi sono infinite di soluzioni, ma ne esiste una sola $\overline{x}$ di norma minima. Tale soluzione si può ittenere tramite SVD

\teorema{
    Sia $A\in \mathbb{R}^{m\times n}$, con $rg(A) = k \leq n$ e sia $A = U\Sigma V^T$ la sua decomposizione in valori singolari, allora il vettore:
    \[
        x^* = \sum^k_i=1 \frac{u_i^Tb}{\sigma_i} v_i   
    \]
    è la soluzione di minima norma al problema:
    \[
        \min_{x\in\mathbb{R^n}} \|Ax - b\|_2^2    
    \]
}

per la proprietà dei vettore ortogonale di non alterare le norme di vettori e in corrispondenza di tale soluzione e si ottiene 
\[
    \|Ax-b\| = \|U^TAx-U^Tb\| = \|U^T AVV^T x - U^Tb\|
\]

Posto $y = V^T x \in \mathbb{R^n}$ e $g=U^Tb\in \mathbb{R^m}$ si ha:
\[
    \|r\| = \|Ax^* - b\|= \sum_{i=k+1}^{n}(u^T_i b)^2
\]
Moltiplicando $U^T$ a destra e a sinistra si ottiene:
\[
    \|Ax-b\|=\|\Sigma y- g\| = \sum^k_{i=i}(\sigma_1y_i - g_i) + \sum^n_{i=k+1} g^2_i
\]

Per minimizzare tale quantità è sufficiente scegliere:
\[
    y_i = \frac{g_i}{\sigma_i} = \frac{U^T b}{\sigma_i}    
\]
poichè $x^* =Vy$ risulta:
\[
    x*=\sum_{i=1}^{k} \frac{u^T_i b }{\sigma_i}v_i   
\]

Allora in corrispondenza di tale soluzione, la norma del residuo è:
\[
    \|r\| = \sum_{i=k+1}^{n}(g_i)^2 = \sum_{i=k+1}^{n} (u^T_i b)^2   
\]
\section{Matrice pseudoinversa}
Qui viene riportata la definizione di una matrice con proprietà molto utili:
\definizione{
    Sia $A\in \mathbb{R}^{m\times n}$, con $rg(A) = k \leq n$ e sia $A = U\Sigma V^T$ la sua decomposizione in valori singolari. Si definisce \textbf{pseudoinversa} la matrice:
    \[
        A^+ = V\Sigma^+ U^T    
    \]
    Dove: 
    \[
        (\Sigma)_{ij}=\begin{cases}
            \frac{1}{\sigma_i} \quad \text{se } i=j \land i\leq k \\
            0 \quad \text{altrimenti}
        \end{cases}
    \]
    Ovvero:
    \[
        \Sigma^+ = \begin{pmatrix}
        \frac{1}{\sigma_1} & 0 & \cdots & 0 &\cdots & 0 \\
        0 & \frac{1}{\sigma_2} & \cdots & 0&\cdots & 0  \\
        \vdots & \vdots & \ddots & \vdots &\cdots & 0 \\
        0 & 0 & \cdots & \frac{1}{\sigma_k} & \cdots & 0 \\
        \vdots & \vdots & \vdots & \vdots &\ddots & 0 \\
        0 & 0 & \cdots & 0 & \cdots & 0
        \end{pmatrix}_{m \times n}
    \]

}
Questa giga matrice gode delle seguenti proprietà:
\begin{enumerate}
    \item \( AA^+ A = A \)
    \item \( A^+ AA^+ = A^+ \)
    \item \( (AA^+)^T = AA^+ \)
    \item \( (A^+ A)^T = A^+ A \)
\end{enumerate}

La pseudoinversa di una matrice rettangolare $A$ permette di scrivere la soluzione del problema dei minimi quadrati in modo simile alla soluzione $x=A^{-1}b$ di un sistema lineare quadrato, cioè
\[
    x^* = V\Sigma U^T b \implies x^* = A^+ b
\]

Tuttavia sebbene la pseudoinversa sia teoricamente potente per analizzare il problema dei minimi quadrati e comprendere le proprietà delle soluzioni, il calcolo diretto della pseudoinversa di una matrice è spesso inefficiente e computazionalmente oneroso, soprattutto per matrici di grandi dimensioni. Questo è dovuto principalmente al costo della decomposizione ai valori singolari (SVD), che richiede un notevole sforzo computazionale. Pertanto non è uno strumento adeguato per calcolarle poiché computazionalmente costoso.

Si può inoltre definire il numero di condizionamento di $A \in \mathbb{R}^{m\times n}$ in termini di pseudoinversa:

\[
    K(A)  = \|A\|\|A^+\|
\]
 
E il numero di condizionamento in norma -2 (o condizionamento spettrale):

\[
    K(A)_2 = \|A\|_2 \cdot \| A^+\|_2 = \begin{cases}
        \frac{\sigma_1}{\sigma_n} & k = vg (A) \\
        \frac{\sigma_1}{\sigma_k} & k = rg(A)
    \end{cases}
\]

Dove: 
\begin{itemize}
    \item \textbf{k=vg(A)}: quando $k$ è uguale al numero di colonne di $A$, si sta considerando la situazione in cui la matrice ha tutte le colonne sono linearmente indipendenti
    \item \textbf{k=rg(A)}: quando $k$ è uguale al rango effettivo di $A$, ovvero il numero massimo di colonne linearmente indipendenti
\end{itemize}

Il parametro di regolarizzazione, come già visto nel caso dei problemi inversi 1D, può
essere scelto in modo euristico oppure applicando il criterio di Massima Discrepanza.
Per verificare sperimentalmente il parametro “ottimale” nel caso in cui si abbia la
soluzione esatta (cioè in un problema test) è quello di verificare quale parametro
rispetto a quelli scelti produce l’ errore minore rispetto alla “ground truth” x GT ,
utilizzando per esempio l’ errore relativo già definito in precdenza:

\[
    ER = \frac{\|x-x_{GT}\|_2^2}{\|x_{GT}\|}    
\]